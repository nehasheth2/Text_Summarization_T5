,source,source_labels,rouge_scores,paper_id,target,cleaned_source,full_source_text,sent_count,avg_word_len,avg_sent_len
0,"['Incremental class learning involves sequentially learning classes in bursts of examples from the same class.', 'This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting.', 'Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale.', 'Here, we propose FearNet for incremental class learning.', 'FearNet is a generative model that does not store previous examples, making it memory efficient.', 'FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex.', 'Memory consolidation is inspired by mechanisms that occur during sleep.', 'FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  ', 'FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.2857142686843872, 0.1818181723356247, 0.22727271914482117, 0.2666666507720947, 0.3243243098258972, 0.2800000011920929, 0.25, 0.2926829159259796, 0.19999998807907104]",SJ1Xmf-Rb,"['FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.', 'This paper presents a novel solution to an incremental classification problem based on a dual memory system. ']","['incremental class learning involves sequentially learning class burst example class ', 'violates assumption underlie method training standard deep neural network  cause suffer catastrophic forgetting ', 'arguably  best method incremental class learning icarl  requires storing training example class  making challenging scale ', ' propose fearnet incremental class learning ', 'fearnet generative model store previous example  making memory efficient ', 'fearnet us braininspired dualmemory system new memory consolidated network recent memory inspired mammalian hippocampal complex network longterm storage inspired medial prefrontal cortex ', 'memory consolidation inspired mechanism occur sleep ', 'fearnet also us module inspired basolateral amygdala determining memory system use recall ', 'fearnet achieves stateoftheart performance incremental class learning image  cifar100  cub200  audio classification  audioset  benchmark ']","Incremental class learning involves sequentially learning classes in bursts of examples from the same class., This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting., Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale., Here, we propose FearNet for incremental class learning., FearNet is a generative model that does not store previous examples, making it memory efficient., FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex., Memory consolidation is inspired by mechanisms that occur during sleep., FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  , FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.
",16,6.006060606060606,10.3125
1,"['Multi-view learning can provide self-supervision when different views are available of the same data.', 'Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.', 'Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.', 'One framework uses a generative objective and the other a discriminative one.', 'In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.', 'We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.']","[1, 0, 0, 0, 0, 0]","[0.20000000298023224, 0.0, 0.15789473056793213, 0.0, 0.10810810327529907, 0.0]",S1xzyhR9Y7,"['Multi-view learning improves unsupervised sentence representation learning', 'Approach uses different, complementary encoders of the input sentence and consensus maximization.', 'The paper presents a multi-view framework for improving sentence representation in NLP tasks using generative and discriminative objective architectures.', 'This paper shows that multi-view frameworks are more effective than using individual encoders for learning sentence representations.']","['multiview learning provide selfsupervision different view available data ', 'distributional hypothesis provides another form useful selfsupervision adjacent sentence plentiful large unlabelled corpus ', 'motivated asymmetry two hemisphere human brain well observation different learning architecture tend emphasise different aspect sentence meaning  present two multiview framework learning sentence representation unsupervised fashion ', 'one framework us generative objective discriminative one ', 'framework  final representation ensemble two view   one view encodes input sentence recurrent neural network  rnn   view encodes simple linear model ', 'show  learning  vector produced multiview framework provide improved representation singleview learnt counterpart  combination different view give representational improvement view demonstrates solid transferability standard downstream task ']","Multi-view learning can provide self-supervision when different views are available of the same data., Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora., Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion., One framework uses a generative objective and the other a discriminative one., In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model., We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.",14,6.030674846625767,11.642857142857142
2,"['We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\n', 'More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\n', 'Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion.', 'During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\n', 'This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.']","[1, 0, 0, 0, 0]","[0.978723406791687, 0.3333333432674408, 0.4150943458080292, 0.1463414579629898, 0.25531914830207825]",HJDUjKeA-,"['We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.', 'A method for learning object representations from pixels for doing reinforcement learning. ', 'The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss. ']","['show discrete object learnt unsupervised fashion pixel  perform reinforcement learning using object representation ', 'precisely  construct differentiable mapping image discrete tabular list object  object consists differentiable position  feature vector  scalar presence value allows representation learnt using attention mechanism ', 'applying mapping atari game  together interaction netstyle architecture calculating quantity object  construct agent play atari game using object learnt unsupervised fashion ', 'training  many natural object emerge  ball paddle pong  submarine fish seaquest ', 'give first reinforcement learning agent atari interpretable object representation  open avenue agent conduct objectbased exploration generalization ']","We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.

, More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.

, Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion., During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.

, This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.",16,5.691780821917808,9.125
3,"['Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).', 'Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.', 'Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.', 'We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps.', 'Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization.', 'As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.']","[0, 0, 1, 0, 0, 0]","[0.11764705181121826, 0.1463414579629898, 0.19999998807907104, 0.1666666567325592, 0.10526315122842789, 0.16326530277729034]",BJgLg3R9KQ,"['A large-scale dataset for training attention models for object recognition leads to more accurate, interpretable, and human-like object recognition.', 'Argues recent gains in visual recognition stem from using visual attention mechanisms in deep convolutional networks, which learn where to focus through a weak form of supervision based on image class labels.', 'Presents a new take on attention in which a large attention dataset is collected and used to train a NN in a supervised manner to exploit self-reported human attention.', 'This paper proposes a new approach to use more informative signals, specifically, regions humans deem important on images, to improve deep convolutional neural networks.']","['recent gain visual recognition originated inclusion attention mechanism deep convolutional network  dcns  ', 'network optimized object recognition  learn attend using weak form supervision derived image class label ', ' demonstrate benefit using stronger supervisory signal teaching dcns attend image region human deem important object recognition ', 'first describe largescale online experiment  clickme  used supplement imagenet nearly half million humanderived  topdown  attention map ', 'using human psychophysics  confirm identified topdown feature clickme diagnostic  bottomup  saliency feature rapid image categorization ', 'proof concept  extend stateoftheart attention network demonstrate adding clickme supervision significantly improves accuracy yield visual feature interpretable similar used human observer ']","Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs)., Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels., Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition., We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps., Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization., As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.",10,5.960264900662252,15.1
4,"['In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.', 'However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.', 'Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  ', 'However, this robustness is only effective tothe same attack method used for adversarial training.  ', 'Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   ', 'However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.', 'In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  ', 'We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0, 0.05882352590560913, 0.2702702581882477, 0.14814814925193787, 0.1702127605676651, 0.12121211737394333, 0.23076923191547394, 0.2666666507720947]",BklpOo09tQ,"['We proposed a time-efficient defense method against one-step and iterative adversarial attacks.', 'Propose a novel, computationaly efficient method named e2SAD which generates sets of two training adversarial samples for each clean training sample.', 'The paper introduces a two-step adversarial defense method, to generate two adversarial examples per clean sample and include them in the actual training loop to achieve robustness and claiming it can outperform more expensive iterative methods.', 'The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks.']","['recent year  deep neural network demonstrated outstanding performancein many machine learning task ', 'however  researcher discovered thesestateoftheart model vulnerable adversarial example  legitimate example added small perturbation unnoticeable human eye ', 'adversarial training  augments training data adversarial example duringthe training process  well known defense improve robustness themodel adversarial attack ', 'however  robustness effective tothe attack method used adversarial training ', 'madry et al   2017  suggest effectiveness iterative multistep adversarial attack particularlythat projected gradient descent  pgd  may considered universal first order adversary applying adversarial training pgd implies resistanceagainst many first order attack ', 'however  computational cost theadversarial training pgd multistep adversarial example muchhigher adversarial training simpler attack technique ', 'paper  show strong adversarial example generated ata cost similar two run fast gradient sign method  fgsm   allowing defense adversarial attack robustness level comparable thatof adversarial training multistep adversarial example ', 'empiricallydemonstrate effectiveness proposed twostep defense approach againstdifferent attack method improvement existing defense strategy ']","In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks., However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes., Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  , However, this robustness is only effective tothe same attack method used for adversarial training.  , Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   , However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques., In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  , We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.",16,6.44131455399061,12.529411764705882
5,"['Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.', 'Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.', 'In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).', 'Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.']","[0, 0, 0, 1]","[0.1818181723356247, 0.1111111044883728, 0.19607841968536377, 0.3492063581943512]",BJLmN8xRW,"['A comparison of five deep neural network architectures for detection of malicious domain names shows surprisingly little difference.', 'Authors propose using five deep architectures for the cybersecurity task of domain generation algorithm detection.', ""Applies several NN architectures to classify url's between begign and malware related URLs."", 'This paper proposes to automatically recognize domain names as malicious or benign by deep networks trained to directly classify the character sequence as such.']","['recently several different deep learning architecture proposed take string character raw input signal automatically derive feature text classification ', 'little study available compare effectiveness approach character based text classification ', 'paper perform empirical comparison important cybersecurity problem dga detection  classifying domain name either benign v produced malware  ie  domain generation algorithm  ', 'training evaluating dataset 2m domain name show surprisingly little difference various convolutional neural network  cnn  recurrent neural network  rnn  based architecture term accuracy  prompting preference simpler architecture  since faster train le prone overfitting ']","Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification., Little studies are available that compare the effectiveness of these approaches for character based text classification with each other., In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm)., Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.",7,5.847328244274809,18.714285714285715
6,"['Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks.', 'Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts.', 'We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.', 'We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases.', 'Our experiments demonstrate that our models are more robust to new NLI datasets.']","[0, 0, 0, 0, 1]","[0.09302324801683426, 0.2380952388048172, 0.24242423474788666, 0.2631579041481018, 0.2857142686843872]",rkMlSnAqYX,"['Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets.', 'The paper proposes an adversarial setup to mitigate annotation artifacts in natural language inference data', 'This paper presents a method for removing bias of a textual entailment model through an adversarial training objective. ']","['recognizing relationship two text important aspect natural language understanding  nlu   variety neural network model proposed solving nlu task ', 'unfortunately  recent work showed datasets model trained often contain bias allow model achieve nontrivial performance without possibly learning relationship two text ', 'propose framework building robust model using adversarial learning encourage model learn latent  biasfree representation ', 'test approach natural language inference  nli  scenario  show adversariallytrained model learn robust representation ignore known datasetspecific bias ', 'experiment demonstrate model robust new nli datasets ']","Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks., Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts., We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations., We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases., Our experiments demonstrate that our models are more robust to new NLI datasets.",9,5.9576271186440675,13.11111111111111
7,"['We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.', 'The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.', 'In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.', 'Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.', 'In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.', 'Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.']","[0, 0, 1, 0, 0, 0]","[0.1599999964237213, 0.0, 0.3333333432674408, 0.0, 0.07692307233810425, 0.1428571343421936]",HkgEQnRqYQ,"['A new state-of-the-art approach for knowledge graph embedding.', 'Presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base.', 'This paper proposes an approach to knowledge graph embedding by modeling relations as rotations in the complex vector space.', 'Proposes a method for graph embedding to be used for link prediction']","['study problem learning representation entity relation knowledge graph predicting missing link ', 'success task heavily relies ability modeling inferring pattern   relation ', 'paper  present new approach knowledge graph embedding called rotate  able model infer various relation pattern including  symmetryantisymmetry  inversion  composition ', 'specifically  rotate model defines relation rotation source entity target entity complex vector space ', 'addition  propose novel selfadversarial negative sampling technique efficiently effectively training rotate model ', 'experimental result multiple benchmark knowledge graph show proposed rotate model scalable  also able infer model various relation pattern significantly outperform existing stateoftheart model link prediction ']","We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links., The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations., In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition., Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space., In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model., Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",13,5.767123287671233,11.23076923076923
8,"['Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification.', 'This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks.', 'However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms.', 'In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels.', 'To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps.', 'Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels.', 'Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\n', 'adversarial attacks without employment of additional target detection and rejection algorithms.\n', 'We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.06451612710952759, 0.06666666269302368, 0.13333332538604736, 0.14999999105930328, 0.05405404791235924, 0.06896550953388214, 0.0714285671710968, 0.1599999964237213, 0.14814814925193787]",rkeDJ04Mf,"['We modified the CNN using HyperNetworks and observed better robustness against adversarial examples.', 'Improving the robustness and reliability of deep convolution neural networks by using data-dependent convolution kernels']","['deep learning algorithm known vulnerable adversarial perturbation various task image classification ', 'problem addressed employing several defense method detection rejection particular type attack ', 'however  training manipulating network according particular defense scheme increase computational complexity learning algorithm ', 'work  propose simple yet effective method improve robustness convolutional neural network  cnns  adversarial attack using data dependent adaptive convolution kernel ', 'end  propose new type hypernetwork order employ statistical property input data feature computation statistical adaptive map ', ' filter convolution weight cnns learned statistical map compute dynamic kernel ', 'thereby  weight kernel collectively optimized learning image classification model robust', 'adversarial attack without employment additional target detection rejection algorithm ', 'empirically demonstrate proposed method enables cnns spontaneously defend different type attack  eg  attack generated gaussian noise  fast gradient sign method  goodfellow et al  2014  blackbox attack  narodytska  kasiviswanathan  2016  ']","Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification., This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks., However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms., In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels., To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps., Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels., Thereby, weights and kernels are collectively optimized for learning of image classification models robust to
, adversarial attacks without employment of additional target detection and rejection algorithms.
, We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).",18,6.1,10.0
9,"['Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\n', 'Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\n', 'Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\n', 'In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\n', 'The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\n', 'This requires back-propagating errors through the solver steps.\n', 'While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\n', 'We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\n', 'Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.11999999731779099, 0.2083333283662796, 0.17777776718139648, 0.25, 0.16949151456356049, 0.1621621549129486, 0.17241378128528595, 0.2857142686843872, 0.2641509473323822]",HyxnZh0ct7,"['We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.', 'The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task as having its own final layer which could be a ridge regression or a logistic regression.', 'This paper proposes a meta-learning approach for the problem of few-shot classification, they use a  method based on parametrizing the learner for each task by a closed-form solver.']","['adapting deep network new concept example challenging  due high computational requirement standard finetuning procedure ', 'work fewshot learning thus focused simple learning technique adaptation  nearest neighbour gradient descent ', 'nonetheless  machine learning literature contains wealth method learn nondeep model efficiently ', 'paper  propose use fast convergent method main adaptation mechanism fewshot learning ', 'main idea teach deep network use standard machine learning tool  ridge regression  part internal model  enabling quickly adapt novel data ', 'requires backpropagating error solver step ', 'normally cost matrix operation involved process would significant  using woodbury identity make small number example work advantage ', 'propose closedform iterative solver  based ridge regression logistic regression component ', 'method constitute simple novel approach problem fewshot learning achieve performance competitive superior state art three benchmark ']","Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.
, Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.
, Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.
, In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.
, The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.
, This requires back-propagating errors through the solver steps.
, While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.
, We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.
, Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",18,5.393939393939394,11.0
10,"['While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).', 'To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.', 'To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.', 'At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.', 'Each answer eliminates some classes, leaving the learner with a partial label.', 'The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.', 'Active learning with partial labels requires', '(i) a sampling strategy to choose (example, class) pairs, and', '(ii) learning from partial labels between rounds.', 'Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.', 'Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.', 'Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.21917808055877686, 0.09836065024137497, 0.20895521342754364, 0.1071428507566452, 0.11999999731779099, 0.0937499925494194, 0.045454543083906174, 0.0833333283662796, 0.13333332538604736, 0.15789473056793213, 0.0, 0.10169491171836853]",HJfSEnRqKQ,"['We provide a new perspective on training a machine learning model from scratch in hierarchical label setting, i.e. thinking of it as two-way communication between human and algorithms, and study how we can both measure and improve the efficiency. ', ""Introduces a new Active Learning setting where the oracle offers a partial or weak label instead of querying for a particular example's label, leading to a simpler retrieval of information."", 'This paper proposes a method of active learning with partial feedback that outperforms existing baselines under a limited budget.', 'The paper considers a multiclass classification problem in which labels are grouped in a given number M of subsets, which contain all individual labels as singletons.']","['many active learning paper assume learner simply ask label receive  real annotation often present mismatch form label  say  one among many class   form annotation  typically yesno binary feedback  ', 'annotate example corpus multiclass classification  might need ask multiple yesno question  exploiting label hierarchy one available ', 'address realistic setting  propose active learning partial feedback  alpf   learner must actively choose example label binary question ask ', 'step  learner selects example  asking belongs chosen  possibly composite  class ', 'answer eliminates class  leaving learner partial label ', 'learner may either ask question example  exact label uncovered  move immediately  leaving first example partially labeled ', 'active learning partial label requires', '  sampling strategy choose  example  class  pair ', ' ii  learning partial label round ', 'experiment tiny imagenet demonstrate effective method improves 26   relative  top1 classification accuracy compared iid  baseline standard active learner given 30  annotation budget would required  naively  annotate dataset ', 'moreover  alpflearners fully annotate tinyimagenet 42  lower cost ', 'surprisingly  observe accounting perexample annotation cost alter conventional wisdom active learner solicit label hard example ']","While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback)., To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available., To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask., At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class., Each answer eliminates some classes, leaving the learner with a partial label., The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled., Active learning with partial labels requires, (i) a sampling strategy to choose (example, class) pairs, and, (ii) learning from partial labels between rounds., Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset., Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost., Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.",27,5.5396825396825395,9.0
11,"['Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.', 'Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.', 'Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.', 'We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.', 'We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.', 'We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.', 'This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.']","[0, 0, 0, 1, 0, 0, 0]","[0.15789473056793213, 0.1666666567325592, 0.21052631735801697, 0.277777761220932, 0.2666666507720947, 0.09090908616781235, 0.07407406717538834]",rJg4J3CqFm,"['We show that Wasserstein spaces are good targets for embedding data with complex semantic structure.', 'Learns embeddings in a discrete space of probability distributions, using a minimized, regularised version of Wasserstein distances.', 'The paper describes a new embedding method that embeds data to the space of probability measures endowed with the Wasserstein distance. ', 'The paper proposes embedding the data into low-dimensional Wasserstein spaces, which can capture the underlying structure of the data more accurately.']","['despite prevalence  euclidean embeddings data fundamentally limited ability capture latent semantic structure  need conform euclidean spatial assumption ', 'consider alternative  embeds data discrete probability distribution wasserstein space  endowed optimal transport metric ', 'wasserstein space much larger flexible euclidean space  successfully embed wider variety metric structure ', 'propose exploit flexibility learning embedding capture semantic information wasserstein distance embedded distribution ', 'examine empirically representational capacity learned wasserstein embeddings  showing embed wide variety complex metric structure smaller distortion equivalent euclidean embedding ', 'also investigate application word embedding  demonstrating unique advantage wasserstein embeddings  directly visualize highdimensional embedding  probability distribution lowdimensional space ', 'obviates need dimensionality reduction technique tsne visualization ']","Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions., Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric., Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures., We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions., We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding., We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space., This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.",15,6.189349112426036,11.266666666666667
12,"['Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces.', 'We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.', 'The data is embedded into a lower-dimensional space by a deep autoencoder.', 'The autoencoder is optimized as part of the clustering process.', 'The resulting network produces clustered data.', 'The presented approach does not rely on prior knowledge of the number of ground-truth clusters.', 'Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective.', 'We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms.', 'Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.']","[0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.6428571343421936, 0.14814814925193787, 0.07692307233810425, 0.0, 0.0, 0.5806451439857483, 0.20689654350280762, 0.1621621549129486]",SJzMATlAZ,"['A clustering algorithm that performs joint nonlinear dimensionality reduction and clustering by optimizing a global continuous objective.', 'Presents a clustering algorithm by jointly solving deep autoencoder and clustering as a global continuous objective, showing better results than state-of-the-art clustering schemas.', 'Deep Continuous Clustering is a clustering method that integrates the autoencoder objective with the clustering objective then train using SGD.']","['clustering highdimensional datasets hard interpoint distance become le informative highdimensional space ', 'present clustering algorithm performs nonlinear dimensionality reduction clustering jointly ', 'data embedded lowerdimensional space deep autoencoder ', 'autoencoder optimized part clustering process ', 'resulting network produce clustered data ', 'presented approach rely prior knowledge number groundtruth cluster ', 'joint nonlinear dimensionality reduction clustering formulated optimization global continuous objective ', 'thus avoid discrete reconfigurations objective characterize prior clustering algorithm ', 'experiment datasets multiple domain demonstrate presented algorithm outperforms stateoftheart clustering scheme  including recent method use deep network ']","Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces., We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly., The data is embedded into a lower-dimensional space by a deep autoencoder., The autoencoder is optimized as part of the clustering process., The resulting network produces clustered data., The presented approach does not rely on prior knowledge of the number of ground-truth clusters., Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective., We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms., Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.",10,6.7,12.0
13,"['Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements.', 'Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation.', 'However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning.', 'Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time.', 'Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure.', 'To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning.', 'As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining.', 'For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients.', 'This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure.', 'For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.09999999403953552, 0.10256409645080566, 0.1463414579629898, 0.16393442451953888, 0.2448979616165161, 0.5714285373687744, 0.2083333283662796, 0.19999998807907104, 0.29999998211860657, 0.12765957415103912]",SJzYdsAqY7,"['To accelerate the computation of convolutional neural networks, we propose a new two-step pruning technique which achieves a higher Winograd-domain weight sparsity without changing the network structure.', 'Proposes a spatial-Winograd pruning framework which allows pruned weight from the spatial domain to be kept in the Winograd domain and improves the sparsity of the Winograd domain.', 'Proposes two techniques for pruning convolutional layers which use the Winograd algorithm']","['deep convolutional neural network  cnns  deployed various application demand immense computational requirement ', 'pruning technique winograd convolution two typical method reduce cnn computation ', 'however  directly combined winograd transformation fill sparsity resulting pruning ', 'li et al   2017  propose sparse winograd convolution weight directly pruned winograd domain  technique practical winograddomain retraining requires low learning rate hence significantly longer training time ', 'besides  liu et al   2018  move relu function winograd domain  help increase weight sparsity requires change network structure ', 'achieve high winograddomain weight sparsity without changing network structure  propose new pruning method  spatialwinograd pruning ', 'first step  spatialdomain weight pruned structured way  efficiently transfer spatialdomain sparsity winograd domain avoids winograddomain retraining ', 'next step  also perform pruning retraining directly winograd domain propose use importance factor matrix adjust weight importance weight gradient ', 'adjustment make possible effectively retrain pruned winograddomain network without changing network structure ', 'three model datasets cifar10  cifar100  imagenet  proposed method achieve winograddomain sparsity 63   50   74   respectively ']","Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements., Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation., However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning., Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time., Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure., To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning., As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining., For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients., This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure., For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.",25,5.991228070175438,8.444444444444445
14,"['In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.', 'We develop a Bayesian nonparametric framework for federated learning with neural networks.', 'Each data server is assumed to train local neural network weights, which are modeled through our framework.', 'We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling.', 'We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.']","[0, 1, 0, 0, 0]","[0.13333332538604736, 0.8333333134651184, 0.06896550953388214, 0.11764705181121826, 0.19354838132858276]",SygHGnRqK7,"['We propose a Bayesian nonparametric model for federated learning with neural networks.', 'Uses beta process to do federated neural matching.', 'The paper considers federate learning of neural networks, where data is distributed on multiple machines and the allocation of data points is potentially inhomogenous and unbalanced.']","['federated learning problem  data scattered across different server exchanging pooling often impractical prohibited ', 'develop bayesian nonparametric framework federated learning neural network ', 'data server assumed train local neural network weight  modeled framework ', 'develop inference approach allows u synthesize expressive global network without additional supervision data pooling ', 'demonstrate efficacy approach federated learning problem simulated two popular image classification datasets ']","In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited., We develop a Bayesian nonparametric framework for federated learning with neural networks., Each data server is assumed to train local neural network weights, which are modeled through our framework., We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling., We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.",7,5.9222222222222225,12.857142857142858
15,"['We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution.', 'Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed.', 'We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second.', 'Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling.', 'Python source code will be open-sourced with the camera-ready paper.']","[1, 0, 0, 0, 0]","[0.3461538553237915, 0.12765957415103912, 0.03278687968850136, 0.04878048226237297, 0.10810810327529907]",B1n8LexRZ,"['General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.', 'Proposes a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. ']","['present generalpurpose method train markov chain monte carlo kernel  parameterized deep neural network  converge mix quickly target distribution ', 'method generalizes hamiltonian monte carlo trained maximize expected squared jumped distance  proxy mixing speed ', 'demonstrate large empirical gain collection simple challenging distribution  instance achieving 106x improvement effective sample size one case  mixing standard hmc make measurable progress second ', 'finally  show quantitative qualitative gain realworld task  latentvariable generative modeling ', 'python source code opensourced cameraready paper ']","We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution., Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed., We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second., Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling., Python source code will be open-sourced with the camera-ready paper.",11,5.768518518518518,9.818181818181818
16,"['This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.', 'We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.', 'The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.', 'We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.', 'To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.', 'Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.', 'The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.', 'To solve this we propose a continuation approach that learns failure modes in related but less robust agents.', 'Our approach also allows reuse of data already collected for training the agent.', 'We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.', 'Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.07999999821186066, 0.08888888359069824, 0.11999999731779099, 0.1304347813129425, 0.17391303181648254, 0.09302324801683426, 0.12765957415103912, 0.17777776718139648, 0.14999999105930328, 0.09302324801683426, 0.1428571343421936]",B1xhQhRcK7,"['We show that rare but catastrophic failures may be missed entirely by random testing, which poses issues for safe deployment. Our proposed approach for adversarial testing fixes this.', 'Proposes a method which learns a failure probability predictor for a learned agent, leading to predictions of which initial states cause a system to fail.', 'This paper proposes an importance sampling approach to sampling failure cases for RL algorithms based on a function learned via a neural network on failures that occur during agent training', 'This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning.']","['paper address problem evaluating learning system safety critical domain autonomous driving  failure catastrophic consequence ', 'focus two problem  searching scenario learned agent fail assessing probability failure ', 'standard method agent evaluation reinforcement learning  vanilla monte carlo  miss failure entirely  leading deployment unsafe agent ', 'demonstrate issue current agent  even matching compute used training sometimes insufficient evaluation ', 'address shortcoming  draw upon rare event probability estimation literature propose adversarial evaluation approach ', 'approach focus evaluation adversarially chosen situation  still providing unbiased estimate failure probability ', 'key difficulty identifying adversarial situation  since failure rare little signal drive optimization ', 'solve propose continuation approach learns failure mode related le robust agent ', 'approach also allows reuse data already collected training agent ', 'demonstrate efficacy adversarial evaluation two standard domain  humanoid control simulated driving ', 'experimental result show method find catastrophic failure estimate failure rate agent multiple order magnitude faster standard evaluation scheme  minute hour rather day ']","This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences., We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure., The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents., We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation., To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach., Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities., The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization., To solve this we propose a continuation approach that learns failure modes in related but less robust agents., Our approach also allows reuse of data already collected for training the agent., We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving., Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",19,5.918552036199095,11.631578947368421
17,"['The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique.', ""By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods."", 'In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior.', 'In this paper, we investigate posterior collapse from the perspective of training dynamics.', ""We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target."", 'As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs.', ""Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update."", 'Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work.', 'Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.12244897335767746, 0.19354838132858276, 0.26229506731033325, 0.17777776718139648, 0.2222222238779068, 0.25531914830207825, 0.3333333134651184, 0.19354838132858276, 0.16129031777381897]",rylDfnCqF7,"['To address posterior collapse in VAEs, we propose a novel yet simple training procedure that aggressively optimizes inference network with more updates. This new training procedure mitigates posterior collapse and leads to a better VAE model. ', 'Looks into the phenomenon of posterior collapse, showing that increased training of the inference network can reduce the problem and lead to better optima.', 'Authors propose changing the training procedure of VAEs only as a solution to posterior collapse, leaving the model and objective untouched.']","['variational autoencoder  vae  popular combination deep latent variable model accompanying variational learning technique ', 'using neural inference network approximate model posterior latent variable  vaes efficiently parameterize lower bound marginal data likelihood optimized directly via gradient method ', 'practice  however  vae training often result degenerate local optimum known  posterior collapse  model learns ignore latent variable approximate posterior mimic prior ', 'paper  investigate posterior collapse perspective training dynamic ', 'find initial stage training inference network fails approximate model true posterior  moving target ', 'result  model encouraged ignore latent encoding posterior collapse occurs ', 'based observation  propose extremely simple modification vae training reduce inference lag  depending model current mutual information latent variable observation  aggressively optimize inference network performing model update ', 'despite introducing neither new model component significant complexity basic vae  approach able avoid problem collapse plagued large amount previous work ', 'empirically  approach outperforms strong autoregressive baseline text image benchmark term heldout likelihood  competitive complex technique avoiding collapse substantially faster ']","The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique., By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods., In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior., In this paper, we investigate posterior collapse from the perspective of training dynamics., We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target., As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs., Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update., Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work., Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.",20,5.814345991561181,11.85
18,"['Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies.', 'To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form.', 'To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge.', 'Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs.', 'The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs.', 'Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method.', 'Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.']","[0, 0, 1, 0, 0, 0, 0]","[0.1599999964237213, 0.25, 0.4000000059604645, 0.25925925374031067, 0.29629629850387573, 0.1860465109348297, 0.26923075318336487]",BJhxcGZCW,"['Generatively discover meaningful, novel entity pairs with a certain medical relationship by purely learning from the existing meaningful entity pairs, without the requirement of additional text corpus for discriminative extraction.', 'Presents a variational autoencoder for generating entity pairs given a relation in a medical setting.', 'In the medical context, this paper describes the classic problem of ""knowledge base completion"" from structured data only.']","['online healthcare service provide general public ubiquitous access medical knowledge reduce information access cost individual society ', 'promote benefit  desired effectively expand scale highquality yet novel relational medical entity pair embody rich medical knowledge structured form ', 'fulfill goal  introduce generative model called conditional relationship variational autoencoder  crvae   discover meaningful novel relational medical entity pair without requirement additional external knowledge ', 'rather discriminatively identifying relationship two given medical entity freetext corpus  directly model understand medical relationship diversely expressed medical entity pair ', 'proposed model introduces generative modeling capacity variational autoencoder entity pair  ability discover new relational medical entity pair solely based existing entity pair ', 'beside entity pair  relationshipenhanced entity representation obtained another appealing benefit proposed method ', 'quantitative qualitative evaluation realworld medical datasets demonstrate effectiveness proposed method generating relational medical entity pair meaningful novel ']","Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies., To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form., To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge., Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs., The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs., Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method., Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.",13,6.317460317460317,14.538461538461538
19,"['Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.', ' In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples', '.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true', '.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning', '.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture', '. The code for our model is available at \\url{https://github.com/daib13/TwoStageVAE}.']","[0, 0, 0, 1, 0, 0]","[0.10810810327529907, 0.11428570747375488, 0.23529411852359772, 0.3333333432674408, 0.17777776718139648, 0.0]",B1e0X3C9tQ,"['We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements.', 'Proposes a two-stage VAE method to generate high-quality samples and avoid blurriness.', 'This paper analyzes the Gaussian VAEs.', 'The paper provides a number of theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders, which are then used to build a new algorithm called ""2 stage VAEs"".']","['although variational autoencoders  vaes  represent widely influential deep generative model  many aspect underlying energy function remain poorly understood ', 'particular  commonly believed gaussian encoderdecoder assumption reduce effectiveness vaes generating realistic sample', ' regard  rigorously analyze vae objective  differentiating situation belief actually true', ' leverage corresponding insight develop simple vae enhancement requires additional hyperparameters sensitive tuning', ' quantitatively  proposal produce crisp sample stable fid score actually competitive variety gan model  retaining desirable attribute original vae architecture', ' code model available url  http  githubcomdaib13twostagevae  ']","Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.,  In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples, .  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true, .  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning, .  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture, . The code for our model is available at \url{https://github.com/daib13/TwoStageVAE}.",12,6.178861788617886,7.6875
20,"['We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  ', 'We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters.', 'The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \n', 'Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  ', 'In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  ', 'We also outperform Random Search $8\\times$.\n   \nOur method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  ', 'We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).']","[0, 0, 1, 0, 0, 0, 0]","[0.19999998807907104, 0.0, 0.2222222238779068, 0.0833333283662796, 0.045454543083906174, 0.1764705777168274, 0.06666666269302368]",H1zriGeCZ,"['A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing', 'Investigates problem of optimizing hyperparameters under the assumption that the unknown function can be approximated, showing that the approximate minimization can be performed over the boolean hypercube.', 'The paper explores hyperparameter optimization by assuming structure in the unknown function mapping hyperparameters to classification accuracy']","['give simple  fast algorithm hyperparameter optimization inspired technique analysis boolean function ', 'focus highdimensional regime canonical example training neural network large number hyperparameters ', 'algorithm   iterative application compressed sensing technique orthogonal polynomial   requires uniform sampling hyperparameters thus easily parallelizable ', 'experiment training deep neural network cifar10 show compared stateoftheart tool  eg  hyperband spearmint   algorithm find significantly improved solution  case better attainable handtuning ', 'term overall running time  ie  time required sample various setting hyperparameters plus additional computation time   least order magnitude faster hyperband bayesian optimization ', 'also outperform random search  8times   method inspired provablyefficient algorithm learning decision tree using discrete fourier transform ', 'obtain improved samplecomplexty bound learning decision tree matching stateoftheart bound running time  polynomial quasipolynomial  respectively  ']","We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  , We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters., The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.
 
, Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  , In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  , We also outperform Random Search $8\times$.
   
Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  , We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).",14,6.281609195402299,11.6
21,"['Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data.', 'Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable.', 'In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  ', 'Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator.', 'With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings.', 'We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.']","[0, 0, 1, 0, 0, 0]","[0.1904761791229248, 0.0, 0.21739129722118378, 0.05714285373687744, 0.1860465109348297, 0.1702127605676651]",Byt3oJ-0W,"['A new method for gradient-descent inference of permutations, with applications to latent matching inference and supervised learning of permutations with neural networks', 'The paper utilizes finite approximation of the Sinkhorn operator to describe how one can construct a neural network for learning from permutation valued training data. ', 'The paper proposes a new method that approximates the discrete max-weight for learning latent permutations']","['permutation matchings core building block variety latent variable model  allow u align  canonicalize  sort data ', 'learning model difficult  however  exact marginalization combinatorial object intractable ', 'response  paper introduces collection new method endtoend learning model approximate discrete maximumweight matching using continuous sinkhorn operator ', 'sinkhorn iteration attractive function simple  easytoimplement analog softmax operator ', ' define gumbelsinkhorn method  extension gumbelsoftmax method  jang et al  2016  maddison2016 et al  2016  distribution latent matchings ', 'demonstrate effectiveness method outperforming competitive baseline range qualitatively different task  sorting number  solving jigsaw puzzle  identifying neural signal worm ']","Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data., Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable., In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  , Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator., With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings., We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.",18,5.9855072463768115,6.9
22,"['Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources.', 'However, existing quantization methods often represent all weights and activations with the same precision (bit-width).', 'In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths.', 'We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization.', 'Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet.', 'Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.']","[0, 0, 0, 1, 0, 0]","[0.14999999105930328, 0.07407406717538834, 0.06896550953388214, 0.3333333432674408, 0.07999999821186066, 0.0]",BJGVX3CqYm,"['A novel differentiable neural architecture search framework for mixed quantization of ConvNets.', 'The authors introduce a new method for neural architecture search which selects the precision quantization of weights at each neural network layer, and use it in the context of network compression.', 'The paper presents a new approach in network quantization by quantizing different layers with different bit-widths and introduces a new differentiable neural architecture search framework.']","['recent work network quantization substantially reduced time space complexity neural network inference  enabling deployment embedded mobile device limited computational memory resource ', 'however  existing quantization method often represent weight activation precision  bitwidth  ', 'paper  explore new dimension design space  quantizing different layer different bitwidths ', 'formulate problem neural architecture search problem propose novel differentiable neural architecture search  dna  framework efficiently explore exponential search space gradientbased optimization ', 'experiment show surpass stateoftheart compression resnet cifar10 imagenet ', 'quantized model 211x smaller model size 1039x lower computational cost still outperform baseline quantized even full precision model ']","Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources., However, existing quantization methods often represent all weights and activations with the same precision (bit-width)., In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths., We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization., Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet., Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.",9,6.2846153846153845,14.444444444444445
23,"['The top-$k$ error is a common measure of performance in machine learning and computer vision.', 'In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.', 'Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.', 'In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.\n', 'Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.', 'Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.', 'The widely used cross-entropy is a special case of our family.', 'Evaluating our smooth loss functions is computationally challenging: a na{\\""i}ve algorithm would require $\\mathcal{O}(\\binom{n}{k})$ operations, where $n$ is the number of classes.', 'Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$.', 'Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.', 'We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.', 'Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.0714285671710968, 0.0624999962747097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06896551698446274, 0.0]",Hk5elxbRW,"['Smooth Loss Function for Top-k Error Minimization', 'Proposes using top-k loss with deep models to address the problem of class confusion with similar classes both present or absent of the training dataset.', 'Smoothes the top-k losses.', 'This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks.']","['top  k  error common measure performance machine learning computer vision ', 'practice  top  k  classification typically performed deep neural network trained crossentropy loss ', 'theoretical result indeed suggest crossentropy optimal learning objective task limit infinite data ', 'context limited noisy data however  use loss function specifically designed top  k  classification bring significant improvement ', 'empirical evidence suggests loss function must smooth nonsparse gradient order work well deep neural network ', 'consequently  introduce family smoothed loss function suited top  k  optimization via deep learning ', 'widely used crossentropy special case family ', 'evaluating smooth loss function computationally challenging  na     algorithm would require  mathcal    binom  n   k    operation   n  number class ', 'thanks connection polynomial algebra divideandconquer approach  provide algorithm time complexity  mathcal    k n   ', 'furthermore  present novel approximation obtain fast stable algorithm gpus single floating point precision ', 'compare performance crossentropy loss marginbased loss various regime noise data size  predominant use case  k5  ', 'investigation reveals loss robust noise overfitting crossentropy ']","The top-$k$ error is a common measure of performance in machine learning and computer vision., In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss., Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data., In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.
, Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks., Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning., The widely used cross-entropy is a special case of our family., Evaluating our smooth loss functions is computationally challenging: a na{\""i}ve algorithm would require $\mathcal{O}(\binom{n}{k})$ operations, where $n$ is the number of classes., Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\mathcal{O}(k n)$., Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision., We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$., Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.",19,5.6525423728813555,12.421052631578947
24,"['Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties.', 'To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.', 'Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property.', 'We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP).', 'In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.']","[0, 1, 0, 0, 0]","[0.2380952388048172, 0.3684210479259491, 0.1764705777168274, 0.2857142686843872, 0.25806450843811035]",BklKFo09YX,"['We introduce Mol-CycleGAN - a new generative model for optimization of molecules to augment drug design.', 'The paper presents an approach for optimizing molecular properties based on the application of CycleGANs to variational autoencoders for molecules and employs a domain-specific VAE called Junction Tree VAE (JT-VAE).', 'This paper uses a variational autoencoders to learn a translation function, from the set of molecules without the interested property to the set of molecules with the property. ']","['designing molecule desired property one biggest challenge drug development  requires optimization chemical compound structure respect many complex property ', 'augment compound design process introduce molcyclegan  cycleganbased model generates optimized compound chemical scaffold interest ', 'namely  given molecule model generates structurally similar one optimized value considered property ', 'evaluate performance model selected optimization objective related structural property  presence halogen group  number aromatic ring  physicochemical property  penalized logp  ', 'task optimization penalized logp druglike molecule model significantly outperforms previous result ']","Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties., To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest., Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property., We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP)., In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.",8,5.991525423728813,14.75
25,"['Knowledge distillation is a potential solution for model compression.', 'The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one.', 'Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network.', 'However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered.', 'To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.', 'By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks.', 'Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates.', 'In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification.', 'Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.20689654350280762, 0.09756097197532654, 0.24390242993831635, 0.1538461446762085, 0.8085106611251831, 0.1666666567325592, 0.1304347813129425, 0.25, 0.05882352590560913]",rJFOptp6Z,"['We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification', 'This paper proposes to transfer the classifier from the model for face classification to the task of alignment and verification.', 'The manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification.']","['knowledge distillation potential solution model compression ', 'idea make small student network imitate target large teacher network  student network competitive teacher one ', 'previous study focus model distillation classification task  propose different architecture initialization student network ', 'however  classification task enough  related task regression retrieval barely considered ', 'solve problem  paper  take face recognition breaking point propose model distillation knowledge transfer face classification alignment verification ', 'selecting appropriate initialization target knowledge transfer  distillation easier nonclassification task ', 'experiment celeba casiawebface datasets demonstrate student network competitive teacher one alignment verification  even surpasses teacher network specific compression rate ', 'addition  achieve stronger knowledge transfer  also use common initialization trick improve distillation performance classification ', 'evaluation casiawebface largescale msceleb1m datasets show effectiveness simple trick ']","Knowledge distillation is a potential solution for model compression., The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one., Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network., However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered., To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification., By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks., Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates., In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification., Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.",19,5.912820512820513,10.263157894736842
26,"['RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.', 'The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.', 'In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.', 'The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.', 'Unlike data augmentation-based improvements, our method does not increase training times significantly.']","[0, 1, 0, 0, 0]","[0.1875, 0.20689654350280762, 0.12903225421905518, 0.18867924809455872, 0.0]",ryCM8zWRb,"['Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling.', 'This paper analyzes existing loss functions for session-based recommendations and proposes two novel losses functions which add a weighting to existing ranking-based loss functions', 'Presents modifications on top of earlier work for session-based recommendation using RNN by weighting negative examples by their ""relevance""', 'This paper discusses the issues for optimizing the loss functions in GRU4Rec, proposes tricks for optimizing, and suggests an enhanced version.']","['rnns shown excellent model sequential data particular sessionbased user behavior ', 'use rnns provides impressive performance benefit classical method sessionbased recommendation ', 'work introduce novel ranking loss function tailored rnns recommendation setting ', 'better performance loss alternative  along trick improvement described work  allow achieve overall improvement 35  term mrr recall  20 previous sessionbased rnn solution 51  classical collaborative filtering approach ', 'unlike data augmentationbased improvement  method increase training time significantly ']","RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior., The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations., In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings., The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches., Unlike data augmentation-based improvements, our method does not increase training times significantly.",8,5.888888888888889,13.5
27,"['In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks.', 'Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task.', 'We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.', 'Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task.', 'Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks.', 'We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.']","[0, 0, 1, 0, 0, 0]","[0.260869562625885, 0.2666666507720947, 0.3913043439388275, 0.3265306055545807, 0.1621621549129486, 0.19999998807907104]",rJUBryZ0W,"['We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.', 'A novel PAC-Bayesian risk bound that serves as an objective function for multi-task machine learning, and an algorithm for minimizing a simplified version of that objective function.', 'Extends existing PAC-Bayes bounds to multi-task learning, to allow the prior to be adapted across different tasks.']","['representational lifelong learning agent aim continually learn solve novel task updating representation light previous task ', 'assumption future task related previous task  representation learned way capture common structure across learned task  allowing learner sufficient flexibility adapt novel aspect new task ', 'develop framework lifelong learning deep neural network based generalization bound  developed within pacbayes framework ', 'learning take place construction distribution network based task seen far  utilization learning new task ', 'thus  prior knowledge incorporated setting historydependent prior novel task ', 'develop gradientbased algorithm implementing idea  based minimizing objective function motivated generalization bound  demonstrate effectiveness numerical example ']","In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks., Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task., We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework., Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task., Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks., We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.",13,5.75,11.692307692307692
28,"['Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models.', 'While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs).', 'In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam.', 'As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD.', 'Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits.', 'By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.']","[0, 0, 0, 1, 0, 0]","[0.2926829159259796, 0.19230768084526062, 0.3333333432674408, 0.4285714328289032, 0.12121211737394333, 0.2631579041481018]",HJSA_e1AW,"['A tailored version of Adam for training DNNs, which bridges the generalization gap between Adam and SGD.', 'Proposes a variant of ADAM optimization algorithm that normalizes weights of each hidden unit using batch normalization', 'Extension of the Adam optimization algorithm to preserve the update direction by adapting the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector']","['optimization algorithm training deep model affect convergence rate stability training process  also highly related generalization performance trained model ', 'adaptive algorithm  adam rmsprop  shown better optimization performance stochastic gradient descent  sgd  many scenario  often lead worse generalization performance sgd  used training deep neural network  dnns  ', 'work  identify two problem regarding direction step size updating weight vector hidden unit  may degrade generalization performance adam ', 'solution  propose normalized directionpreserving adam  ndadam  algorithm  control update direction step size precisely  thus bridge generalization gap adam sgd ', 'following similar rationale  improve generalization performance classification task regularizing softmax logits ', 'bridging gap sgd adam  also shed light certain optimization algorithm generalize better others ']","Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models., While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs)., In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam., As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD., Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits., By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.",18,5.785714285714286,9.333333333333334
29,"['Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.', 'However, autonomously learning effective sets of options is still a major challenge in the field.', 'In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.', 'Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.', 'We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  ', 'We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.', 'It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.', 'We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.1702127605676651, 0.2380952388048172, 0.21276594698429108, 0.22727271914482117, 0.2448979616165161, 0.2380952388048172, 0.1818181723356247, 0.13333332538604736]",Bk8ZcAxR-,"['We show how we can use the successor representation to discover eigenoptions in stochastic domains, from raw pixels. Eigenoptions are options learned to navigate the latent dimensions of a learned representation.', 'Extends the idea of eigenoptions to domains with stochastic transitions and where state features are learned.', 'Shows equivalence between proto value functions and successor representations and derives the idea of eigen options as a mechanism in option discovery', 'The paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions can be used to define options called eigenoptions.']","['option reinforcement learning allow agent hierarchically decompose task subtasks  potential speed learning planning ', 'however  autonomously learning effective set option still major challenge field ', 'paper focus recently introduced idea using representation learning method guide option discovery process ', 'specifically  look eigenoptions  option obtained representation encode diffusive information flow environment ', 'extend existing algorithm eigenoption discovery setting stochastic transition handcrafted feature available ', 'propose algorithm discovers eigenoptions learning nonlinear state representation raw pixel ', 'exploit recent success deep reinforcement learning literature equivalence protovalue function successor representation ', 'use traditional tabular domain provide intuition approach atari 2600 game demonstrate potential ']","Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning., However, autonomously learning effective sets of options is still a major challenge in the field., In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process., Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment., We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  , We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels., It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation., We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",12,6.153333333333333,12.5
30,"['One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.', 'We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.', 'However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.', 'In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.', 'In addition, we present a tighter upper bound that leverages network coefficients.', 'We test both on trained networks.', 'The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.', 'The refined upper bound is particularly stronger on networks with narrow layers.  ']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.19999998807907104, 0.4444444477558136, 0.27586206793785095, 0.4150943458080292, 0.09302325546741486, 0.10810810327529907, 0.3125, 0.09090908616781235]",B1MAJhR5YX,"['We provide improved upper bounds for the number of linear regions used in network expressivity, and an highly efficient algorithm (w.r.t. exact counting) to obtain probabilistic lower bounds on the actual number of linear regions.', 'Contributes to the study of the number of linear regions in RELU neural networks by using an approximate probabilistic counting algorithm and analysis', 'Builds off previous work studying the counting of linear regions in deep neural networks, and improves the upper bound previously proposed by changing the dimensionality constraint', 'The paper deals with expressiveness of a piecewise linear neural network, characterized by the number of linear regions of the function modeled, and leverages probabilistic algorithms to compute the bounds faster, and proves tighter bounds.']","['one form characterizing expressiveness piecewise linear neural network number linear region  piece  function modeled ', 'observed substantial progress topic lower upper bound maximum number linear region counting procedure ', 'however  bound account dimension network exact counting may take prohibitive amount time  therefore making infeasible benchmark expressiveness network ', 'work  approximate number linear region specific rectifier network algorithm probabilistic lower bound mixedinteger linear set ', 'addition  present tighter upper bound leverage network coefficient ', 'test trained network ', 'algorithm probabilistic lower bound several order magnitude faster exact counting value reach similar order magnitude  hence making approach viable method compare expressiveness network ', 'refined upper bound particularly stronger network narrow layer ']","One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled., We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure., However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks., In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets., In addition, we present a tighter upper bound that leverages network coefficients., We test both on trained networks., The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks., The refined upper bound is particularly stronger on networks with narrow layers.  ",15,5.433526011560693,11.533333333333333
31,"['The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision.', 'This critical faculty allows us to understand highly complex visual scenes.', 'Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene.', 'Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory.', 'We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory.', 'We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees.', 'The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer.', 'By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction.', 'Our model obtains highly competitive classification performance on MNIST and CIFAR-10.', 'As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation.', 'Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism).', ""Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.""]","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.0555555522441864, 0.06451612710952759, 0.19512194395065308, 0.3243243098258972, 0.4000000059604645, 0.08888888359069824, 0.277777761220932, 0.14999999105930328, 0.06451612710952759, 0.09756097197532654, 0.3913043439388275, 0.25]",B1fbosCcYm,"['A biologically inspired working memory that can be integrated in recurrent visual attention models for state of the art performance', 'Introduces a new network architecture inspired by visual attentive working memory and applies it to classification tasks and using it as a generative model', 'The paper augments the recurrent attention model with a novel Hebb-Rosenblatt working memory model and achieves competitive results on MNIST']","['ability look multiple time series poseadjusted glimpse fundamental human vision ', 'critical faculty allows u understand highly complex visual scene ', 'short term memory play integral role aggregating information obtained glimpse informing interpretation scene ', 'computational model attempted address glimpsing visual attention failed incorporate notion memory ', 'introduce novel  biologically inspired visual working memory architecture term hebbrosenblatt memory ', 'subsequently introduce fully differentiable short term attentive working memory model  stawm  us transformational attention learn memory image see ', 'state hebbrosenblatt memory embedded stawm weight space layer ', 'projecting different query layer obtain goaloriented latent representation task including classification visual reconstruction ', 'model obtains highly competitive classification performance mnist cifar10 ', 'demonstrated celeba dataset  perform reconstruction model learns make sequence update canvas constitute partsbased representation ', 'classification self supervised representation obtained mnist shown line state art model  none use visual attention mechanism  ', 'finally  show stawm trained dual constraint classification reconstruction provide interpretable visual sketchpad help open  blackbox  deep learning ']","The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision., This critical faculty allows us to understand highly complex visual scenes., Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene., Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory., We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory., We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees., The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer., By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction., Our model obtains highly competitive classification performance on MNIST and CIFAR-10., As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation., Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism)., Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.",15,5.8040816326530615,16.333333333333332
32,"['Generative models have been successfully applied to image style transfer and domain translation.', 'However, there is still a wide gap in the quality of results when learning such tasks on musical audio.', 'Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.', 'In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.', 'First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.', 'We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).', 'Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.', 'This allows a faster and more stable training, along with a controllable latent space encoder.', 'By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.', 'Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.', 'We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.', 'We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.']","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.05405404791235924, 0.0, 0.04651162400841713, 0.10256409645080566, 0.09756097197532654, 0.17777776718139648, 0.0833333283662796, 0.10526315122842789, 0.2916666567325592, 0.08163265138864517, 0.0952380895614624, 0.08510638028383255]",HJgOl3AqY7,"['The paper uses Variational Auto-Encoding and network conditioning for Musical Timbre Transfer, we develop and generalize our architecture for many-to-many instrument transfers together with visualizations and evaluations.', 'Proposes a Modulated Variational auto-Encoder to perform musical timbre transfer by replacing the usual adversarial translation criterion by a Maxiimum Mean Discrepancy', 'Describes a many-to-many model for musical timbre transfer which builds on recent developments in domain and style transfer', 'Proposes a hybrid VAE-based model to perform timbre transfer on recordings of musical instruments.']","['generative model successfully applied image style transfer domain translation ', 'however  still wide gap quality result learning task musical audio ', 'furthermore  translation model enable onetoone onetomany transfer relying separate encoders decoder complex  computationallyheavy model ', 'paper  introduce modulated variational autoencoders  move  perform musical timbre transfer ', 'first  define timbre transfer applying part auditory property musical instrument onto another ', 'show achieve improve task conditioning existing domain translation technique featurewise linear modulation  film  ', ' replacing usual adversarial translation criterion maximum mean discrepancy  mmd  objective  alleviate need auxiliary pair discriminative network ', 'allows faster stable training  along controllable latent space encoder ', 'conditioning system several different instrument  generalize manytomany transfer within single variational architecture able perform multidomain transfer ', 'model map input 3dimensional representation  successfully translating timbre one instrument another supporting sound synthesis reduced set control parameter ', 'evaluate method reconstruction generation task analyzing auditory descriptor distribution across transferred domain ', 'show architecture incorporates generative control multidomain transfer  yet remaining rather light  fast train effective small datasets ']","Generative models have been successfully applied to image style transfer and domain translation., However, there is still a wide gap in the quality of results when learning such tasks on musical audio., Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models., In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer., First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another., We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM)., Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks., This allows a faster and more stable training, along with a controllable latent space encoder., By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers., Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters., We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains., We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.",24,6.104166666666667,10.0
33,"['We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.', 'Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.', 'This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.', 'Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs approximate inference as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.', 'Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.', 'Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.', 'Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.']","[0, 1, 0, 0, 0, 0, 0]","[0.6341463327407837, 0.7083333134651184, 0.08695651590824127, 0.09836065024137497, 0.16326530277729034, 0.08695651590824127, 0.08510638028383255]",HJx54i05tX,"['We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena.', 'A theoretical analysis of autoencoders with weights tied between encoder and decoder (weight-tied) via mean field analysis', 'Analyses the performances of weighted tied auto-encoders by building on recent progress in analysis of high-dimensional statistics problems and specifically, the message passing algorithm', 'This paper studies auto-encoders under several assumptions, and points out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations.']","['study behavior weighttied multilayer vanilla autoencoders assumption random weight ', 'via exact characterization limit large dimension  analysis reveals interesting phase transition phenomenon depth becomes large ', ' particular  provides quantitative answer insight three question yet fully understood literature ', 'firstly  provide precise answer random deep weighttied autoencoder model performs  approximate inference  posed scellier et al   2018   connection reversibility considered several theoretical study ', 'secondly  show deep autoencoders display higher degree sensitivity perturbation parameter  distinct shallow counterpart ', 'thirdly  obtain insight pitfall training initialization practice  demonstrate experimentally possible train deep autoencoder  even tanh activation depth large 200 layer  without resorting technique layerwise pretraining batch normalization ', 'analysis specific depth lipschitz activation  analytical technique may broader applicability ']","We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights., Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large., This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature., Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs approximate inference as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies., Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts., Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization., Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.",19,5.876404494382022,8.9
34,"['Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).', 'Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and\n', 'kernel smoothing we construct a new generative model  Cramer-Wold AutoEncoder (CWAE).', 'CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.', 'As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.']","[0, 0, 1, 0, 0]","[0.13636362552642822, 0.6000000238418579, 0.7272727489471436, 0.19512194395065308, 0.06666666269302368]",rkgwuiA9F7,"['Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model  Cramer-Wold AutoEncoder (CWAE).', 'This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution', 'Introduces a variation on the Wasserstein AudoEncoders which is a novel regularized auto-encoder architecture that proposes a specific choice of the divergence penalty', 'This paper proposes the Cramer-Wold autoencoder, which uses the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.']","['assessing distance betweeen true sample distribution key component many state art generative model  wasserstein autoencoder  wae  ', 'inspired prior work slicedwasserstein autoencoders  swae ', 'kernel smoothing construct new generative model  cramerwold autoencoder  cwae  ', 'cwae cost function  based introduced cramerwold distance sample  simple closedform case normal prior ', 'consequence  simplifying optimization procedure  need sampling necessary evaluate distance function training loop   cwae performance match quantitatively qualitatively waemmd  wae using maximum mean discrepancy based distance function  often improves upon swae ']","Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE)., Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and
, kernel smoothing we construct a new generative model  Cramer-Wold AutoEncoder (CWAE)., CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior., As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.",10,5.900900900900901,11.1
35,"['We propose a rejection sampling scheme using the discriminator of a GAN to\n', 'approximately correct errors in the GAN generator distribution.', 'We show that\n', 'under quite strict assumptions, this will allow us to recover the data distribution\n', 'exactly.', 'We then examine where those strict assumptions break down and design a\n', 'practical algorithmcalled Discriminator Rejection Sampling (DRS)that can be\n', 'used on real data-sets.', 'Finally, we demonstrate the efficacy of DRS on a mixture of\n', 'Gaussians and on the state of the art SAGAN model.', 'On ImageNet, we train an\n', 'improved baseline that increases the best published Inception Score from 52.52 to\n', '62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79.', 'We then use\n', 'DRS to further improve on this baseline, improving the Inception Score to 76.08\n', 'and the FID to 13.75.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.6896551847457886, 0.23999999463558197, 0.09999999403953552, 0.13333332538604736, 0.13793103396892548, 0.0, 0.0952380895614624, 0.29629629850387573, 0.23076923191547394, 0.09090908616781235, 0.13793103396892548, 0.12903225421905518, 0.20000000298023224, 0.19999998807907104, 0.17391304671764374]",S1GkToR5tm,"['We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator.', ' Proposes a rejection sampling algorithm for sampling from the GAN generator.', 'This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling, to help filter good samples from GANs generator.']","['propose rejection sampling scheme using discriminator gan', 'approximately correct error gan generator distribution ', 'show', 'quite strict assumption  allow u recover data distribution', 'exactly ', 'examine strict assumption break design', 'practical algorithmcalled discriminator rejection sampling  drs ', 'used real datasets ', 'finally  demonstrate efficacy drs mixture', 'gaussians state art sagan model ', 'imagenet  train', 'improved baseline increase best published inception score 5252', '6236 reduces frechet inception distance 1865 1479 ', 'use', 'drs improve baseline  improving inception score 7608', 'fid 1375 ']","We propose a rejection sampling scheme using the discriminator of a GAN to
, approximately correct errors in the GAN generator distribution., We show that
, under quite strict assumptions, this will allow us to recover the data distribution
, exactly., We then examine where those strict assumptions break down and design a
, practical algorithmcalled Discriminator Rejection Sampling (DRS)that can be
, used on real data-sets., Finally, we demonstrate the efficacy of DRS on a mixture of
, Gaussians and on the state of the art SAGAN model., On ImageNet, we train an
, improved baseline that increases the best published Inception Score from 52.52 to
, 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79., We then use
, DRS to further improve on this baseline, improving the Inception Score to 76.08
, and the FID to 13.75.",20,5.181818181818182,6.6
36,"['The quality of the features used in visual recognition is of fundamental importance for the overall system.', 'For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition.', 'Visual features have recently been extracted from trained convolutional neural networks.', 'Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process.', 'In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.', 'This methodology is evaluated on different datasets and compared with state-of-the-art approaches.']","[0, 0, 1, 0, 0, 0]","[0.2222222238779068, 0.0, 0.43478259444236755, 0.0, 0.1764705777168274, 0.0]",SyGT_6yCZ,"['A simple fast method for extracting visual features from convolutional neural networks', 'Proposes a fast way to learn convolutional features that later can be used with any classifier by using reduced numbers of training epocs and specific schedule delays of learning rate', 'Use a learning rate decay scheme that is fixed relative to the number of epochs used in training and extract the penultimate layer output as features to train a conventional classifier.']","['quality feature used visual recognition fundamental importance overall system ', 'long time  lowlevel handdesigned feature algorithm sift hog obtained best result image recognition ', 'visual feature recently extracted trained convolutional neural network ', 'despite highquality result  one main drawback approach  compared handdesigned feature  training time required learning process ', 'paper  propose simple fast way train supervised convolutional model feature extraction still maintaining highquality ', 'methodology evaluated different datasets compared stateoftheart approach ']","The quality of the features used in visual recognition is of fundamental importance for the overall system., For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition., Visual features have recently been extracted from trained convolutional neural networks., Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process., In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality., This methodology is evaluated on different datasets and compared with state-of-the-art approaches.",11,5.908256880733945,9.909090909090908
37,"['We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).', 'We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.', 'The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.', 'First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.', 'Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.', 'Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.', 'Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.', 'In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.3870967626571655, 0.3125, 0.1249999925494194, 0.05882352590560913, 0.1395348757505417, 0.1249999925494194, 0.13793103396892548, 0.1249999925494194]",BJej72AqF7,"['We provide new insights and interpretations of RNNs from a max-affine spline operators perspective.', 'Rewrites equations of Elman RNN in terms of so-called max-affine spline operators', 'Provide a novel approach toward understanding RNNs using max-affline spline operators (MASO) by rewriting them with piecewise affine and convex activations MASOs', 'The authors build upon max-affine spline operator interpetation of a substantial class of deep networks, focusing on Recurrent Neural Networks using noise in initial hidden state acts as regularization']","['develop framework understanding improving recurrent neural network  rnns  using maxaffine spline operator  masos  ', 'prove rnns using piecewise affine convex nonlinearities written simple piecewise affine spline operator ', 'resulting representation provides several new perspective analyzing rnns  three study paper ', 'first  show rnn internally partition input space training build partition time ', 'second  show affine slope parameter rnn corresponds inputspecific template  interpret rnn performing simple template matching  matched filtering  given input ', 'third  carefully examining maso rnn affine mapping  prove using random initial hidden state corresponds explicit l2 regularization affine parameter  mollify exploding gradient improve generalization ', 'extensive experiment several datasets various modality demonstrate validate conclusion ', 'particular  using random initial hidden state elevates simple rnns near stateoftheart performer datasets ']","We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs)., We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator., The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper., First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time., Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input., Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization., Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions., In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.",16,5.701657458563536,11.3125
38,"['Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering.', ' Transducing text to logical forms which can be operated on is a brittle and error-prone process', '. Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly', '. These issues are addressed by Neural Theorem Provers (NTPs) (Rocktschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prologs backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations', '. In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namely', 'a) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, and', 'b) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.', 'Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.', 'The proposed method is able to extract rules and provide explanationsinvolving both textual patterns and KB relationsfrom large KBs and text corpora.', 'We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.22727271914482117, 0.15789473056793213, 0.22641508281230927, 0.12903225421905518, 0.1538461446762085, 0.10526315122842789, 0.21052631735801697, 0.1395348757505417, 0.1904761791229248, 0.1230769157409668]",BJzmzn0ctX,"['We scale Neural Theorem Provers to large datasets, improve the rule learning process, and extend it to jointly reason over text and Knowledge Bases.', 'Proposes an extension of the Neural Theorem Provers system that addresses the main issues of this model by reducing the time and space complexity of the model', 'Scales NTPs by using approximate nearest neighbour search over facts and rules during unification and suggests parameterizing predicates using attention over known predicates', 'improves upon the previously proposed Neural Theorem Prover approach by using nearest neighbor search.']","['reasoning text knowledge base  kb  major challenge artificial intelligence  application machine reading  dialogue  question answering ', 'transducing text logical form operated brittle errorprone process', ' operating directly text jointly learning representation transformation thereof mean neural architecture lack ability learn exploit general rule datainefficient generalise correctly', ' issue addressed neural theorem provers  ntps   rocktschel  riedel  2017   neurosymbolic system based continuous relaxation prolog  backward chaining algorithm  symbolic unification atom replaced differentiable operator computing similarity embedding representation', ' paper  first propose neighbourhoodapproximated neural theorem provers  nantps  consisting two extension tontps  namely', ' method drastically reducing previously prohibitive time space complexity inference learning ', 'b  attention mechanism improving rule learning process  deeming usable realworld datasets ', ' propose novel approach jointly reasoning kb fact textual mention  jointly embedding shared embedding space ', 'proposed method able extract rule provide explanationsinvolving textual pattern kb relationsfrom large kb text corpus ', 'show nantps perform par ntps fraction cost  achieve competitive link prediction result challenging largescale datasets  including wn18  wn18rr  fb15k237  without textual mention  able provide explanation prediction extract interpretable rule ']","Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering.,  Transducing text to logical forms which can be operated on is a brittle and error-prone process, . Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly, . These issues are addressed by Neural Theorem Provers (NTPs) (Rocktschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prologs backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations, . In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namely, a) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, and, b) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets., Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space., The proposed method is able to extract rules and provide explanationsinvolving both textual patterns and KB relationsfrom large KBs and text corpora., We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.",26,5.8830188679245285,9.137931034482758
39,"[""We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data."", 'Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.', 'We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.', 'Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.', 'Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.', 'We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.', 'Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.', 'This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.4923076927661896, 0.29411762952804565, 0.1666666567325592, 0.14035087823867798, 0.23076923191547394, 0.11538460850715637, 0.14084506034851074, 0.35087719559669495]",HyFaiGbCW,"['Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using an explainable dynamical systems model, Reservoir Computing, and a biologically plausible learning technique based on analogies.', 'Claims results of ""combining transformations"" in the context of RC by using an echo-state network with standard tanh acctivations with the difference that recurrent weights are not trained', 'Novel method of classifying different distorions of MNIST data', 'The paper uses an echo state network to learn to classify image transformations between pairs of images into one of fives classes.']","['investigate method reservoir computing network  rcn  learns concept similar  different  pair image using small training dataset generalizes concept previously unseen type data ', 'specifically  show rcn trained identify relationship imagepairs drawn subset digit mnist database depth map subset visual scene moving camera generalizes learned transformation image digit unseen training depth map different visual scene ', 'infer  using principal component analysis  high dimensional reservoir state generated input image pair specific transformation converge time unique relationship ', 'thus  opposed training entire high dimensional reservoir state  reservoir need train unique relationship  allowing reservoir perform well training example ', 'thus  generalization learning unseen image interpretable term clustering reservoir state onto attractor corresponding transformation reservoir space ', 'find rcns identify generalize linear nonlinear transformation  combination transformation  naturally robust effective image classifier ', 'additionally  rcns perform significantly better state art neural network classification technique deep siamese neural network  snns  generalization task mnist dataset complex depth map visual scene moving camera ', 'work help bridge gap explainable machine learning biological learning analogy using small datasets  point new direction investigation learning process ']","We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data., Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes., We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship., Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples., Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space., We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier., Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera., This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.",19,5.673992673992674,14.368421052631579
40,"['We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.', 'GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset.', 'Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.', 'We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.', 'We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.']","[1, 0, 0, 0, 0]","[0.8205128312110901, 0.13333332538604736, 0.09999999403953552, 0.19512194395065308, 0.178571417927742]",H1xAH2RqK7,"['We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations with certified privacy/fairness guarantees', 'This paper uses a GAN model to provide an overview of the related work to Private/Fair Representation Learning (PRL).', 'This paper presents an adversarial-based approach for private and fair representations by learned distortion of data that minimises the dependency on sensitive variables while the degree of distortion is constrained.', 'The authors describe a framework of how to learn a demographic parity representation that can be used to train certain classifiers.']","['present generative adversarial privacy fairness  gapf   datadriven framework learning private fair representation data ', 'gapf leverage recent advance adversarial learning allow data holder learn  universal  representation decouple set sensitive attribute rest dataset ', 'gapf  finding optimal decorrelation scheme formulated constrained minimax game generative decorrelator adversary ', 'show appropriately chosen adversarial loss function  gapf provides privacy guarantee strong informationtheoretic adversary enforces demographic parity ', 'also evaluate performance gapf multidimensional gaussian mixture model real datasets  show designer certify representation learned adversary fixed architecture perform well complex adversary ']","We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data., GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset., Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary., We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity., We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.",9,6.155038759689923,14.333333333333334
41,"['Current machine learning algorithms can be easily fooled by adversarial examples.', 'One possible solution path is to make models that use confidence thresholding to avoid making mistakes.', 'Such models refuse to make a prediction when they are not confident of their answer.', 'We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.', ""Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability."", 'We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.', 'Experiments show the attack attains good results in practice.', 'We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  ', 'We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.13793103396892548, 0.24242423474788666, 0.060606054961681366, 0.2380952388048172, 0.277777761220932, 0.19512194395065308, 0.07407406717538834, 0.16326530277729034, 0.07999999821186066]",H1g0piA9tQ,"['We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding', 'This paper introduces a family of attack on confidence thresholding algortihms, focusing mainly on evaluation methodologies.', 'Proposes an evaluation method for confidence thresholding defense models and an approach for generating adversarial examples by choosing the wrong class with the most confidence when using targeted attacks', 'The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack.']","['current machine learning algorithm easily fooled adversarial example ', 'one possible solution path make model use confidence thresholding avoid making mistake ', 'model refuse make prediction confident answer ', 'propose evaluate model term tradeoff curve goal high success rate clean example low failure rate adversarial example ', 'existing untargeted attack developed model use confidence thresholding tend underestimate model  vulnerability ', 'propose maxconfidence family attack  optimal variety theoretical setting  including one realistic setting  attack linear model ', 'experiment show attack attains good result practice ', 'show simple defense able perform well mnist cifar  contributing previous call mnist retired benchmarking dataset adversarial robustness research ', 'release code evaluation part cleverhans  papernot et al 2018  library  iclr reviewer careful look contributed feature cleverhans avoid deanonymizing submission  ']","Current machine learning algorithms can be easily fooled by adversarial examples., One possible solution path is to make models that use confidence thresholding to avoid making mistakes., Such models refuse to make a prediction when they are not confident of their answer., We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples., Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability., We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models., Experiments show the attack attains good results in practice., We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  , We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).",12,5.49738219895288,15.916666666666666
42,"['Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.', 'However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.', 'This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.', 'It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.', 'We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.', 'Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.', 'We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.', 'Each task provides only binary rewards indicating whether or not the goal is achieved.', 'Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.', 'Extensive experiments demonstrate that this method leads to faster converge and improved task performance.']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.06896550953388214, 0.19999998807907104, 0.0, 0.2631579041481018, 0.2631579041481018, 0.07407406717538834, 0.11764705181121826, 0.0, 0.22857142984867096, 0.1599999964237213]",Sklsm20ctX,"['a novel method to learn with sparse reward using adversarial reward re-labeling', 'Proposes to use a competitive multi-agent setting for encouraging exploration and shows that CER + HER > HER ~ CER', 'Propose a new method for learning from sparse rewards in model-free reinforcement learning settings and densifying reward', 'To address the sparse reward problems and encourage exploration in RL algorithms, the authors propose a relabeling strategy called Competitive Experience Reply (CER).']","['deep learning achieved remarkable success solving challenging reinforcement learning  rl  problem dense reward function provided ', 'however  sparse reward environment still often suffers need carefully shape reward function guide policy optimization ', 'limit applicability rl real world since reinforcement learning domainspecific knowledge required ', 'therefore great practical importance develop algorithm learn binary signal indicating successful task completion unshaped  sparse reward signal ', 'propose novel method called competitive experience replay  efficiently supplement sparse reward placing learning context exploration competition pair agent ', 'method complement recently proposed hindsight experience replay   inducing automatic exploratory curriculum ', 'evaluate approach task reaching various goal location ant maze manipulating object robotic arm ', 'task provides binary reward indicating whether goal achieved ', 'method asymmetrically augments sparse reward pair agent learning task  creating competitive game designed drive exploration ', 'extensive experiment demonstrate method lead faster converge improved task performance ']","Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided., However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization., This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required., It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals., We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents., Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum., We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm., Each task provides only binary rewards indicating whether or not the goal is achieved., Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration., Extensive experiments demonstrate that this method leads to faster converge and improved task performance.",14,5.836538461538462,14.857142857142858
43,"['This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.', 'The model is formulated as a conditional generative model with two levels of hierarchical latent variables.', 'The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.', 'The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.', 'This amounts to using a Gaussian mixture model (GMM) for the latent distribution.', 'Extensive evaluation demonstrates its ability to control the aforementioned attributes.', 'In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.']","[0, 0, 0, 1, 0, 0, 0]","[0.23076923191547394, 0.24242423474788666, 0.11428570747375488, 0.3333333432674408, 0.19354838132858276, 0.0714285671710968, 0.05405404791235924]",rygkk305YQ,"['Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.', 'Describes the conditioned GAN model to generate speaker conditioned Mel spectra by augmenting the z-space corresponding to the identification', 'This paper proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitating fine-grained control over various attributes', 'This paper proposes a model that can control non-annotated attributes such as speaking style, accent, background noise, etc.']","['paper proposes neural endtoend texttospeech  tt  model control latent attribute generated speech rarely annotated training data  speaking style  accent  background noise  recording condition ', 'model formulated conditional generative model two level hierarchical latent variable ', 'first level categorical variable  represents attribute group  eg  cleannoisy  provides interpretability ', 'second level  conditioned first  multivariate gaussian variable  characterizes specific attribute configuration  eg  noise level  speaking rate  enables disentangled finegrained control attribute ', 'amount using gaussian mixture model  gmm  latent distribution ', 'extensive evaluation demonstrates ability control aforementioned attribute ', 'particular  capable consistently synthesizing highquality clean speech regardless quality training data target speaker ']","This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions., The model is formulated as a conditional generative model with two levels of hierarchical latent variables., The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability., The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes., This amounts to using a Gaussian mixture model (GMM) for the latent distribution., Extensive evaluation demonstrates its ability to control the aforementioned attributes., In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.",17,6.215277777777778,7.578947368421052
44,"['Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.', 'We identify a fundamental problem due to soft attention in these models as a cause.', 'To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.', 'Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.', 'On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.']","[1, 0, 0, 0, 0]","[0.29629629850387573, 0.1538461446762085, 0.13793103396892548, 0.043478257954120636, 0.06451612710952759]",B12Js_yRb,"['Enabling Visual Question Answering models to count by handling overlapping object proposals.', 'This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count.', 'Focuses on a counting problem in visual question answering using attention mechanism and propsoe a differentiable counting compoent which explicitly counts the number of objects', 'This paper tackles the object counting problem in visual question answering, it proposes many heuristics to find the correct count.']","['visual question answering  vqa  model struggled counting object natural image far ', 'identify fundamental problem due soft attention model cause ', 'circumvent problem  propose neural network component allows robust counting object proposal ', 'experiment toy task show effectiveness component obtain stateoftheart accuracy number category vqa v2 dataset without negatively affecting category  even outperforming ensemble model single model ', 'difficult balanced pair metric  component give substantial improvement counting strong baseline 66  ']","Visual Question Answering (VQA) models have struggled with counting objects in natural images so far., We identify a fundamental problem due to soft attention in these models as a cause., To circumvent this problem, we propose a neural network component that allows robust counting from object proposals., Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model., On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.",8,5.514285714285714,13.125
45,"['We propose a simple and robust training-free approach for building sentence representations.', 'Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.', 'We model the semantic meaning of a word in a sentence based on two aspects.', 'One is its relatedness to the word vector subspace already spanned by its contextual words.', 'The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  ', 'Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.', 'This approach requires zero training and zero parameters, along with efficient inference performance.', 'We evaluate our approach on 11 downstream NLP tasks.', 'Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.']","[0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.3333333432674408, 0.12765957415103912, 0.10526315122842789, 0.0, 0.0, 0.09090908616781235, 0.277777761220932, 0.060606054961681366, 0.35087719559669495]",rJedbn0ctQ,"['A simple and training-free approach for sentence embeddings with competitive performance compared with sophisticated models requiring either large amount of training data or prolonged training time.', 'Presented a new training-free way of generating sentence embedding with systematic analysis', 'Proposes a new geometry-based method for sentence embedding from word embedding vectors by quantifying the novelty, significance, and corpus-uniqueness of each word', 'This paper explores sentence embedding based on orthogonal decomposition of the spanned space by word embeddings']","['propose simple robust trainingfree approach building sentence representation ', 'inspired gramschmidt process geometric theory  build orthogonal basis subspace spanned word surrounding context sentence ', 'model semantic meaning word sentence based two aspect ', 'one relatedness word vector subspace already spanned contextual word ', 'novel semantic meaning shall introduced new basis vector perpendicular existing subspace ', 'following motivation  develop innovative method based orthogonal basis combine pretrained word embeddings sentence representation ', 'approach requires zero training zero parameter  along efficient inference performance ', 'evaluate approach 11 downstream nlp task ', 'experimental result show model outperforms existing zerotraining alternative task competitive approach relying either large amount labelled data prolonged training time ']","We propose a simple and robust training-free approach for building sentence representations., Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence., We model the semantic meaning of a word in a sentence based on two aspects., One is its relatedness to the word vector subspace already spanned by its contextual words., The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  , Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation., This approach requires zero training and zero parameters, along with efficient inference performance., We evaluate our approach on 11 downstream NLP tasks., Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.",12,5.5602409638554215,13.833333333333334
46,"['In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.', 'Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.', 'In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.', 'We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.', 'To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.', 'These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.', 'We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.', 'We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.', 'Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.20512819290161133, 0.037735845893621445, 0.14999999105930328, 0.2800000011920929, 0.7755101919174194, 0.2702702581882477, 0.39024388790130615, 0.22857142984867096, 0.2380952388048172]",HJcSzz-CZ,"['We propose novel extensions of Prototypical Networks that are augmented with the ability to use unlabeled examples when producing prototypes.', 'This paper is an extension of a prototypical network that considers employing the unlabeled examples available to help train each episode', 'Studies the problem of semi-supervised few-shot classification by extending the prototypical networks into the setting of semi-supervised learning with example from distractor classes', 'Extends the Prototypical Network to the semi-supervised setting by updating prototypes using assigned pseudo-labels, dealing with distractors, and weighing samples using distance to the original prototypes.']","['fewshot classification  interested learning algorithm train classifier handful labeled example ', 'recent progress fewshot classification featured metalearning  parameterized model learning algorithm defined trained episode representing different classification problem  small labeled training set corresponding test set ', 'work  advance fewshot classification paradigm towards scenario unlabeled example also available within episode ', 'consider two situation  one unlabeled example assumed belong set class labeled example episode  well challenging situation example distractor class also provided ', 'address paradigm  propose novel extension prototypical network  snell et al  2017  augmented ability use unlabeled example producing prototype ', 'model trained endtoend way episode  learn leverage unlabeled example successfully ', 'evaluate method version omniglot miniimagenet benchmark  adapted new framework augmented unlabeled example ', 'also propose new split imagenet  consisting large set class  hierarchical structure ', 'experiment confirm prototypical network learn improve prediction due unlabeled example  much like semisupervised algorithm would ']","In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples., Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set., In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode., We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided., To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes., These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully., We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples., We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure., Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.",21,5.64935064935065,11.0
47,"['We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.', 'Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space.', 'We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations.', 'We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean.', 'We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.']","[1, 0, 0, 0, 0]","[0.3125, 0.1599999964237213, 0.21621620655059814, 0.17777776718139648, 0.1249999925494194]",SyMhLo0qKQ,"['We theoretically prove that linear interpolations are unsuitable for analysis of trained implicit generative models. ', 'Studies the problem of when the linear interpolant between two random variables follows the same distribution, related to prior distribution of an implicit generative model', 'This work asks how to interpolate in the latent space given a latent variable model.']","['investigate property multidimensional probability distribution context latent space prior distribution implicit generative model ', 'work revolves around phenomenon arising decoding linear interpolation two random latent vector  region latent space close proximity origin space oversampled  restricts usability linear interpolation tool analyse latent space ', 'show distribution mismatch eliminated completely proper choice latent probability distribution using nonlinear interpolation ', 'prove trade interpolation linear  latent distribution even basic property required stable training  finite mean ', 'use multidimensional cauchy distribution example prior distribution  also provide general method creating nonlinear interpolation  easily applicable large family commonly used latent distribution ']","We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models., Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space., We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations., We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean., We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.",10,5.713375796178344,15.7
48,"['Deep neural networks (DNN) have shown promising performance in computer vision.', 'In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.', 'Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).', 'The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.', 'Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.', 'In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.', 'To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.', 'The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.', ""The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations."", 'The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.', 'A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.', 'The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.']","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.0, 0.0, 0.1111111044883728, 0.0, 0.19354838132858276, 0.0, 0.08163265138864517, 0.06666666269302368, 0.20000000298023224, 0.0, 0.11428570747375488, 0.14814814925193787]",rk1FQA0pW,"['Detection of lung nodule starting from projection data rather than images.', 'DNNs are used for patch-based lung nodule detection in CT projection data.', 'Jointly modeling computed tomography reconstruction and lesion detection in the lung by training the mapping from raw sinogram to detection outputs in an end-to-end manner', 'Presents an end to end training of a CNN architecture that combines CT image signal processing and image analysis.']","['deep neural network  dnn  shown promising performance computer vision ', 'medical imaging  encouraging result achieved deep learning application segmentation  lesion detection classification ', 'nearly deep learning based image analysis method work reconstructed image  obtained original acquisition via solving inverse problem  reconstruction  ', 'reconstruction algorithm designed human observer  necessarily optimized dnns often observe feature incomprehensible human eye ', 'hence  desirable train dnns directly original data lie different domain image ', 'paper  proposed endtoend dnn abnormality detection medical imaging ', 'align acquisition annotation made radiologist image domain  dnn built unrolled version iterative reconstruction algorithm map acquisition image  followed 3d convolutional neural network  cnn  detect abnormality reconstructed image ', 'two network trained jointly order optimize entire dnn detection task original acquisition ', 'dnn implemented lung nodule detection lowdose chest computed tomography  ct   numerical simulation done generate acquisition 1018 chest ct image radiologist  annotation ', 'proposed endtoend dnn demonstrated better sensitivity accuracy task compared twostep approach  reconstruction detection dnns trained separately ', 'significant reduction false positive rate suspicious lesion observed  crucial known overdiagnosis lowdose lung ct imaging ', 'image reconstructed proposed endtoend network also presented enhanced detail region interest ']","Deep neural networks (DNN) have shown promising performance in computer vision., In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification., Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction)., The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes., Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images., In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging., To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images., The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions., The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations., The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately., A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging., The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.",23,5.730769230769231,12.434782608695652
49,"['Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.', 'As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to learn to navigate and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments.', 'Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal.', 'In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?', 'Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.', 'Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.', 'We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments.', 'Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal.', 'However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.']","[0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.2222222238779068, 0.2461538463830948, 0.145454540848732, 0.20408162474632263, 0.1666666567325592, 0.17391303181648254, 0.19230768084526062, 0.11538460850715637, 0.19230768084526062]",BkiIkBJ0b,"['We quantitatively and qualitatively evaluate deep reinforcement learning based navigation methods under a variety of conditions to answer the question of how close they are to replacing classical path planners and mapping algorithms.', 'Evaluate a Deep RL-based model on training mazes by measuring repeated latency to goal and comparison to shortest route']","['deep reinforcement learning  drl  algorithm demonstrated progress learning find goal challenging environment ', 'title paper mirowski et al   2016  suggests  one might assume drlbased algorithm able  learn navigate  thus ready replace classical mapping pathplanning algorithm  least simulated environment ', 'yet  experiment analysis earlier work  clear strategy used algorithm navigating maze finding goal ', 'paper  pose study underlying question  drl algorithm form mapping andor pathplanning ', 'experiment show algorithm memorizing map maze testing stage  rather  training stage ', 'hence  drl algorithm fall short qualifying mapping pathplanning algorithm reasonable definition mapping ', 'extend experiment mirowski et al   2016  separating set training testing map ablative coverage space experiment ', 'systematic experiment show nava3cd1d2l algorithm  trained tested map  able choose shorter path goal ', 'however  tested unseen map algorithm utilizes wallfollowing strategy find goal without mapping path planning ']","Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments., As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to learn to navigate and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments., Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal., In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?, Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage., Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping., We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments., Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal., However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.",20,5.2,10.227272727272727
50,"['In many robotic applications, it is crucial to maintain a belief about the state of \n', 'a system, like the location of a robot or the pose of an object.\n', 'These state estimates serve as input for planning and decision making and \n', 'provide feedback during task execution. \n', 'Recursive Bayesian Filtering algorithms address the state estimation problem,\n', 'but they require a model of the process dynamics and the sensory observations as well as \n', 'noise estimates that quantify the accuracy of these models. \n', 'Recently, multiple works have demonstrated that the process and sensor models can be \n', 'learned by end-to-end training through differentiable versions of Recursive Filtering methods.\n', 'However, even if the predictive models are known, finding suitable noise models \n', 'remains challenging.', 'Therefore, many practical applications rely on very simplistic noise \n', 'models. \n', 'Our hypothesis is that end-to-end training through differentiable Bayesian \n', 'Filters enables us to learn more complex heteroscedastic noise models for\n', 'the system dynamics.', 'We evaluate learning such models with different types of \n', 'filtering algorithms and on two different robotic tasks.', 'Our experiments show that especially \n', 'for sampling-based filters like the Particle Filter, learning heteroscedastic noise \n', 'models can drastically improve the tracking performance in comparison to using \n', 'constant noise models.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761791229248, 0.0833333283662796, 0.0, 0.1818181723356247, 0.09999999403953552, 0.0, 0.3636363446712494, 0.0, 0.5, 0.10526315122842789, 0.0, 0.2857142686843872, 0.09090908616781235, 0.2857142686843872]",BylBns0qtX,"['We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters', 'Proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-toend through differentiable Bayesian Filters and two different versions of the Unscented Kalman Filter', 'Revisits Bayes filters and evaluates the benefit of training the observation and process noise models while keeping all other models fixed', 'This paper presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms. The approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated diagonal covariance.']","['many robotic application  crucial maintain belief state', 'system  like location robot pose object ', 'state estimate serve input planning decision making', 'provide feedback task execution ', 'recursive bayesian filtering algorithm address state estimation problem ', 'require model process dynamic sensory observation well', 'noise estimate quantify accuracy model ', 'recently  multiple work demonstrated process sensor model', 'learned endtoend training differentiable version recursive filtering method ', 'however  even predictive model known  finding suitable noise model', 'remains challenging ', 'therefore  many practical application rely simplistic noise', 'model ', 'hypothesis endtoend training differentiable bayesian', 'filter enables u learn complex heteroscedastic noise model', 'system dynamic ', 'evaluate learning model different type', 'filtering algorithm two different robotic task ', 'experiment show especially', 'samplingbased filter like particle filter  learning heteroscedastic noise', 'model drastically improve tracking performance comparison using', 'constant noise model ']","In many robotic applications, it is crucial to maintain a belief about the state of 
, a system, like the location of a robot or the pose of an object.
, These state estimates serve as input for planning and decision making and 
, provide feedback during task execution. 
, Recursive Bayesian Filtering algorithms address the state estimation problem,
, but they require a model of the process dynamics and the sensory observations as well as 
, noise estimates that quantify the accuracy of these models. 
, Recently, multiple works have demonstrated that the process and sensor models can be 
, learned by end-to-end training through differentiable versions of Recursive Filtering methods.
, However, even if the predictive models are known, finding suitable noise models 
, remains challenging., Therefore, many practical applications rely on very simplistic noise 
, models. 
, Our hypothesis is that end-to-end training through differentiable Bayesian 
, Filters enables us to learn more complex heteroscedastic noise models for
, the system dynamics., We evaluate learning such models with different types of 
, filtering algorithms and on two different robotic tasks., Our experiments show that especially 
, for sampling-based filters like the Particle Filter, learning heteroscedastic noise 
, models can drastically improve the tracking performance in comparison to using 
, constant noise models.",29,5.842639593908629,6.793103448275862
51,"['Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning.', 'These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data.', 'However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning.', 'In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes.', 'DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections.', ""These connections are added based on a node's relationship to its ancestors and descendants."", 'A weighting scheme is further used to weigh their contribution depending on the distance to the node.', 'Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.1304347813129425, 0.16949151456356049, 0.19999998807907104, 0.2769230604171753, 0.17777776718139648, 0.17777776718139648, 0.1304347813129425, 0.12244897335767746]",rkgs0oAqFQ,"['We rethink the way information can be exploited more efficiently in the knowledge graph in order to improve performance on the Zero-Shot Learning task and propose a dense graph propagation (DGP) module for this purpose.', 'This authors propose a solution to the problem of over-smoothing in Graph conv networks by allowing dense propagation between all related nodes, weighted by the mutual distance.', 'Proposes a novel graph convolutional neural network to tackle the problem of zero-shot classification by using relational structures between classes as input of graph convolutional networks to learn classifiers of unseen classes']","['graph convolutional neural network recently shown great potential task zeroshot learning ', 'model highly sample efficient related concept graph structure share statistical strength allowing generalization new class faced lack data ', 'however  find extensive use laplacian smoothing layer current approach easily dilute knowledge distant node consequently decrease performance zeroshot learning ', 'order still enjoy benefit brought graph structure preventing dilution knowledge distant node  propose dense graph propagation  dgp  module carefully designed direct link among distant node ', 'dgp allows u exploit hierarchical graph structure knowledge graph additional connection ', 'connection added based node relationship ancestor descendant ', 'weighting scheme used weigh contribution depending distance node ', 'combined finetuning representation twostage training approach method outperforms stateoftheart zeroshot learning approach ']","Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning., These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data., However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning., In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes., DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections., These connections are added based on a node's relationship to its ancestors and descendants., A weighting scheme is further used to weigh their contribution depending on the distance to the node., Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.",10,5.785310734463277,17.7
52,"['In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.', 'By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.', 'We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.', 'Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.', 'Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.\n']","[0, 1, 0, 0, 0]","[0.1764705777168274, 0.3720930218696594, 0.05405404791235924, 0.25531914830207825, 0.09999999403953552]",H1xpe2C5Km,"['A capsule-based semantic segmentation, in which the probabilities of the class labels are traced back through capsule pipeline. ', 'The authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes', 'Proposes a traceback layer for capsule networks to do semantic segmentation and makes explicit use of part-whole relationship in the capsule layers', 'Proposes a trace-back method based on the CapsNet concept of Sabour to perform a semantic segmentation in parallel to classification.']","['paper  propose capsulebased neural network model solve semantic segmentation problem ', 'taking advantage extractable partwhole dependency available capsule layer  derive probability class label individual capsule recursive  layerbylayer procedure ', 'model procedure traceback pipeline take central piece build endtoend segmentation network ', 'proposed framework  imagelevel class label object boundary jointly sought explicit manner  pose significant advantage stateoftheart fully convolutional network  fcn  solution ', 'experiment conducted modified mnist neuroimages demonstrate model considerably enhance segmentation performance compared leading fcn variant ']","In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem., By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure., We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network., Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions., Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.
",10,6.119658119658119,11.7
53,"['Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. \n', 'Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.', 'In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.', 'Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.', 'First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.', 'Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.', 'Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.']","[0, 0, 0, 0, 0, 1, 0]","[0.24561403691768646, 0.11764705181121826, 0.27586206793785095, 0.17241378128528595, 0.23529411852359772, 0.34375, 0.25806450843811035]",SkMON20ctX,"['We look at SGD as a trajectory in the space of probability measures, show its connection to Markov processes, propose a simple Markov model of SGD learning, and experimentally compare it with SGD using information theoretic quantities. ', 'Constructs a Markov chain that follows a shorted path in TV metric on P and shows that trajectories of SGD and \\alpha-SMLC have similar conditional entropy', 'Studies the trajectory of H(\\hat{y}) versus H(\\hat{y}|y) on the information plane for stochastic gradient descent methods for training neural networks', ""Describes SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction.""]","['studying evolution information theoretic quantity stochastic gradient descent  sgd  learning artificial neural network  anns  gained popularity recent year ', 'nevertheless  type experiment require estimating mutual information entropy becomes intractable moderately large problem ', 'work propose framework understanding sgd learning information plane consists observing entropy conditional entropy output label ann ', 'experimental result theoretical justification shown  assumption  sgd learning trajectory appear similar different ann architecture ', 'first  sgd learning modeled hidden markov process  hmp  whose entropy tends increase maximum ', ' shown sgd learning trajectory appears move close shortest path initial final joint distribution space probability measure equipped total variation metric ', 'furthermore  shown trajectory learning information plane provide alternative observing learning process  potentially richer information learning trajectory training test error ']","Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. 
, Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems., In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN., Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures., First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum., Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric., Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.",14,5.777777777777778,13.5
54,"['Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling.', 'However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition.', 'This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.', 'The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes.', 'Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.']","[0, 0, 1, 0, 0]","[0.052631575614213943, 0.1818181723356247, 0.2631579041481018, 0.052631575614213943, 0.11999999731779099]",HkeoOo09YX,"['This paper proposes a method to automate the design of stochastic gradient MCMC proposal using meta learning approach. ', 'Prsents a meta-learning approach to automatically design MCMC sampler based on Hamiltonian dynamics to mix faster on problems similar to training problems', 'Parameterizes diffusion and curl matrices by neural networks and meta-learn and optimize an sg-mcmc algorithm. ']","['stochastic gradient markov chain monte carlo  sgmcmc  become increasingly popular simulating posterior sample largescale bayesian modeling ', 'however  existing sgmcmc scheme tailored specific probabilistic model  even simple modification underlying dynamical system requires significant physical intuition ', 'paper present first metalearning algorithm allows automated design underlying continuous dynamic sgmcmc sampler ', 'learned sampler generalizes hamiltonian dynamic statedependent drift diffusion  enabling fast traversal efficient exploration energy landscape ', 'experiment validate proposed approach bayesian fully connected neural network  bayesian convolutional neural network bayesian recurrent neural network task  showing learned sampler outperforms generic  handdesigned sgmcmc algorithm  generalizes different datasets larger architecture ']","Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling., However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition., This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler., The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes., Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.",12,6.902439024390244,10.25
55,"['We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature.', 'Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions.', 'We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function.', 'We then provide a set of systematic experiments that explore its hyper-parameters.', ""We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned."", 'We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.']","[0, 1, 0, 0, 0, 0]","[0.17142856121063232, 0.23529411852359772, 0.05882352590560913, 0.07999999821186066, 0.045454543083906174, 0.0]",ryzm6BATZ,"['Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks', 'Proposes an energy-based formulation to the BEGAN modeal and modifies it to include an image quality assessment based term', 'Proposes some new energy function in the BEGAN (boundary equilibrium GAN framework), including l_1 score, Gradient magnitude similarity score, and chrominance score.']","['propose new  multicomponent energy function energybased generative adversarial network  gans  based method image quality assessment literature ', 'approach expands boundary equilibrium generative adversarial network  began  outlining shortcoming original energy loss function ', 'address shortcoming incorporating l1 score  gradient magnitude similarity score  chrominance score new energy function ', 'provide set systematic experiment explore hyperparameters ', 'show energy function component able represent slightly different set feature  require evaluation criterion ass whether adequately learned ', 'show model using new energy function able produce better image representation began model predicted way ']","We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature., Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions., We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function., We then provide a set of systematic experiments that explore its hyper-parameters., We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned., We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.",10,5.737226277372263,13.7
56,"['Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.', 'Its performance depends crucially on a damping coefficient.', 'Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.', 'We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.', 'AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.', ""We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives."", 'Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.']","[0, 0, 0, 1, 0, 0, 0]","[0.260869562625885, 0.11764705181121826, 0.07547169178724289, 0.3181818127632141, 0.0833333283662796, 0.17777776718139648, 0.2800000011920929]",Syxt5oC5YQ,"['We introduce a simple variant of momentum optimization which is able to outperform classical momentum, Nesterov, and Adam on deep learning tasks with minimal hyperparameter tuning.', 'Introduces a variant of momentum that aggregates several velocities with different dampening coefficients that significantly decreases oscillation', 'Proposed an aggregated momentum methods for gradient based optimization by using multiple velocity vectors with different damping factors instead of a single velocity vector to improve stability', 'The authors combine several update steps together to achieve aggregated momentum also demonstrating it is more stable than the other momentum methods']","['momentum simple widely used trick allows gradientbased optimizers pick speed along low curvature direction ', 'performance depends crucially damping coefficient ', 'largecamping coefficient potentially deliver much larger speedup  prone oscillation instability  hence one typically resort small value 05 09 ', 'propose aggregated momentum  aggmo   variant momentum combine multiple velocity vector different damping coefficient ', 'aggmo trivial implement  significantly dampens oscillation  enabling remain stable even aggressive damping coefficient 0999 ', 'reinterpret nesterov accelerated gradient descent special case aggmo analyze rate convergence quadratic objective ', 'empirically  find aggmo suitable dropin replacement momentum method  frequently delivers faster convergence little tuning ']","Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions., Its performance depends crucially on a damping coefficient., Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9., We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients., AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999., We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives., Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.",13,5.9640287769784175,10.692307692307692
57,"['Recurrent Neural Networks architectures excel at processing sequences by\n', 'modelling dependencies over different timescales.', 'The recently introduced\n', 'Recurrent Weighted Average (RWA) unit captures long term dependencies\n', 'far better than an LSTM on several challenging tasks.', 'The RWA achieves\n', 'this by applying attention to each input and computing a weighted average\n', 'over the full history of its computations.', 'Unfortunately, the RWA cannot\n', 'change the attention it has assigned to previous timesteps, and so struggles\n', 'with carrying out consecutive tasks or tasks with changing requirements.\n', 'We present the Recurrent Discounted Attention (RDA) unit that builds on\n', 'the RWA by additionally allowing the discounting of the past.\n', 'We empirically compare our model to RWA, LSTM and GRU units on\n', 'several challenging tasks.', 'On tasks with a single output the RWA, RDA and\n', 'GRU units learn much quicker than the LSTM and with better performance.\n', 'On the multiple sequence copy task our RDA unit learns the task three\n', 'times as quickly as the LSTM or GRU units while the RWA fails to learn at\n', 'all.', 'On the Wikipedia character prediction task the LSTM performs best\n', 'but it followed closely by our RDA unit.', 'Overall our RDA unit performs\n', 'well and is sample efficient on a large variety of sequence tasks.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.07999999821186066, 0.0, 0.0, 0.07999999821186066, 0.0, 0.0, 0.1428571343421936, 0.08695651590824127, 0.09999999403953552, 0.2142857164144516, 0.0, 0.29629629850387573, 0.07999999821186066, 0.1428571343421936, 0.0, 0.07692307233810425, 0.06896550953388214, 0.14814814925193787, 0.13333332538604736, 0.07999999821186066, 0.0, 0.0, 0.0714285671710968]",BJ78bJZCZ,"['We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time', 'This paper proposes the Recurrent Discounted Attention (RDA), an extension to Recurrent Weighted Average (RWA) by adding a discount factor.', 'Extends the recurrent weight average to overcome the limitation of the original method while maintaining its advantage and proposes the method of using Elman nets as the base RNN']","['recurrent neural network architecture excel processing sequence', 'modelling dependency different timescales ', 'recently introduced', 'recurrent weighted average  rwa  unit capture long term dependency', 'far better lstm several challenging task ', 'rwa achieves', 'applying attention input computing weighted average', 'full history computation ', 'unfortunately  rwa', 'change attention assigned previous timesteps  struggle', 'carrying consecutive task task changing requirement ', 'present recurrent discounted attention  rda  unit build', 'rwa additionally allowing discounting past ', 'empirically compare model rwa  lstm gru unit', 'several challenging task ', 'task single output rwa  rda', 'gru unit learn much quicker lstm better performance ', 'multiple sequence copy task rda unit learns task three', 'time quickly lstm gru unit rwa fails learn', '', 'wikipedia character prediction task lstm performs best', 'followed closely rda unit ', 'overall rda unit performs', 'well sample efficient large variety sequence task ']","Recurrent Neural Networks architectures excel at processing sequences by
, modelling dependencies over different timescales., The recently introduced
, Recurrent Weighted Average (RWA) unit captures long term dependencies
, far better than an LSTM on several challenging tasks., The RWA achieves
, this by applying attention to each input and computing a weighted average
, over the full history of its computations., Unfortunately, the RWA cannot
, change the attention it has assigned to previous timesteps, and so struggles
, with carrying out consecutive tasks or tasks with changing requirements.
, We present the Recurrent Discounted Attention (RDA) unit that builds on
, the RWA by additionally allowing the discounting of the past.
, We empirically compare our model to RWA, LSTM and GRU units on
, several challenging tasks., On tasks with a single output the RWA, RDA and
, GRU units learn much quicker than the LSTM and with better performance.
, On the multiple sequence copy task our RDA unit learns the task three
, times as quickly as the LSTM or GRU units while the RWA fails to learn at
, all., On the Wikipedia character prediction task the LSTM performs best
, but it followed closely by our RDA unit., Overall our RDA unit performs
, well and is sample efficient on a large variety of sequence tasks.",28,5.199029126213592,7.357142857142857
58,"['Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging.', 'In this paper, we introduce variance layers, a different kind of stochastic layers.', 'Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance.', 'It means that each object is represented by a zero-mean distribution in the space of the activations.', 'We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks.', 'We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors.', 'We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.']","[0, 0, 1, 0, 0, 0, 0]","[0.25, 0.10810810327529907, 0.3589743673801422, 0.3499999940395355, 0.19607841968536377, 0.19512194395065308, 0.15686273574829102]",B1GAUs0cKQ,"['It is possible to learn a zero-centered Gaussian distribution over the weights of a neural network by learning only variances, and it works surprisingly well.', 'This paper investigates the effects of mean of variational posterior and proposes variance layer, which only uses variance to store information', 'Studies variance neural networks which approximate the posterior of Bayesian neural networks with zero-mean Gaussian distributions']","['ordinary stochastic neural network mostly rely expected value weight make prediction  whereas induced noise mostly used capture uncertainty  prevent overfitting slightly boost performance testtime averaging ', 'paper  introduce variance layer  different kind stochastic layer ', 'weight variance layer follows zeromean distribution parameterized variance ', 'mean object represented zeromean distribution space activation ', 'show layer learn surprisingly well  serve efficient exploration tool reinforcement learning task provide decent defense adversarial attack ', 'also show number conventional bayesian neural network naturally converge zeromean posterior ', 'observe case zeromean parameterization lead much better training objective flexible conventional parameterizations mean learned ']","Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging., In this paper, we introduce variance layers, a different kind of stochastic layers., Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance., It means that each object is represented by a zero-mean distribution in the space of the activations., We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks., We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors., We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.",12,5.685897435897436,13.0
59,"['Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data.', 'At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks.', 'In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work.', 'At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective.', 'Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI.', 'In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.']","[0, 0, 0, 0, 1, 0]","[0.17391303181648254, 0.09090908616781235, 0.09090908616781235, 0.21052631735801697, 0.2641509473323822, 0.0363636314868927]",SkaPsfZ0W,"['We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.', 'Proposes a new network of GCNs with two approaches: a fully connected layer on top of stacked features and attention mechanism that uses scalar weight per GCN.', 'Presents a Network of Graph Convolutional Networks that uses random walk statistics to extract information from near and distant neighbors in the graph']","['graph convolutional network  gcns  recently proposed architecture success semisupervised learning graphstructured data ', 'time  unsupervised learning graph embeddings benefited information contained random walk ', 'paper propose model  network gcns  ngcn   marries two line work ', 'core  ngcn train multiple instance gcns node pair discovered different distance random walk  learns combination instance output optimizes classification objective ', 'experiment show proposed ngcn model achieves stateoftheart performance challenging node classification task consider  cora  citeseer  pubmed  ppi ', 'addition  proposed method desirable property  including generalization recently proposed semisupervised learning method graphsage  allowing u propose nsage  resilience adversarial input perturbation ']","Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data., At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks., In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work., At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective., Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI., In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.",18,5.972222222222222,8.0
60,"['Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy.', 'However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy.', 'In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.', 'We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network.', 'Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches.', 'Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.']","[0, 0, 0, 1, 0, 0]","[0.4000000059604645, 0.09756097197532654, 0.1818181723356247, 0.42424240708351135, 0.22727271914482117, 0.04878048226237297]",SJtChcgAW,"['A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error.', 'Presents a cheap pruning algorithm for dense layers of DNNs.', 'Proposes a solution to the problem of pruning DNNs by posing the Net-trim objective function as a Difference of convex(DC) function.']","['recent dnn pruning algorithm succeeded reducing number parameter fully connected layer often little drop classification accuracy ', 'however existing pruning scheme either applied training require costly retraining procedure pruning regain classification accuracy ', 'paper propose cheap pruning algorithm based difference convex  dc  optimisation ', 'also provide theoretical analysis growth generalisation error  ge  new pruned network ', 'method used convex regulariser allows controlled degradation classification accuracy order magnitude faster competing approach ', 'experiment common feedforward neural network show sparsity level 90  method achieves 10  higher classification accuracy compared hard thresholding ']","Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy., However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy., In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation., We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network., Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches., Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.",6,5.720588235294118,22.666666666666668
61,"['Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years.', 'It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing.', 'In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage.', 'Our model is simple but extremely effective in terms of video sequence labeling.', 'The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.']","[0, 0, 0, 0, 1]","[0.10256409645080566, 0.04999999329447746, 0.1846153736114502, 0.06451612710952759, 0.44999998807907104]",r1nzLmWAb,"['We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets.', 'Discusses the problem of action segmentation in long videos, up to 10 minutes long by using a temporal convolutional encoder-decoder architecture', 'Proposes a combination of temporal convolutional and recurrent network for video action segmentation']","['action segmentation milestone towards building automatic system understand untrimmed video received considerable attention recent year ', 'typically modeled sequence labeling problem contains intrinsic sufficient difference text parsing speech processing ', 'paper  introduce novel hybrid temporal convolutional recurrent network  tricornet   encoderdecoder architecture  encoder consists hierarchy temporal convolutional kernel capture local motion change different action  decoder hierarchy recurrent neural network able learn memorize longterm action dependency encoding stage ', 'model simple extremely effective term video sequence labeling ', 'experimental result three public action segmentation datasets shown proposed model achieves superior performance state art ']","Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years., It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing., In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage., Our model is simple but extremely effective in terms of video sequence labeling., The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.",7,5.8936170212765955,20.142857142857142
62,"['Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative.', 'It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher.', 'Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved.', 'Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications.', 'Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance.', 'In this work, we argue that feature is the most important knowledge from teacher.', 'It is sufficient for student to just learn good features regardless of the target task.', 'From this discovery, we further present an efficient learning strategy to mimic features stage by stage.', 'Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.\n']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.0624999962747097, 0.23529411852359772, 0.2222222238779068, 0.1249999925494194, 0.12903225421905518, 0.13793103396892548, 0.13333332538604736, 0.2666666507720947, 0.052631575614213943]",rJegl2C9K7,"['This paper proposes to transfer knowledge from deep model to shallow one by mimicking features stage by stage.', 'Explains a stage by stage knowledge transer method by using different structures of resnets', 'This paper proposes dividing a network into multiple parts and distilling each part sequentially to improve distillation performance in deep teacher networks']","['convolutional neural network  cnns  become deeper deeper recent year  making study model acceleration imperative ', 'common practice employ shallow network  called student  learn deep one  termed teacher ', 'prior work made many attempt transfer different type knowledge teacher student  however  two problem remaining unsolved ', 'firstly  knowledge used existing method highly dependent task dataset  limiting application ', 'secondly  lack effective training scheme transfer process  leading degradation performance ', 'work  argue feature important knowledge teacher ', 'sufficient student learn good feature regardless target task ', 'discovery  present efficient learning strategy mimic feature stage stage ', 'extensive experiment demonstrate importance feature show proposed approach significantly narrow gap student teacher  outperforming stateoftheart method ']","Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative., It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher., Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved., Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications., Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance., In this work, we argue that feature is the most important knowledge from teacher., It is sufficient for student to just learn good features regardless of the target task., From this discovery, we further present an efficient learning strategy to mimic features stage by stage., Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.
",22,5.598802395209581,7.590909090909091
63,"['We augment adversarial training (AT) with worst case adversarial training\n', '(WCAT) which improves adversarial robustness by 11% over the current state-\n', 'of-the-art result in the `2-norm on CIFAR-10.', 'We interpret adversarial training as\n', 'Total Variation Regularization, which is a fundamental tool in mathematical im-\n', 'age processing, and WCAT as Lipschitz regularization, which appears in Image\n', 'Inpainting.', 'We obtain verifiable worst and average case robustness guarantees,\n', 'based on the expected and maximum values of the norm of the gradient of the\n', 'loss.']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.2222222238779068, 0.19999998807907104, 0.0, 0.25, 0.06666666269302368, 0.13333332538604736, 0.1428571343421936, 0.0]",HkxAisC9FQ,"['Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization', 'Explores augmenting the training loss with an additional gradient regularization term to improve robustness of models against adversarial examples', 'Uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.']","['augment adversarial training   worst case adversarial training', ' wcat  improves adversarial robustness 11  current state', 'oftheart result  2norm cifar10 ', 'interpret adversarial training', 'total variation regularization  fundamental tool mathematical im', 'age processing  wcat lipschitz regularization  appears image', 'inpainting ', 'obtain verifiable worst average case robustness guarantee ', 'based expected maximum value norm gradient', 'loss ']","We augment adversarial training (AT) with worst case adversarial training
, (WCAT) which improves adversarial robustness by 11% over the current state-
, of-the-art result in the `2-norm on CIFAR-10., We interpret adversarial training as
, Total Variation Regularization, which is a fundamental tool in mathematical im-
, age processing, and WCAT as Lipschitz regularization, which appears in Image
, Inpainting., We obtain verifiable worst and average case robustness guarantees,
, based on the expected and maximum values of the norm of the gradient of the
, loss.",13,5.765432098765432,6.230769230769231
64,"['The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \\{\\textit{passage, question}\\} pair and select one of the $n$ given options.', 'The current state of the art model for this task first computes a query-aware representation for the passage and then \\textit{selects} the option which has the maximum similarity with this representation.', 'However, when humans perform this task they do not just focus on option selection but use a combination of \\textit{elimination} and \\textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).', 'This process could be repeated multiple times till the reader is finally ready to select the correct option.', 'We propose \\textit{ElimiNet}, a neural network based model which tries to mimic this process.', 'Specifically, it has gates which decide whether an option can be eliminated given the \\{\\textit{document, question}\\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).', 'The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.', 'We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.', 'Further we show that taking an ensemble of our \\textit{elimination-selection} based method with a \\textit{selection} based method gives us an improvement of 7\\% (relative) over the best reported performance on this dataset.    \n']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.054054051637649536, 0.1666666567325592, 0.06896551698446274, 0.0714285671710968, 0.07999999821186066, 0.04444444179534912, 0.3030303120613098, 0.10256409645080566, 0.0]",B1bgpzZAZ,"['A model combining elimination and selection for answering multiple choice questions', 'Gives an elaboration on the Gated Attention Reader adding gates based on answer elimination in multiple choice reading comprehension', 'This paper proposes the use of an elimination gate in model architectures for reading comprehension tasks but does not achieve state-of-the-art results', 'This paper propses a new multi-choice reading comprehension model based on the idea that some options should be eliminated to infer better passage/question representations.']","['task reading comprehension multiple choice question  requires human  machine  read given   textit  passage  question    pair select one  n  given option ', 'current state art model task first computes queryaware representation passage textit  selects  option maximum similarity representation ', 'however  human perform task focus option selection use combination textit  elimination  textit  selection   specifically  human would first try eliminate irrelevant option read document light new information  perhaps ignore portion corresponding eliminated option  ', 'process could repeated multiple time till reader finally ready select correct option ', 'propose textit  eliminet   neural network based model try mimic process ', 'specifically  gate decide whether option eliminated given   textit  document  question    pair try make document representation orthogonal eliminatedd option  akin ignoring portion document corresponding eliminated option  ', 'model make multiple round partial elimination refine document representation finally us selection module pick best option ', 'evaluate model recently released large scale race dataset show outperforms current state art model 7 13 question type dataset ', 'show taking ensemble textit  eliminationselection  based method textit  selection  based method give u improvement 7   relative  best reported performance dataset ']","The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{\textit{passage, question}\} pair and select one of the $n$ given options., The current state of the art model for this task first computes a query-aware representation for the passage and then \textit{selects} the option which has the maximum similarity with this representation., However, when humans perform this task they do not just focus on option selection but use a combination of \textit{elimination} and \textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option)., This process could be repeated multiple times till the reader is finally ready to select the correct option., We propose \textit{ElimiNet}, a neural network based model which tries to mimic this process., Specifically, it has gates which decide whether an option can be eliminated given the \{\textit{document, question}\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option)., The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option., We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset., Further we show that taking an ensemble of our \textit{elimination-selection} based method with a \textit{selection} based method gives us an improvement of 7\% (relative) over the best reported performance on this dataset.    
",16,5.448763250883392,16.647058823529413
65,"['Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.', 'The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.', 'It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  ', 'In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.', 'Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.', ""Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy."", 'We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.', 'Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.', 'Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.', 'We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.0555555522441864, 0.514285683631897, 0.05882352590560913, 0.0, 0.05405404791235924, 0.052631575614213943, 0.0, 0.3030303120613098]",rkl6As0cF7,"['We proposed a novel probabilisitic recursive reasoning (PR2) framework for multi-agent deep reinforcement learning tasks.', 'Proposes a new approach for fully decentralized training in multi-agent reinforcement learning', 'Tackles the problem of endowing RL agents with recursive reasoning capabilities in a multi-agent setting based on the hypothesis that recursive reasoning is beneficial for them to converge to non-trival equilibria', 'The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. ']","['human capable attributing latent mental content belief  intention others ', 'social skill critical everyday life reason potential consequence behavior plan ahead ', 'known human use reasoning ability recursively  ie  considering others believe belief ', 'paper  start level  1  recursion introduce probabilistic recursive reasoning  pr2  framework multiagent reinforcement learning ', 'hypothesis beneficial agent account opponent would react future behavior ', 'pr2 framework  adopt variational bayes method approximate opponent  conditional policy  agent find best response improve policy ', 'develop decentralizedtrainingdecentralizedexecution algorithm  pr2q pr2actorcritic  proved converge selfplay scenario one nash equilibrium ', 'method tested matrix game differential game  nontrivial equilibrium common gradientbased method fail converge ', 'experiment show critical reason opponent believe agent belief ', 'expect work contribute new idea modeling opponent multiagent reinforcement learning community ']","Humans are capable of attributing latent mental contents such as beliefs, or intentions to others., The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead., It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  , In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning., Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors., Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy., We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium., Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge., Our experiments show that it is critical to reason about how the opponents believe about what the agent believes., We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  
",18,5.627358490566038,11.157894736842104
66,"['Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.', 'However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.', 'A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.', 'To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.', 'Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.', 'We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.', 'We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.', 'We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.21621620655059814, 0.12121211737394333, 0.12244897335767746, 0.5454545617103577, 0.0, 0.1621621549129486, 0.05882352590560913, 0.2380952388048172]",rkEfPeZRb,"['A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing unambiguous gradients.', 'Proposes a variance-based gradient compression method to reducee the communication overhead of distributed deep learning', 'Proposes a novel way of compressing gradient updates for distributed SGD in order to speed up overall execution', 'Introduces variance-based gradient compression method for efficient distributed training of neural networks and measuring ambuiguity.']","['due substantial computational cost  training stateoftheart deep neural network largescale datasets often requires distributed training using multiple computation worker ', 'however  nature  worker need frequently communicate gradient  causing severe bottleneck  especially lower bandwidth connection ', 'method proposed compress gradient efficient communication  either suffer low compression ratio significantly harm resulting model accuracy  particularly applied convolutional neural network ', 'address issue  propose method reduce communication overhead distributed deep learning ', 'key observation gradient update delayed unambiguous  high amplitude  low variance  gradient calculated ', 'also present efficient algorithm compute variance prove obtained negligible additional cost ', 'experimentally show method achieve high compression ratio maintaining result model accuracy ', 'also analyze efficiency using computation communication cost model provide evidence method enables distributed deep learning many scenario commodity environment ']","Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers., However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections., A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks., To address these issues, we propose a method to reduce the communication overhead of distributed deep learning., Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated., We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost., We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy., We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.",17,6.146067415730337,10.470588235294118
67,"['In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.', 'We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics.', 'Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion.', 'We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.']","[1, 0, 0, 0]","[0.3461538553237915, 0.043478257954120636, 0.09302324801683426, 0.1818181723356247]",rJWechg0Z,"['A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization', 'Improves the correlation alignment approach to domain adaptation by replacing the Euclidean distance with the geodesic Log-Euclidean distance between two covariance matices, and automatically selecting the balancing cost by the entropy on the target domain.', 'Proposal for minimal-entropy correlation alignment, an unsupervised domain adaptation algorithm which links together entropy minimization and correlation alignment methods.']","['work  face problem unsupervised domain adaptation novel deep learning approach leverage finding entropy minimization induced optimal alignment second order statistic source target domain ', 'formally demonstrate hypothesis  aiming achieving optimal alignment practical case  adopt principled strategy  differently current euclidean approach  deploys alignment along geodesic ', 'pipeline implemented adding standard classification loss  labeled source domain   sourcetotarget regularizer weighted unsupervised datadriven fashion ', 'provide extensive experiment ass superiority framework standard domain modality adaptation benchmark ']","In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains., We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics., Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion., We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.",10,5.932203389830509,11.8
68,"['Catastrophic interference has been a major roadblock in the research of continual learning.', 'Here we propose a variant of the back-propagation algorithm, ""Conceptor-Aided Backprop"" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks.', 'Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting.', 'CAB extends these results to deep feedforward networks.', 'On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.']","[0, 1, 0, 0, 0]","[0.24242423474788666, 0.8181818127632141, 0.1111111044883728, 0.0, 0.0476190410554409]",B1al7jg0b,"['We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.', 'This paper applies the notion of conceptors, a form a regulariser, to prevent forgetting in continual learning in the training of neural networks on sequential tasks.', 'Introduces a method for learning new tasks, without interfering previous tasks, using conceptors.']","['catastrophic interference major roadblock research continual learning ', 'propose variant backpropagation algorithm   conceptoraided backprop   cab   gradient shielded conceptors degradation previously learned task ', 'conceptors origin reservoir computing  previously shown overcome catastrophic forgetting ', 'cab extends result deep feedforward network ', 'disjoint permuted mnist task  cab outperforms two method coping catastrophic interference recently proposed ']","Catastrophic interference has been a major roadblock in the research of continual learning., Here we propose a variant of the back-propagation algorithm, ""Conceptor-Aided Backprop"" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks., Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting., CAB extends these results to deep feedforward networks., On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.",9,6.223529411764706,9.444444444444445
69,"['Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task.', 'Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks.', 'In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora.', 'Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering.', 'It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering.', 'On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses.', 'The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.']","[1, 0, 0, 0, 0, 0, 0]","[0.3529411852359772, 0.20408162474632263, 0.10810810327529907, 0.0555555522441864, 0.13333332538604736, 0.25641024112701416, 0.0]",H1eqviAqYX,"['Analyze the reason for neural response generative models preferring universal replies; Propose a method to avoid it.', 'Investigates the problem of universal replies plaguing the Seq2Seq neural generation models', 'The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase.']","['recent advance neural sequencetosequence  seq2seq  model reveal purely datadriven approach response generation task ', 'despite diverse variant application  existing seq2seq model prone producing short generic reply  block neural network architecture utilized practical opendomain response generation task ', 'research  analyze critical issue perspective optimization goal model specific characteristic humantohuman conversational corpus ', 'analysis conducted decomposing goal neural response generation  nrg  optimization word selection ordering ', 'derived decomposing seq2seq based nrg model naturally tend select common word compose response  ignore semantic query word ordering ', 'basis analysis  propose maxmarginal ranking regularization term avoid seq2seq model producing generic uninformative response ', 'empirical experiment benchmark several metric validated analysis proposed methodology ']","Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task., Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks., In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora., Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering., It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering., On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses., The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.",12,6.0,13.75
70,"['The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.', 'Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.', 'We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.', 'Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.\n', 'We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.', 'Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.', 'An interactive online demo of our model is available at http://code2seq.org.', 'Our code, data and trained models are available at http://github.com/tech-srl/code2seq.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.4571428596973419, 0.15789473056793213, 0.5, 0.20000000298023224, 0.25, 0.0, 0.07999999821186066, 0.0]",H1gKYo09tX,"['We leverage the syntactic structure of source code to generate natural language sequences.', 'Presents a method for generating sequences from code by parsing and producing a syntax tree', 'This paper introduces an AST-based encoding for programming code and shows its effectiveness in the tasks of extreme code summarization and code captioning.', 'This paper presents a new code-to-sequence model that leverages the syntactic structure of programming languages to encode source code snippets and then decode them to natural language']","['ability generate natural language sequence source code snippet variety application code summarization  documentation  retrieval ', 'sequencetosequence  seq2seq  model  adopted neural machine translation  nmt   achieved stateoftheart performance task treating source code sequence token ', 'present code2seq  alternative approach leverage syntactic structure programming language better encode source code ', 'model represents code snippet set compositional path abstract syntax tree  ast  us attention select relevant path decoding ', 'demonstrate effectiveness approach two task  two programming language  four datasets 16m example ', 'model significantly outperforms previous model specifically designed programming language  well general stateoftheart nmt model ', 'interactive online demo model available http  code2seqorg ', 'code  data trained model available http  githubcomtechsrlcode2seq ']","The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval., Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens., We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code., Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.
, We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples., Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models., An interactive online demo of our model is available at http://code2seq.org., Our code, data and trained models are available at http://github.com/tech-srl/code2seq.",16,6.089171974522293,9.8125
71,"['We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.', 'The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.', 'Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\n', 'Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.', 'As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\n', 'Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.']","[1, 0, 0, 0, 0, 0]","[0.5625, 0.09090908616781235, 0.051282044500112534, 0.09999999403953552, 0.1666666567325592, 0.17241379618644714]",rJe7FW-Cb,"['We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.', 'Describes a novel attentional mechanism applid to fine-grained recognition that consistently improves the recognition accuracy of the baseline', 'This paper proposes a feed-forward attention mechanism for fine-grained image classification', 'This paper presents an interesting attention mechanism for fine-grained image classification.']","['propose novel attention mechanism enhance convolutional neural network finegrained recognition ', 'proposed mechanism reuses cnn feature activation find informative part image different depth help gating mechanism without part annotation ', 'thus  used augment layer cnn extract low highlevel local information discriminative ', 'differently  approach  mechanism propose need single pas input trained endtoend sgd ', 'consequence  proposed mechanism modular  architectureindependent  easy implement  faster iterative approach ', 'experiment show  augmented approach  wide residual network systematically achieve superior performance five different finegrained recognition datasets  adience age gender recognition benchmark  caltechucsd birds2002011  stanford dog  stanford car  uec food100  obtaining competitive stateoftheart score ']","We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition., The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations., Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. 

, Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD., As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.

, Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.",20,6.013245033112582,7.55
72,"['Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator.', 'We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances.', 'We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.', 'We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator.', 'Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin.', 'Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.']","[0, 0, 0, 1, 0, 0]","[0.1666666567325592, 0.20000000298023224, 0.1818181723356247, 0.5600000023841858, 0.07407406717538834, 0.0]",rkQsMCJCb,"['We replace normal convolutions with adaptive convolutions to improve GANs generator.', 'Proposes to replace convolutions in the generator with an Adaptive Convolution Block that learns to generate convolution weigths adn biases of upsampling operations adaptively per pixel location', 'Uses Adaptive Convolution in the context of GANs with a block called AdaConvBlock that replaces regular Convolution, this gives more local context per kernel weight so that it can generate locally flexible objects.']","['existing gans architecture generate image use transposed convolution resizeconvolution upsampling algorithm lower higher resolution feature map generator ', 'argue kind fixed operation problematic gans model object different visual appearance ', 'propose novel adaptive convolution method learns upsampling algorithm based local context location address problem ', 'modify baseline gans architecture replacing normal convolution adaptive convolution generator ', 'experiment cifar10 dataset show modified model improve baseline model large margin ', 'furthermore  model achieve stateoftheart performance cifar10 stl10 datasets unsupervised setting ']","Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator., We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances., We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem., We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator., Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin., Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.",7,5.92436974789916,17.0
73,"['Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.', 'However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.', 'In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.', 'Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.', 'Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.', 'Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.', 'These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.', 'Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.', 'We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.05128204822540283, 0.1304347813129425, 0.19999998807907104, 0.27272728085517883, 0.07843136787414551, 0.1666666567325592, 0.1702127605676651, 0.3255814015865326, 0.14035087823867798]",rkr1UDeC-,"['We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines.', 'Proposes a method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent', 'Proposal for an online distillation method called co-distillation, applied at scale, where two different models are trained to match predictions of the other model in addition to minimizing its own loss.', 'Online distillation technique is introduced to accelerate traditional algorithms for large-scaled distributed neural network training']","['technique ensembling distillation promise model quality improvement paired almost base model ', 'however  due increased testtime cost  ensemble  increased complexity training pipeline  distillation   technique challenging use industrial setting ', 'paper explore variant distillation relatively straightforward use require complicated multistage setup many new hyperparameters ', 'first claim online distillation enables u use extra parallelism fit large datasets twice fast ', 'crucially  still speed training even already reached point additional parallelism provides benefit synchronous asynchronous stochastic gradient descent ', 'two neural network trained disjoint subset data share knowledge encouraging model agree prediction model would made ', 'prediction come stale version model safely computed using weight rarely get transmitted ', 'second claim online distillation costeffective way make exact prediction model dramatically reproducible ', 'support claim using experiment criteo display ad challenge dataset  imagenet  largest todate dataset used neural language modeling  containing  6times 10  11   token based common crawl repository web data ']","Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model., However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings., In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters., Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast., Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent., Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made., These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted., Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible., We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",15,5.472340425531915,15.666666666666666
74,"['Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis.', 'Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings.', 'In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.', 'A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set.', 'We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data.', 'Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem.', 'We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.']","[0, 0, 1, 0, 0, 0, 0]","[0.09999999403953552, 0.045454539358615875, 0.4166666567325592, 0.1702127605676651, 0.2448979616165161, 0.2380952388048172, 0.25641024112701416]",r1saNM-RW,"['We present an algorithm for speeding up SVM training on massive data sets by constructing compact representations that provide efficient and provably approximate inference.', 'Studies the approach of coreset for SVM and aims at sampling a small set of weighted points such that the loss function over the points provably approximates that over the whole dataset', 'The paper suggests an importance sampling based Coreset construction to represent large training data for SVMs']","['support vector machine  svms  one popular algorithm classification regression analysis ', 'despite popularity  even efficient implementation proven computationally expensive train largescale  especially streaming setting ', 'paper  propose novel coreset construction algorithm efficiently generating compact representation massive data set speed svm training ', 'coreset weighted subset original data point svms trained coreset provably competitive trained original  massive  data set ', 'provide lower upper bound number sample required obtain accurate approximation svm problem function complexity input data ', 'analysis also establishes sufficient condition existence sufficiently compact representative coresets svm problem ', 'empirically evaluate practical effectiveness algorithm synthetic realworld data set ']","Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis., Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings., In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training., A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set., We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data., Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem., We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.",10,5.698717948717949,15.6
75,"['The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates.', 'Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors.', 'For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions.', 'We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension.', 'We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.']","[1, 0, 0, 0, 0]","[0.31111109256744385, 0.1428571343421936, 0.1666666567325592, 0.28070175647735596, 0.1846153736114502]",HyxjwgbRZ,"['We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning.', 'Provided a convergence analysis of Sign SGD algorithm for non-covex cases', 'The paper explores an algorithm that uses the sign of the gradients instead of actual gradients for training deep models']","['sign stochastic gradient descent method  signsgd  utilizes sign stochastic gradient update ', 'since signsgd carry onebit quantization gradient  extremely practical distributed optimization gradient need aggregated different processor ', 'first time  establish convergence rate signsgd general nonconvex function transparent condition ', 'show rate signsgd reach firstorder critical point match sgd term number stochastic gradient call  roughly linear factor dimension ', 'carry simple experiment explore behaviour sign gradient descent  without stochasticity  close saddle point show often help completely avoid without using either stochasticity curvature information ']","The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates., Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors., For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions., We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension., We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.",8,5.658914728682171,16.125
76,"['Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection.', 'Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe.', 'While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries. \n\n', 'To this end, we introduce a transparent middleware layer for neural network acceleration.', 'The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices.', 'In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks.', 'We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively.']","[0, 0, 0, 1, 0, 0, 0]","[0.1304347813129425, 0.11320754140615463, 0.20689654350280762, 0.3684210479259491, 0.20408162474632263, 0.1428571343421936, 0.34375]",rkf5hnNDj7,"['We introduce a transparent middleware for neural network acceleration, with own compiler engine, achieving up to 11.8x speed up on CPUs and 2.3x on GPUs.', 'This paper proposes a transparent middleware layer for neural network acceleration and obtains some acceleration results on basic CPU and GPU architectures']","['deep learning found numerous application thanks versatility accuracy pattern recognition problem visual object detection ', 'learning inference deep neural network  however  memory compute intensive improving efficiency one major challenge framework pytorch  tensorflow  caffe ', 'efficiency problem partially addressed specialized hardware corresponding proprietary library  believe neural network acceleration transparent user support hardware platform deep learning library ', 'end  introduce transparent middleware layer neural network acceleration ', 'system built around compiler deep learning  allowing one combine devicespecific library custom optimization supporting numerous hardware device ', 'contrast project  explicitly target optimization prediction training neural network ', 'present current development status preliminary encouraging result  standard x86 server  using cpu system achieves 118x speedup inference 80x batchedprediction  128   gpus achieve 17x 23x speedup respectively ']","Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection., Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe., While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries. 

, To this end, we introduce a transparent middleware layer for neural network acceleration., The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices., In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks., We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively.",16,5.824468085106383,11.75
77,"['Performance of neural networks can be significantly improved by encoding known invariance for particular tasks.', 'Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation.', 'In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance.', 'We call our new method the Conic Convolution and DFT Network (CFNet).', 'We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection.', 'Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.']","[0, 0, 1, 0, 0, 0]","[0.06666666269302368, 0.13793103396892548, 0.23076923191547394, 0.2222222238779068, 0.1702127605676651, 0.11764705181121826]",BJepX2A9tX,"['We propose conic convolution and the 2D-DFT to encode rotation equivariance into an neural network.', 'In the context of image classification, the paper proposes a convolutional neural network architecture with rotation-equivariant feature maps that are eventually made rotation-invariant by using the magnitude of the 2D discrete Fourier transform (DFT).', 'Authors provide a rotation invariant neural network via combining conic convolution and 2D-DFT']","['performance neural network significantly improved encoding known invariance particular task ', 'many image classification task  related cellular imaging  exhibit invariance rotation ', 'particular  aid convolutional neural network learning rotation invariance  consider simple  efficient conic convolutional scheme encodes rotational equivariance  along method integrating magnitude response 2ddiscretefourier transform  2ddft  encode global rotational invariance ', 'call new method conic convolution dft network  cfnet  ', 'evaluated efficacy cfnet compared standard cnn groupequivariant cnn  gcnn  several different image classification task demonstrated improved performance  including classification accuracy  computational efficiency  robustness hyperparameter selection ', 'taken together  believe cfnet represents new scheme potential improve many imaging analysis application ']","Performance of neural networks can be significantly improved by encoding known invariance for particular tasks., Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation., In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance., We call our new method the Conic Convolution and DFT Network (CFNet)., We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection., Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.",16,6.357142857142857,8.75
78,"['The problem of visual metamerism is defined as finding a family of perceptually\n', 'indistinguishable, yet physically different images.', 'In this paper, we propose our\n', 'NeuroFovea metamer model, a foveated generative model that is based on a mixture\n', 'of peripheral representations and style transfer forward-pass algorithms.', 'Our\n', 'gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder\n', 'which allows us to encode images in high dimensional space and interpolate\n', 'between the content and texture information with adaptive instance normalization\n', 'anywhere in the visual field.', 'Our contributions include:', '1) A framework for\ncomputing metamers that resembles a noisy communication system via a foveated\nfeed-forward encoder-decoder network  We observe that metamerism arises as a\nbyproduct of noisy perturbations that partially lie in the perceptual null space;', '2)\nA perceptual optimization scheme as a solution to the hyperparametric nature of\nour metamer model that requires tuning of the image-texture tradeoff coefficients\neverywhere in the visual field which are a consequence of internal noise;', '3) An\n', 'ABX psychophysical evaluation of our metamers where we also find that the rate\n', 'of growth of the receptive fields in our model match V1 for reference metamers\n', 'and V2 between synthesized samples.', 'Our model also renders metamers at roughly\n', 'a second, presenting a 1000 speed-up compared to the previous work, which now\n', 'allows for tractable data-driven metamer experiments.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.1818181723356247, 0.0, 0.0, 0.09090908616781235, 0.0, 0.09999999403953552, 0.09090908616781235, 0.0, 0.13333332538604736, 0.0, 0.23255813121795654, 0.1463414579629898, 0.08695651590824127, 0.08695651590824127, 0.0, 0.11764705181121826, 0.1818181723356247, 0.0]",BJzbG20cFQ,"['We introduce a novel feed-forward framework to generate visual metamers', 'Proposes a NeuroFovea model for generation of point-of-fixation metamers by using a style transfer approach via and Encoder-Decoder style architecture', 'An analysis of metamerism and a model capable of rapidly producing metamers of value for experimental psychophysics and other domains.', 'The paper proposes a fast method for generating visual metamers  physically different images that cannot be told apart from an original  via foveated, fast, arbitrary style transfer']","['problem visual metamerism defined finding family perceptually', 'indistinguishable  yet physically different image ', 'paper  propose', 'neurofovea metamer model  foveated generative model based mixture', 'peripheral representation style transfer forwardpass algorithm ', '', 'gradientdescent free model parametrized foveated vgg19 encoderdecoder', 'allows u encode image high dimensional space interpolate', 'content texture information adaptive instance normalization', 'anywhere visual field ', 'contribution include ', '1  framework computing metamers resembles noisy communication system via foveated feedforward encoderdecoder network  observe metamerism arises byproduct noisy perturbation partially lie perceptual null space ', '2  perceptual optimization scheme solution hyperparametric nature metamer model requires tuning imagetexture tradeoff coefficient everywhere visual field consequence internal noise ', '3 ', 'abx psychophysical evaluation metamers also find rate', 'growth receptive field model match v1 reference metamers', 'v2 synthesized sample ', 'model also render metamers roughly', 'second  presenting 1000 speedup compared previous work ', 'allows tractable datadriven metamer experiment ']","The problem of visual metamerism is defined as finding a family of perceptually
, indistinguishable, yet physically different images., In this paper, we propose our
, NeuroFovea metamer model, a foveated generative model that is based on a mixture
, of peripheral representations and style transfer forward-pass algorithms., Our
, gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder
, which allows us to encode images in high dimensional space and interpolate
, between the content and texture information with adaptive instance normalization
, anywhere in the visual field., Our contributions include:, 1) A framework for
computing metamers that resembles a noisy communication system via a foveated
feed-forward encoder-decoder network  We observe that metamerism arises as a
byproduct of noisy perturbations that partially lie in the perceptual null space;, 2)
A perceptual optimization scheme as a solution to the hyperparametric nature of
our metamer model that requires tuning of the image-texture tradeoff coefficients
everywhere in the visual field which are a consequence of internal noise;, 3) An
, ABX psychophysical evaluation of our metamers where we also find that the rate
, of growth of the receptive fields in our model match V1 for reference metamers
, and V2 between synthesized samples., Our model also renders metamers at roughly
, a second, presenting a 1000 speed-up compared to the previous work, which now
, allows for tractable data-driven metamer experiments.",25,5.7318181818181815,8.8
79,"['Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks.', 'Towards explaining this phenomenon, we adopt a margin-based perspective.', 'We establish:', '1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,', '2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks.', 'In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees.', 'The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees.', 'We validate this gap between the two approaches empirically.', 'Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization.', 'We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.1111111044883728, 0.12903225421905518, 0.3333333134651184, 0.25641024112701416, 0.1111111044883728, 0.2448979616165161, 0.19354838132858276, 0.0624999962747097, 0.31578946113586426]",HJGtFoC5Fm,"['We show that training feedforward relu networks with a weak regularizer results in a maximum margin and analyze the implications of this result.', 'Studies margin theory for neural sets  and shows that max margin is monotonically increasing in size of the network', 'This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations, obtaining a generalization upper bound which does not increase with the network size.']","['past work shown  somewhat surprisingly  overparametrization help generalization neural network ', 'towards explaining phenomenon  adopt marginbased perspective ', 'establish ', '1  multilayer feedforward relu network  global minimizer weaklyregularized crossentropy loss maximum normalized margin among network ', '2  result  increasing overparametrization improves normalized margin generalization error bound deep network ', 'case twolayer network  infinitewidth neural network enjoys best generalization guarantee ', 'typical infinite feature method kernel method  compare neural net margin kernel method construct natural instance kernel method much weaker generalization guarantee ', 'validate gap two approach empirically ', 'finally  infiniteneuron viewpoint also fruitful analyzing optimization ', 'show perturbed gradient flow infinitesize network find global optimizer polynomial time ']","Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks., Towards explaining this phenomenon, we adopt a margin-based perspective., We establish:, 1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,, 2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks., In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees., The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees., We validate this gap between the two approaches empirically., Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization., We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.",17,6.319727891156463,8.647058823529411
80,"['We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.', 'The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.', 'The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.', 'Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.']","[0, 0, 0, 1]","[0.38596490025520325, 0.1492537260055542, 0.1666666567325592, 0.5283018946647644]",H1Dy---0Z,"['A distributed architecture for deep reinforcement learning at scale, using parallel data-generation to improve the state of the art on the Arcade Learning Environment benchmark in a fraction of the wall-clock training time of previous approaches.', 'Examines a distirbuted Deep RL system in which experiences, rather than gradients, are shared between the parallel works and the cetralized learner', 'A parallel approach to DQN training, based on the idea of having multiple actors collecting data in parallel while a single learner trains the model from experiences sampled from central replay memory.', 'This paper proposes a distributed architecture for deep reinforcement learning at scale, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework']","['propose distributed architecture deep reinforcement learning scale  enables agent learn effectively order magnitude data previously possible ', 'algorithm decouples acting learning  actor interact instance environment selecting action according shared neural network  accumulate resulting experience shared experience replay memory  learner replay sample experience update neural network ', 'architecture relies prioritized experience replay focus significant data generated actor ', 'architecture substantially improves state art arcade learning environment  achieving better final performance fraction wallclock training time ']","We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible., The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network., The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors., Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",7,5.813559322033898,16.857142857142858
81,"['Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly.', 'In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).', 'Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed.', 'As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform.', 'Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization.', 'We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.']","[0, 1, 0, 0, 0, 0]","[0.0714285671710968, 0.25641024112701416, 0.20689654350280762, 0.09090908616781235, 0.1249999925494194, 0.060606054961681366]",S1fHmlbCW,"['Neural architectures providing representations of irregularly observed signals that provably enable signal reconstruction.', 'Proves that convolutional neural networks with Leaky ReLU activation function are nonlinear frames, with similar results for non-uniformly sampled time-series', 'This article considers neural networks over time-series and show that the first convolutional filters can be chosen to represent a discrete wavelet transform.']","['designing neural network continuoustime stochastic process challenging  especially observation made irregularly ', 'article  analyze neural network frame theoretic perspective identify sufficient condition enable smoothly recoverable representation signal l2  r  ', 'moreover  show  certain assumption  property hold even signal irregularly observed ', 'converge family  convolutional  neural network satisfy condition  show optimize convolution filter constraining effectively compute discrete wavelet transform ', 'neural network efficiently divide timeaxis signal orthogonal subspace different temporal scale localization ', 'evaluate resulting neural network assortment synthetic realworld task  parsimonious autoencoding  video classification  financial forecasting ']","Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly., In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R)., Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed., As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform., Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization., We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.",14,6.313432835820896,9.571428571428571
82,"['Most state-of-the-art neural machine translation systems, despite being different\n', 'in architectural skeletons (e.g., recurrence, convolutional), share an indispensable\n', 'feature: the Attention.', 'However, most existing attention methods are token-based\n', 'and ignore the importance of phrasal alignments, the key ingredient for the success\n', 'of phrase-based statistical machine translation.', 'In this paper, we propose\n', 'novel phrase-based attention methods to model n-grams of tokens as attention\n', 'entities.', 'We incorporate our phrase-based attentions into the recently proposed\n', 'Transformer network, and demonstrate that our approach yields improvements of\n', '1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation\n', 'tasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks \n', 'on WMT newstest2014 using WMT16 training data.\n']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.0714285671710968, 0.0, 0.1666666567325592, 0.0714285671710968, 0.0, 0.0, 0.14814814925193787, 0.07692307233810425, 0.0, 0.0, 0.06896550953388214, 0.07999999821186066]",r1xN5oA5tm,"['Phrase-based attention mechanisms to assign attention on phrases, achieving token-to-phrase, phrase-to-token, phrase-to-phrase attention alignments, in addition to existing token-to-token attentions.', 'Paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams(phrases).']","['stateoftheart neural machine translation system  despite different', 'architectural skeleton  eg  recurrence  convolutional   share indispensable', 'feature  attention ', 'however  existing attention method tokenbased', 'ignore importance phrasal alignment  key ingredient success', 'phrasebased statistical machine translation ', 'paper  propose', 'novel phrasebased attention method model ngrams token attention', 'entity ', 'incorporate phrasebased attention recently proposed', 'transformer network  demonstrate approach yield improvement', '13 bleu englishtogerman 05 bleu germantoenglish translation', 'task  175 135 bleu point englishtorussian russiantoenglish translation task', 'wmt newstest2014 using wmt  16 training data ']","Most state-of-the-art neural machine translation systems, despite being different
, in architectural skeletons (e.g., recurrence, convolutional), share an indispensable
, feature: the Attention., However, most existing attention methods are token-based
, and ignore the importance of phrasal alignments, the key ingredient for the success
, of phrase-based statistical machine translation., In this paper, we propose
, novel phrase-based attention methods to model n-grams of tokens as attention
, entities., We incorporate our phrase-based attentions into the recently proposed
, Transformer network, and demonstrate that our approach yields improvements of
, 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation
, tasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks 
, on WMT newstest2014 using WMT16 training data.
",23,6.669642857142857,4.869565217391305
83,"['Intuitively, unfamiliarity should lead to lack of confidence.', 'In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training.', 'Unlike domain adaptation methods, we cannot gather an ""unexpected dataset"" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected.', 'We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score.', 'Experimentally, we investigate the overconfidence problem and evaluate our solution by creating ""familiar"" and ""novel"" test splits, where ""familiar"" are identically distributed with training and ""novel"" are not.', 'We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods.', ""In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing."", 'Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.05128204822540283, 0.2222222238779068, 0.1071428507566452, 0.19780218601226807, 0.1111111044883728, 0.1818181723356247, 0.07999999821186066, 0.19230768084526062]",S1giro05t7,"['Deep networks are more likely to be confidently wrong when testing on unexpected data. We propose an experimental methodology to study the problem, and two methods to reduce confident errors on unknown input distributions.', 'Proposes two ideas for reducing overconfident wrong predictions: ""G-distillation"" of am ensemble with extra unsupervised data and Novelty Confidence Reduction using novelty detector', 'The authors propose two methods for estimating classification confidence on novel unseen data distributions. The first idea is to use ensemble methods as the base approach to help identify uncertain cases and then use distillation methods to reduce the ensemble into a single model mimicking behavior of the ensemble. The second idea is to use a novelty detector classifier and weight the network output by the novelty score.']","['intuitively  unfamiliarity lead lack confidence ', 'reality  current algorithm often make highly confident yet wrong prediction faced unexpected test sample unknown distribution different training ', 'unlike domain adaptation method  gather  unexpected dataset  prior test  unlike novelty detection method  besteffort original task prediction still expected ', 'compare number method related field calibration epistemic uncertainty modeling  well two proposed method reduce overconfident error sample unknown novel distribution without drastically increasing evaluation time   1  gdistillation  training ensemble classifier distill single model using labeled unlabeled example   2  ncr  reducing prediction confidence based novelty detection score ', 'experimentally  investigate overconfidence problem evaluate solution creating  familiar   novel  test split   familiar  identically distributed training  novel  ', 'discover calibrating using temperature scaling familiar data best singlemodel method improving novel confidence  followed proposed method ', 'addition  method  nll performance roughly equivalent regularly trained model certain degree smoothing ', 'calibrating also reduce confident error  example  gender recognition 95  demographic group different training data ']","Intuitively, unfamiliarity should lead to lack of confidence., In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training., Unlike domain adaptation methods, we cannot gather an ""unexpected dataset"" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected., We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score., Experimentally, we investigate the overconfidence problem and evaluate our solution by creating ""familiar"" and ""novel"" test splits, where ""familiar"" are identically distributed with training and ""novel"" are not., We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods., In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing., Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.",23,6.027272727272727,9.565217391304348
84,"['Progress in deep learning is slowed by the days or weeks it takes to train large models.', 'The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources.', 'In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available.', 'Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products.', 'We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps.', 'At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%.', 'Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%.', 'Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.1538461446762085, 0.09756097197532654, 0.37931033968925476, 0.08510638028383255, 0.17241378128528595, 0.05128204822540283, 0.04999999329447746, 0.16326530277729034]",rkLyJl-0-,"['We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.', 'Proposes a new algorithm where they claim to use Hessian implicitly and are using a motivation from power-series', 'Presents a new 2nd-order algorithm that implicitly uses curvature information and shows the intuition behind the approximation schemes in the algorithms and validates the heuristics in various experiments.']","['progress deep learning slowed day week take train large model ', 'natural solution using hardware limited diminishing return  lead inefficient use additional resource ', 'paper  present large batch  stochastic optimization algorithm faster widely used algorithm fixed amount computation  also scale substantially better computational resource become available ', 'algorithm implicitly computes inverse hessian minibatch produce descent direction  without either explicit approximation hessian hessianvector product ', 'demonstrate effectiveness algorithm successfully training large imagenet model  inceptionv3  resnetv150  resnetv1101 inceptionresnetv2  minibatch size 32000 loss validation error relative current baseline  increase total number step ', 'smaller minibatch size  optimizer improves validation error model 0809  ', 'alternatively  trade accuracy reduce number training step needed roughly 1030  ', 'work practical easily usable others  one hyperparameter  learning rate  need tuning  furthermore  algorithm computationally cheap commonly used adam optimizer ']","Progress in deep learning is slowed by the days or weeks it takes to train large models., The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources., In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available., Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products., We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps., At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\%., Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\%., Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.",19,5.6028708133971294,11.0
85,"['Recent work has shown that performing inference with fast, very-low-bitwidth\n', '(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\n', 'results.', 'However, although 2-bit approximated networks have been shown to\n', 'be quite accurate, 1 bit approximations, which are twice as fast, have restrictively\n', 'low accuracy.', 'We propose a method to train models whose weights are a mixture\n', 'of bitwidths, that allows us to more finely tune the accuracy/speed trade-off.', 'We\n', 'present the middle-out criterion for determining the bitwidth for each value, and\n', 'show how to integrate it into training models with a desired mixture of bitwidths.\n', 'We evaluate several architectures and binarization techniques on the ImageNet\n', 'dataset.', 'We show that our heterogeneous bitwidth approximation achieves superlinear\n', 'scaling of accuracy with bitwidth.', 'Using an average of only 1.4 bits, we are\n', 'able to outperform state-of-the-art 2-bit architectures.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.0952380895614624, 0.0, 0.0, 0.0, 0.09090908616781235, 0.0, 0.1904761791229248, 0.1538461446762085, 0.1904761791229248, 0.4000000059604645, 0.1249999925494194, 0.0, 0.0]",HJDV5YxCW,"['We introduce fractional bitwidth approximation and show it has significant advantages.', 'Suggests a method for varying the degree of quantization in a neural network during the forward propagation phase', 'Maintaining the accuracy of 2bits netword while using less than 2bits weights']","['recent work shown performing inference fast  verylowbitwidth', ' eg  1 2 bit  representation value model yield surprisingly accurate', 'result ', 'however  although 2bit approximated network shown', 'quite accurate  1 bit approximation  twice fast  restrictively', 'low accuracy ', 'propose method train model whose weight mixture', 'bitwidths  allows u finely tune accuracyspeed tradeoff ', '', 'present  middleout  criterion determining bitwidth value ', 'show integrate training model desired mixture bitwidths ', 'evaluate several architecture binarization technique imagenet', 'dataset ', 'show heterogeneous bitwidth approximation achieves superlinear', 'scaling accuracy bitwidth ', 'using average 14 bit ', 'able outperform stateoftheart 2bit architecture ']","Recent work has shown that performing inference with fast, very-low-bitwidth
, (e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate
, results., However, although 2-bit approximated networks have been shown to
, be quite accurate, 1 bit approximations, which are twice as fast, have restrictively
, low accuracy., We propose a method to train models whose weights are a mixture
, of bitwidths, that allows us to more finely tune the accuracy/speed trade-off., We
, present the middle-out criterion for determining the bitwidth for each value, and
, show how to integrate it into training models with a desired mixture of bitwidths.
, We evaluate several architectures and binarization techniques on the ImageNet
, dataset., We show that our heterogeneous bitwidth approximation achieves superlinear
, scaling of accuracy with bitwidth., Using an average of only 1.4 bits, we are
, able to outperform state-of-the-art 2-bit architectures.",26,5.7,5.384615384615385
86,"['Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model.', 'We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  ', 'We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  ', 'Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.']","[0, 0, 1, 0]","[0.09090908616781235, 0.2083333283662796, 0.22727271914482117, 0.11764705181121826]",BJxRVnC5Fm,"['Mean Replacement is an efficient method to improve the loss after pruning and Taylor approximation based scoring functions works better with absolute values. ', 'Proposes a simple improvement to methods for unit pruning using ""mean replacement""', 'This paper presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning']","['pruning unit deep network help speed inference training well reduce size model ', 'show bias propagation pruning technique consistently outperforms common approach merely removing unit  regardless architecture dataset ', 'also show simple adaptation existing scoring function allows u select best unit prune ', 'finally  show unit selected best performing scoring function somewhat consistent course training  implying dead part network appear stage training ']","Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model., We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  , We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  , Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.",7,4.970873786407767,14.714285714285714
87,"['Due to the phenomenon of posterior collapse, current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective.', 'We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information.', 'Our proposed -VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior.', 'For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis.', 'We demonstrate our methods efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32  32.']","[0, 0, 1, 0, 0]","[0.05714285373687744, 0.12121211737394333, 0.23076923191547394, 0.0833333283662796, 0.05714285373687744]",BJe0Gn0cY7,"[' Avoid posterior collapse by lower bounding the rate.', 'Presents an approach to preventing posterior collapse in VAEs by limiting the family of the variational approximation to the posterior', 'This paper introduces a constraint on the family of variational posteriors such that the KL term can be controlled to combat posterior collapse in deep generative models such as VAEs']","['due phenomenon  posterior collapse   current latent variable generative model pose challenging design choice either weakens capacity decoder requires altering training objective ', 'develop alternative utilizes powerful generative model decoder  optimize variational lower bound  ensures latent variable preserve encode useful information ', 'proposed vaes achieve constraining variational family posterior minimum distance prior ', 'sequential latent variable model  approach resembles classic representation learning approach slow feature analysis ', 'demonstrate method  efficacy modeling text lm1b modeling image  learning representation  improving sample quality  achieving state art loglikelihood cifar10 imagenet 32  32 ']","Due to the phenomenon of posterior collapse, current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective., We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information., Our proposed -VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior., For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis., We demonstrate our methods efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32  32.",10,5.876923076923077,13.0
88,"['Mini-batch gradient descent and its variants are commonly used in deep learning.', 'The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient.', 'However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network.', 'Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds.', 'We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification.', 'Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data.', 'Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes.', 'We also identified an interesting phenomenon, batch size boom.', 'The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.0476190447807312, 0.2142857164144516, 0.17391303181648254, 0.23529411852359772, 0.18518517911434174, 0.6666666865348816, 0.3636363446712494, 0.1538461446762085, 0.1304347813129425]",SybqeKgA-,"['We developed a batch adaptive momentum that can achieve lower loss compared with mini-batch methods after scanning same epochs of data, and it is more robust against large step size.', 'This paper addresses the problem of automatically tuning batch size during deep learning training, and claims to extend batch adaptive SGD to adaptive momentum and adopt the algorithms to complex neural networks problems.', 'The paper proposes generalizing an algorithm which performs SGD with adaptive batch sizes by adding momentum to the utility function']","['minibatch gradient descent variant commonly used deep learning ', 'principle minibatch gradient descent use noisy gradient calculated batch estimate real gradient  thus balancing computation cost per iteration uncertainty noisy gradient ', 'however  batch size fixed hyperparameter requiring manual setting training neural network ', 'yin et al   2017  proposed batch adaptive stochastic gradient descent  basgd  dynamically choose proper batch size learning proceeds ', 'extend basgd momentum algorithm evaluate basgd batch adaptive momentum  bamomentum  two deep learning task natural language processing image classification ', 'experiment confirm batch adaptive method achieve lower loss compared minibatch method scanning epoch data ', 'furthermore  bamomentum robust larger step size  dynamically enlarge batch size reduce larger uncertainty brought larger step size ', 'also identified interesting phenomenon  batch size boom ', 'code implementing batch adaptive framework open source  applicable gradientbased optimization problem ']","Mini-batch gradient descent and its variants are commonly used in deep learning., The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient., However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network., Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds., We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification., Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data., Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes., We also identified an interesting phenomenon, batch size boom., The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.",15,5.678947368421053,11.875
89,"['Deep learning models for graphs have advanced the state of the art on many tasks.', 'Despite their recent success, little is known about their robustness.', 'We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  ', 'Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.', 'Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings.', 'Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information.', 'Our attacks do not assume any knowledge about or access to the target classifiers.']","[0, 0, 1, 0, 0, 0, 0]","[0.2857142686843872, 0.0, 0.375, 0.22857142984867096, 0.10810810327529907, 0.1538461446762085, 0.1428571343421936]",Bylnx209YX,"['We use meta-gradients to attack the training procedure of deep neural networks for graphs.', 'Studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. ', 'An algorithm to alter graph structure by adding/deleting edges so as to degrade the global performance of node classification, and the idea to use meta-learning to solve the bilevel optimization problem.']","['deep learning model graph advanced state art many task ', 'despite recent success  little known robustness ', 'investigate training time attack graph neural network node classification perturb discrete graph structure ', 'core principle use metagradients solve bilevel problem underlying trainingtime attack  essentially treating graph hyperparameter optimize ', 'experiment show small graph perturbation consistently lead strong decrease performance graph convolutional network  even transfer unsupervised embeddings ', 'remarkably  perturbation created algorithm misguide graph neural network perform worse simple baseline ignores relational information ', 'attack assume knowledge access target classifier ']","Deep learning models for graphs have advanced the state of the art on many tasks., Despite their recent success, little is known about their robustness., We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  , Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize., Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings., Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information., Our attacks do not assume any knowledge about or access to the target classifiers.",11,5.796992481203008,12.090909090909092
90,"['Numerous models for grounded language understanding have been recently proposed, including', '(i) generic models that can be easily adapted to any given task and', '(ii) intuitively appealing modular models that require background knowledge to be instantiated.', 'We compare both types of models in how much they lend themselves to a particular form of systematic generalization.', 'Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them.', 'Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected.', 'We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization.', 'We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization.', 'Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.\n']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.060606054961681366, 0.17142856121063232, 0.1764705777168274, 0.29999998211860657, 0.12765957415103912, 0.40816324949264526, 0.4285714328289032, 0.24390242993831635, 0.1463414579629898]",HkezXnA9YX,"[""We show that modular structured models are the best in terms of systematic generalization and that their end-to-end versions don't generalize as well."", 'This paper evaluates systemic generalization between modular neural networks and otherwise generic models via introduction of a new, spatial reasoning dataset', 'A targeted empirical evaluation of generalization in models for visual reasoning, focused on the problem of recognizing (object, relation, object) triples in synthetic scenes featuring letters and numbers.']","['numerous model grounded language understanding recently proposed  including', '  generic model easily adapted given task', ' ii  intuitively appealing modular model require background knowledge instantiated ', 'compare type model much lend particular form systematic generalization ', 'using synthetic vqa test  evaluate model capable reasoning possible object pair training small subset ', 'finding show generalization modular model much systematic highly sensitive module layout  ie  exactly module connected ', 'furthermore investigate modular model generalize well could made endtoend learning layout parametrization ', 'find endtoend method prior work often learn inappropriate layout parametrizations facilitate systematic generalization ', 'result suggest  addition modularity  systematic generalization language understanding may require explicit regularizers prior ']","Numerous models for grounded language understanding have been recently proposed, including, (i) generic models that can be easily adapted to any given task and, (ii) intuitively appealing modular models that require background knowledge to be instantiated., We compare both types of models in how much they lend themselves to a particular form of systematic generalization., Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them., Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected., We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization., We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization., Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.
",14,5.6976744186046515,11.466666666666667
91,"['The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them.', ""Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments."", ""Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions."", 'Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. \n', 'As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary.', 'Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.']","[0, 1, 0, 0, 0, 0]","[0.23529411852359772, 0.5106382966041565, 0.25, 0.1428571343421936, 0.23076923191547394, 0.09302324801683426]",rJlEojAqFm,"[""Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents."", 'A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. ', 'Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations']","['behavioral dynamic multiagent system rich orderly structure  leveraged understand system  improve artificial agent learn operate ', 'introduce relational forward model  rfm  multiagent learning  network learn make accurate prediction agent  future behavior multiagent environment ', 'model operate discrete entity relation present environment  produce interpretable intermediate representation offer insight drive agent  behavior  event mediate intensity valence social interaction ', 'furthermore  show embedding rfm module inside agent result faster learning system compared nonaugmented baseline ', 'autonomous system develop interact become multiagent nature  developing richer analysis tool characterizing agent make decision increasingly necessary ', 'moreover  developing artificial agent quickly safely learn coordinate one another  human shared environment  crucial ']","The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them., Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments., Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions., Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. 
, As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary., Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.",16,5.963855421686747,10.375
92,"['We show that gradient descent on an unregularized logistic regression\n', 'problem, for almost all separable datasets, converges to the same direction as the max-margin solution.', 'The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss.', 'Furthermore,\n', 'we show this convergence is very slow, and only logarithmic in the\n', 'convergence of the loss itself.', 'This can help explain the benefit\n', 'of continuing to optimize the logistic or cross-entropy loss even\n', 'after the training error is zero and the training loss is extremely\n', 'small, and, as we show, even if the validation loss increases.', 'Our\n', 'methodology can also aid in understanding implicit regularization\n', 'in more complex models and with other optimization methods.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3030303120613098, 0.2702702581882477, 0.1702127605676651, 0.05714285373687744, 0.1428571343421936, 0.06896551698446274, 0.24242423474788666, 0.0624999962747097, 0.05882352590560913, 0.0, 0.0]",r1q7n9gAb,"['The normalized solution of gradient descent on logistic regression (or a similarly decaying loss) slowly converges to the L2 max margin solution on separable data.', 'The paper offers a formal proof that gradient descent on the logistic loss converges very slowly to the hard SVM solution in the case where the data are linearly separable. ', 'This paper focuses on characterising the behaviour of log loss minimisation on linearly separable data, and shows that log-loss, minimised with gradient descent, leads to convergence to the max-margin solution.']","['show gradient descent unregularized logistic regression', 'problem  almost separable datasets  converges direction maxmargin solution ', 'result generalizes also monotone decreasing loss function infimum infinity  also discus multiclass generalization cross entropy loss ', 'furthermore ', 'show convergence slow  logarithmic', 'convergence loss ', 'help explain benefit', 'continuing optimize logistic crossentropy loss even', 'training error zero training loss extremely', 'small   show  even validation loss increase ', '', 'methodology also aid understanding implicit regularization', 'complex model optimization method ']","We show that gradient descent on an unregularized logistic regression
, problem, for almost all separable datasets, converges to the same direction as the max-margin solution., The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss., Furthermore,
, we show this convergence is very slow, and only logarithmic in the
, convergence of the loss itself., This can help explain the benefit
, of continuing to optimize the logistic or cross-entropy loss even
, after the training error is zero and the training loss is extremely
, small, and, as we show, even if the validation loss increases., Our
, methodology can also aid in understanding implicit regularization
, in more complex models and with other optimization methods.",20,5.440944881889764,6.35
93,"['Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.', 'For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier.', 'Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.', 'This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals.', 'To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image.', ""Then we introduce two techniques for improving our networks' out-of-sample performance."", 'The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable.', ""The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.\n"", 'We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.145454540848732, 0.13636362552642822, 1.0, 0.0, 0.14035087823867798, 0.054054051637649536, 0.1666666567325592, 0.08695651590824127, 0.1666666567325592]",rJEjjoR9K7,"['Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.', 'A domain generalization approach to reveal semantic information based on a linear projection scheme from CNN and NGLCM output layers.', 'The paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks']","['despite impressive performance evaluated iid  holdout data  deep neural network depend heavily superficial statistic training data liable break distribution shift ', 'example  subtle change background texture image break seemingly powerful classifier ', 'building previous work domain generalization  hope produce classifier generalize previously unseen domain  even domain identifier available training ', 'setting challenging model may extract many distributionspecific  superficial  signal together distributionagnostic  semantic  signal ', 'overcome challenge  incorporate graylevel cooccurrence matrix  glcm  extract pattern prior knowledge suggests superficial  sensitive texture unable capture gestalt image ', 'introduce two technique improving network  outofsample performance ', 'first method built reverse gradient method push model learn representation glcm representation predictable ', 'second method built independence introduced projecting model representation onto subspace orthogonal glcm representation ', 'test method battery standard domain generalization data set  interestingly  achieve comparable better performance compared domain generalization method explicitly require sample target distribution training ']","Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift., For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier., Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training., This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals., To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image., Then we introduce two techniques for improving our networks' out-of-sample performance., The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable., The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.
, We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.",16,6.017857142857143,13.176470588235293
94,"['In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.', 'In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors.', 'Our approach alternates between two threads.', 'In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles.', 'In the second, we minimize the approximated detection score by searching for the optimal camouflage.', 'Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.']","[1, 0, 0, 0, 0, 0]","[0.3499999940395355, 0.24390242993831635, 0.0, 0.2666666507720947, 0.11764705181121826, 0.3265306055545807]",SJgEl3A5tm,"['We propose a method to learn physical vehicle camouflage to adversarially attack object detectors in the wild. We find our camouflage effective and transferable.', 'The authors investigate the problem of learning a camouflage pattern which, when applied to a simulated vehicle, will prevent an object detector from detecting it.', 'This paper targets adversarial learning for interfering car detection by learning camouflage patterns']","['paper  conduct intriguing experimental study physical adversarial attack object detector wild ', 'particular  learn camouflage pattern hide vehicle detected stateoftheart convolutional neural network based detector ', 'approach alternate two thread ', 'first  train neural approximation function imitate simulator applies camouflage vehicle vehicle detector performs given image camouflaged vehicle ', 'second  minimize approximated detection score searching optimal camouflage ', 'experiment show learned camouflage hide vehicle imagebased detector many test case also generalizes different environment  vehicle  object detector ']","In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild., In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors., Our approach alternates between two threads., In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles., In the second, we minimize the approximated detection score by searching for the optimal camouflage., Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.",12,5.69672131147541,10.166666666666666
95,"['As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows.', 'Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data.', 'On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  ', 'We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  ', 'We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  ', 'We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.']","[0, 0, 0, 1, 0, 0]","[0.0555555522441864, 0.23529411852359772, 0.1860465109348297, 0.2978723347187042, 0.260869562625885, 0.22727271914482117]",rJhR_pxCZ,"['We combine differentiable decision trees with supervised variational autoencoders to enhance interpretability of classification. ', 'This paper proposes a hybrid model of a variational autoencoder composed with a differentiable decision tree, and an accompanying training scheme, with experiments demonstrating tree classification performance, neg. log likelihood performance, and latent space interpretability.', 'The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE and a differentiable decision tree']","['deep learningbased classifier increasingly adopted realworld application  importance understanding particular label chosen grows ', 'single decision tree example simple  interpretable classifier  unsuitable use complex  highdimensional data ', 'hand  variational autoencoder  vae  designed learn factored  lowdimensional representation data  typically encodes highlikelihood data intrinsically nonseparable way ', 'introduce differentiable decision tree  ddt  modular component deep network simple  differentiable loss function allows endtoend optimization deep network compress highdimensional data classification single decision tree ', 'also explore power labeled data supervised vae  svae  gaussian mixture prior  leverage label information produce highquality generative model improved bound loglikelihood ', 'combine svae ddt get classifiervae  cvae   competitive classification error loglikelihood  despite optimizing simultaneously using simple encoderdecoder architecture ']","As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows., Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data., On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  , We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  , We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  , We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.",17,6.0,10.235294117647058
96,"['We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.', 'We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples.', 'We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class.', 'To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available.', 'Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights.', 'Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.']","[1, 0, 0, 0, 0, 0]","[0.3478260934352875, 0.1428571343421936, 0.21739129722118378, 0.1860465109348297, 0.14999999105930328, 0.13636362552642822]",rJl0r3R9KX,"['A practical and provably guaranteed approach for training efficiently classifiers in the presence of label shifts between Source and Target data sets', 'The authors propose a new algorithm for improving the stability of class importance weighting estimation procedure with a two-step procedure.', 'The authors consider the problem of learning under label shifts, where label proportions differ while conditionals are equal, and propose an improved estimator with regularization.']","['propose regularized learning label shift  rlls   principled practical domainadaptation algorithm correct shift label distribution source target domain ', 'first estimate importance weight using labeled source data unlabeled target data  train classifier weighted source sample ', 'derive generalization bound classifier target domain independent  ambient  data dimension  instead depends complexity function class ', 'best knowledge  first generalization bound labelshift problem label target domain available ', 'based bound  propose regularized estimator smallsample regime account uncertainty estimated weight ', 'experiment cifar10 mnist datasets show rlls improves classification accuracy  especially low sample largeshift regime  compared previous method ']","We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain., We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples., We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class., To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available., Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights., Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.",13,5.484076433121019,12.076923076923077
97,"['The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset.', 'This is because the real visual world has a few classes that are common while others are rare.', 'Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset.', 'To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes.', 'To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.', 'The proposed method is simple and easy-to-implement in existing deep learning frameworks.', 'Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.']","[0, 0, 0, 0, 0, 0, 1]","[0.1875, 0.14814814925193787, 0.07692307233810425, 0.21052631735801697, 0.20000000298023224, 0.09090908616781235, 0.22857142984867096]",Bk9nkMa4G,"['Approach to improve classification accuracy on classes in the tail.', 'The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution.', 'Proposal for a Bayesian framework with a Gaussian mixture model to address an issue in classification applications, that the number of training data from different classes is unbalanced.']","['statistic real visual world present longtailed distribution  class significantly training instance remaining class dataset ', 'real visual world class common others rare ', 'unfortunately  performance convolutional neural network typically unsatisfactory trained using longtailed dataset ', 'alleviate issue  propose method discriminatively learns embedding simple bayesian classifier balance classpriors generalize well rare class ', 'end  proposed approach us gaussian mixture model factor classlikelihoods classpriors longtailed dataset ', 'proposed method simple easytoimplement existing deep learning framework ', 'experiment publicly available datasets show proposed approach improves performance class training instance  maintaining comparable performance stateoftheart class abundant training example ']","The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset., This is because the real visual world has a few classes that are common while others are rare., Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset., To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes., To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset., The proposed method is simple and easy-to-implement in existing deep learning frameworks., Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.",11,5.878205128205129,14.181818181818182
98,"['As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents.', 'Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior.', 'However, this topic has been relatively under-explored in the reinforcement learning community.', 'In this work we present a method for synthesizing states of interest for a trained agent.', 'Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary.', 'Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system.', 'To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.', 'In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods.', 'We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator.', 'Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.25, 0.1621621549129486, 0.19354838132858276, 0.42424240708351135, 0.1621621549129486, 0.21276594698429108, 0.23255813121795654, 0.29999998211860657, 0.11428570747375488, 0.08163265138864517]",BJf9k305Fm,"['We present a method to synthesize states of interest for reinforcement learning agents in order to analyze their behavior. ', 'This paper proposes a generative model of visual observations in RL that is capable of generating observations of interests.', 'An approach for visualizing states of interest that involves a variational autoencoder that learns to reconstruct state space and an optimization step that finds conditioning parameters to generate synthetic images.']","['deep reinforcement learning applied task  growing need better understand probe learned agent ', 'visualizing understanding decision making process valuable comprehend identify problem learned behavior ', 'however  topic relatively underexplored reinforcement learning community ', 'work present method synthesizing state interest trained agent ', 'state could situation  eg  crashing damaging car  specific action necessary ', ' critical state high low reward achieved  eg  risky state  often interesting understand situational awareness system ', 'end  learn generative model state space environment use latent space optimize target function state interest ', 'experiment show method generate insightful visualization variety environment reinforcement learning method ', 'explore issue standard atari benchmark game well autonomous driving simulator ', 'based efficiency able identify significant decision scenario technique  believe general approach could serve important tool ai safety application ']","As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents., Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior., However, this topic has been relatively under-explored in the reinforcement learning community., In this work we present a method for synthesizing states of interest for a trained agent., Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary., Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system., To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest., In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods., We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator., Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.",15,5.247747747747748,13.058823529411764
99,"['We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.', 'This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples.', 'We show that such deep abstaining classifiers can:', '(i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;', '(ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and', '(iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes.', 'We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.']","[1, 0, 0, 0, 0, 0, 0]","[0.47999998927116394, 0.13636362552642822, 0.1666666567325592, 0.2181818187236786, 0.31111109256744385, 0.25, 0.3492063581943512]",rJxF73R9tX,"['A deep abstaining neural network trained with a novel loss function that learns representations for when to abstain enabling robust learning in the presence of different types of noise.', 'A new loss function for training a deep neural network which can abstain, with performance looked at from angles in existence of structured noise, in existence of unstructured noise, and open world detection.', 'This manuscript introduces deep abstaining classifiers which modifies the multiclass cross-entropy loss with an abstention loss, which is then applied to perturbed image classification tasks']","['introduce deep abstaining classifier  deep neural network trained novel loss function provides abstention option training ', 'allows dnn abstain confusing difficulttolearn example improving performance nonabstained sample ', 'show deep abstaining classifier ', '  learn representation structured noise  noisy training label confusing example correlated underlying feature  learn abstain based feature ', ' ii  enable robust learning presence arbitrary unstructured noise identifying noisy sample ', ' iii  used effective outofcategory detector learns reliably abstain presented sample unknown class ', 'provide analytical result loss function behavior enable automatic tuning accuracy coverage  demonstrate utility deep abstaining classifier using multiple image benchmark  result indicate significant improvement learning presence label noise ']","We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training., This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples., We show that such deep abstaining classifiers can:, (i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;, (ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and, (iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes., We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.",9,5.828025477707007,17.444444444444443
100,"['Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  ', 'However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability.', 'In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence.', 'This approach can be easily applied to both linear and nonlinear function approximators. \n', 'HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.']","[0, 0, 1, 0, 0]","[0.0624999962747097, 0.1599999964237213, 0.1621621549129486, 0.0, 0.1249999925494194]",rylbWhC5Ym,"['A regularization technique for TD learning that avoids temporal over-generalization, especially in Deep Networks', 'A variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states.', 'The paper introduces HR-TD, a variation of the TD(0) algorithm, meant to improve the over-generalization problem in conventional TD']","['temporal difference learning function approximation widely used recently led several successful result ', 'however  compared original tabularbased method  one major drawback temporal difference learning neural network function approximators tend overgeneralize across temporally successive state  resulting slow convergence even instability ', 'work  propose novel td learning method  hadamard product regularized td  hrtd   reduces overgeneralization thus lead faster convergence ', 'approach easily applied linear nonlinear function approximators ', 'hrtd evaluated several linear nonlinear benchmark domain  show improvement learning behavior performance ']","Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  , However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability., In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence., This approach can be easily applied to both linear and nonlinear function approximators. 
, HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.",12,6.151785714285714,9.333333333333334
101,"['We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. \n', 'To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters.', 'Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation.', 'As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters.', 'We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation.', 'Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.']","[0, 0, 0, 0, 0, 1]","[0.3571428656578064, 0.0, 0.17391303181648254, 0.037735845893621445, 0.2545454502105713, 0.38805970549583435]",Bkl-43C9FQ,"['We present a new CNN kernel for unstructured grids for spherical signals, and show significant accuracy and parameter efficiency gain on tasks such as 3D classfication and omnidirectional image segmentation.', 'An efficient method enabling deep learning on spherical data that reaches competitive/state-of-the-art numbers with much less parameters than popular approaches.', 'The paper proposes a novel convolutional kernel for CNN on the unstructured grids and formulates the convolution by a linear combination of differential operators.']","['present efficient convolution kernel convolutional neural network  cnns  unstructured grid using parameterized differential operator focusing spherical signal panorama image planetary signal ', 'end  replace conventional convolution kernel linear combination differential operator weighted learnable parameter ', 'differential operator efficiently estimated unstructured grid using onering neighbor  learnable parameter optimized standard backpropagation ', 'result  obtain extremely efficient neural network match outperform stateoftheart network architecture term performance significantly lower number network parameter ', 'evaluate algorithm extensive series experiment variety computer vision climate science task  including shape classification  climate pattern segmentation  omnidirectional image semantic segmentation ', 'overall  present  1  novel cnn approach unstructured grid using parameterized differential operator spherical signal   2  show unique kernel parameterization allows model achieve higher accuracy significantly fewer network parameter ']","We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. 
, To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters., Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation., As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters., We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation., Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.",14,6.416184971098266,12.357142857142858
102,"['Prediction is arguably one of the most basic functions of an intelligent system.', 'In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult.', 'However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up.', 'To exploit this, we decouple visual prediction from a rigid notion of time.', 'While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable ""bottleneck"" frames no matter when they occur.', 'We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks.', 'Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.']","[0, 0, 0, 0, 0, 0, 1]","[0.0, 0.08510638028383255, 0.05882352590560913, 0.09090908616781235, 0.1269841194152832, 0.1304347813129425, 0.19607841968536377]",SyzVb3CcFX,"['In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning.', 'A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction.', 'Reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead is trained to generate frames that happen at any point in the future.']","['prediction arguably one basic function intelligent system ', 'general  problem predicting event future two waypoints exceedingly difficult ', 'however  phenomenon naturally pas relatively predictable bottleneck  predict precise trajectory robot arm rest holding object  certain must picked object ', 'exploit  decouple visual prediction rigid notion time ', 'conventional approach predict frame regularly spaced temporal interval  timeagnostic predictor  tap  tied specific time may instead discover predictable  bottleneck  frame matter occur ', 'evaluate approach future intermediate frame prediction across three robotic manipulation task ', 'prediction higher visual quality  also correspond coherent semantic subgoals temporally extended task ']","Prediction is arguably one of the most basic functions of an intelligent system., In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult., However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up., To exploit this, we decouple visual prediction from a rigid notion of time., While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable ""bottleneck"" frames no matter when they occur., We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks., Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.",13,5.598684210526316,11.692307692307692
103,"['In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly.', ""We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process."", 'First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes.', ""Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location."", 'Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior.', 'We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.\n']","[0, 1, 0, 0, 0, 0]","[0.1904761791229248, 0.3333333432674408, 0.2857142686843872, 0.31111109256744385, 0.08510638028383255, 0.1538461446762085]",ryBnUWb0b,"[""We used an LSTM to detect when a smartphone walks into a building. Then we predict the device's floor level using data from sensors aboard the smartphone."", ""The paper introduces a system to estimate a floor-level via their mobile device's sensor data using an LSTM and changes in barometric pressure"", 'Proposal for a two-step method to determine which floor a mobile phone is on inside a tall building.']","['city tall building  emergency responder need accurate floor level location find 911 caller quickly ', 'introduce system estimate victim floor level via mobile device sensor data twostep process ', 'first  train neural network determine smartphone enters exit building via gps signal change ', 'second  use barometer equipped smartphone measure change barometric pressure entrance building victim indoor location ', 'unlike impractical previous approach  system first require use beacon  prior knowledge building infrastructure  knowledge user behavior ', 'demonstrate realworld feasibility 63 experiment across five different tall building throughout new york city system predicted correct floor level 100  accuracy ']","In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly., We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process., First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes., Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location., Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior., We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.
",12,5.405797101449275,11.5
104,"['Sparse reward is one of the most challenging problems in reinforcement learning (RL).', 'Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals.', 'Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation.', ""We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation."", 'We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn.', 'We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons.', 'We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.']","[0, 0, 0, 0, 0, 1, 0]","[0.0, 0.07407406717538834, 0.1666666567325592, 0.1875, 0.14999999105930328, 0.2222222238779068, 0.05128204822540283]",HyM8V2A9Km,"['Combine language goal representation with hindsight experience replays.', 'This paper considers the assumption implicit in hindsight experience replay, that there is access to a mapping from states to goals, and proposes a natural language goal representation.', 'This submission uses Hindsight Experience Replay framework with natural language goals to improve the sample-efficiency of instruction-following models.']","['sparse reward one challenging problem reinforcement learning  rl  ', 'hindsight experience replay   attempt address issue converting failure experience successful one relabeling goal ', 'despite effectiveness  limited applicability lack compact universal goal representation ', 'present augmenting experience via teacher advice  actrce   efficient reinforcement learning technique extends framework using natural language goal representation ', 'first analyze difference among goal representation  show actrce efficiently solve difficult reinforcement learning problem challenging 3d navigation task  whereas nonlanguage goal representation failed learn ', 'also show language goal representation  agent generalize unseen instruction  even generalize instruction unseen lexicon ', 'demonstrate crucial use hindsight advice solve challenging task  also found little amount hindsight advice sufficient learning take  showing practical aspect method ']","Sparse reward is one of the most challenging problems in reinforcement learning (RL)., Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals., Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation., We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation., We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn., We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons., We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.",15,5.883040935672515,11.4
105,"['Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval.', 'Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected.', 'Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features.', 'Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances.', 'This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances.', 'Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost.', 'This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.']","[0, 0, 0, 0, 0, 1, 0]","[0.0555555522441864, 0.07843136787414551, 0.06451612710952759, 0.15789473056793213, 0.25, 0.29411762952804565, 0.1666666567325592]",B1e0KsRcYQ,"['We propose a joint codebook and factorization scheme to improve second order pooling.', 'This paper presents a way to combine existing factorized second order representations with a codebook style hard assignment.', 'Proposal for a novel bilinear representation based on a codebook model, and an efficient formulation in which codebook-based projections are factorized via shared projection to further reduce parameter size.']","['learning rich compact representation open topic many field word embedding  visual questionanswering  object recognition image retrieval ', 'although deep neural network  convolutional  made major breakthrough last year providing hierarchical  semantic abstract representation task  representation necessary rich needed compact expected ', 'model using higher order statistic  bilinear pooling  provide richer representation cost higher dimensional feature ', 'factorization scheme proposed without able reach original compactness first order model  heavy loss performance ', 'paper address two point extending factorization scheme codebook strategy  allowing compact representation dimensionality first order representation  second order performance ', 'moreover  extend framework joint codebook factorization scheme  granting reduction term parameter computation cost ', 'formulation lead stateoftheart result compact secondorder model additional parameter intermediate representation dimension similar firstorder statistic ']","Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval., Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected., Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features., Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances., This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances., Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost., This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.",18,6.031914893617022,10.444444444444445
106,"['Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.', 'Such models often outperform their simpler counterparts significantly.', 'However, their performance relies on the availability of large amounts of labeled data, which are rarely available.', 'To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.', 'We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.', 'We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.', 'Finally, we discuss the optimal setting for applying weak supervision using this methodology.']","[0, 0, 0, 0, 1, 0, 0]","[0.1111111044883728, 0.0, 0.15789473056793213, 0.2222222238779068, 0.26923075318336487, 0.12765957415103912, 0.17142856121063232]",rygDeZqap7,"['We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction.', 'A semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset and applies some of them to annotate unlabeled examples for semi-supervised learning.', 'This paper addresses the problem of generating training data for biological relation extraction, and uses predictions from data labeled by weak classifiers as additional training data for a meta learning algorithm.', 'This paper proposes a combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task']","['natural language understanding research recently shifted towards complex machine learning deep learning algorithm ', 'model often outperform simpler counterpart significantly ', 'however  performance relies availability large amount labeled data  rarely available ', 'tackle problem  propose methodology extending training datasets arbitrarily big size training complex  datahungry model using weak supervision ', 'apply methodology biomedical relation extraction  task training datasets excessively timeconsuming expensive create  yet major impact downstream application drug discovery ', 'demonstrate two smallscale controlled experiment method consistently enhances performance lstm network  performance improvement comparable handlabeled training data ', 'finally  discus optimal setting applying weak supervision using methodology ']","Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms., Such models often outperform their simpler counterparts significantly., However, their performance relies on the availability of large amounts of labeled data, which are rarely available., To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision., We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery., We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data., Finally, we discuss the optimal setting for applying weak supervision using this methodology.",15,6.355555555555555,9.0
107,"['We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations.', 'CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations.', 'Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly.', 'Our approach offers two major advantages:', '(i) for each prediction, valid instance-specific explanations are generated with no computational overhead and', '(ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings.', 'We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations.', 'Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.2857142686843872, 0.31111109256744385, 0.1111111044883728, 0.0, 0.05405404791235924, 0.1621621549129486, 0.25, 0.1538461446762085]",HJUOHGWRb,"['A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability.', 'The authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable.', 'Proposal for a combination of neural nets and graphical models by using a deep neural net to predict the parameters of a graphical model.']","['introduce contextual explanation network  cens   class model learn predict generating leveraging intermediate explanation ', 'cens deep network generate parameter contextspecific probabilistic graphical model used prediction play role explanation ', 'contrary existing posthoc modelexplanation tool  cens learn predict explain jointly ', 'approach offer two major advantage ', '  prediction  valid instancespecific explanation generated computational overhead', ' ii  prediction via explanation act regularization boost performance lowresource setting ', 'prove local approximation decision boundary network consistent generated explanation ', 'result image text classification survival analysis task demonstrate cens competitive stateoftheart offering additional insight behind prediction  valuable decision support ']","We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations., CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations., Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly., Our approach offers two major advantages:, (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and, (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings., We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations., Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.",11,6.392857142857143,12.727272727272727
108,"['The goal of imitation learning (IL) is to enable a learner to imitate an experts behavior given the experts demonstrations.', 'Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.', 'However, GAIL requires a huge number of interactions with environment during training.', 'We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.', 'To this end, we propose a model free, off-policy IL algorithm for continuous control.', 'The keys of our algorithm are two folds:', '1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),', '2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the experts demonstrations.', 'Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.2926829159259796, 0.10256409645080566, 0.4000000059604645, 0.3414634168148041, 0.21621620655059814, 0.12903225421905518, 0.2380952388048172, 0.19607841968536377, 0.2800000011920929]",rJ3fy0k0Z,"['We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.', 'Proposes to extend the determinist policy gradient algorithm to learn from demonstrations, while combined with a type of density estimation of the expert.', 'This paper considers the problem of model-free imitation learning and proposes an extension of the generative adversarial imitation learning algorithm by replacing the stochastic policy of the learner with a deterministic one.', 'The paper combines IRL, adversarial training, and ideas from deterministic policy gradients with the goal of decreasng sample complexity']","['goal imitation learning  il  enable learner imitate expert  behavior given expert  demonstration ', 'recently  generative adversarial imitation learning  gail  successfully achieved even complex continuous control task ', 'however  gail requires huge number interaction environment training ', 'believe il algorithm could applicable realworld environment number interaction could reduced ', 'end  propose model free  offpolicy il algorithm continuous control ', 'key algorithm two fold ', '1  adopting deterministic policy allows u derive novel type policy gradient call deterministic policy imitation gradient  dpig  ', '2  introducing function call state screening function  ssf  avoid noisy policy update state typical appeared expert  demonstration ', 'experimental result show algorithm achieve goal il least ten time le interaction gail variety continuous control task ']","The goal of imitation learning (IL) is to enable a learner to imitate an experts behavior given the experts demonstrations., Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks., However, GAIL requires a huge number of interactions with environment during training., We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced., To this end, we propose a model free, off-policy IL algorithm for continuous control., The keys of our algorithm are two folds:, 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),, 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the experts demonstrations., Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.",13,5.424418604651163,13.23076923076923
109,"['Convolution acts as a local feature extractor in convolutional neural networks (CNNs).', 'However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.', 'This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.', 'The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.', 'The outputs are the weighted sum of these filters outputs, extraction of both vertex features and strength of correlation between vertices.', 'It\n', 'can be used with both directed and undirected graphs.', 'The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.', 'Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a number\n', 'of data sets.', 'As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.11428570747375488, 0.09090908616781235, 0.0555555522441864, 0.0, 0.09999999403953552, 0.15789473056793213, 0.06451612710952759, 0.0, 0.0]",H113pWZRb,"['Low computational complexity graph CNN (without approximation) with better classification accuracy', 'Proposes a new CNN approach to graph classification using a filter based on outgoing walks of increasing length to incorporate information from more distant vertices in one propagation step.', 'Proposal for a new neural network architecture for semi-supervised graph classification, building upon graph polynomial filters and utilizing them on successive neural network layers with ReLU activation functions.', 'The paper introduces Topology Adaptive GCN to generalize convolutional networks to graph-structured data']","['convolution act local feature extractor convolutional neural network  cnns  ', 'however  convolution operation applicable input data supported irregular graph social network  citation network  knowledge graph ', 'paper proposes topology adaptive graph convolutional network  tagcn   novel graph convolutional network generalizes cnn architecture graphstructured data provides systematic way design set fixedsize learnable filter perform convolution graph ', 'topology filter adaptive topology graph scan graph perform convolution  replacing square filter gridstructured data traditional cnns ', 'output weighted sum filter  output  extraction vertex feature strength correlation vertex ', '', 'used directed undirected graph ', 'proposed tagcn inherits property convolution cnn gridstructured data  also consistent convolution defined graph signal processing ', ' approximation convolution needed  tagcn exhibit better performance existing graphconvolutionapproximation method number', 'data set ', 'polynomial degree two adjacency matrix used  tagcn also computationally simpler recent method ']","Convolution acts as a local feature extractor in convolutional neural networks (CNNs)., However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs., This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs., The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs., The outputs are the weighted sum of these filters outputs, extraction of both vertex features and strength of correlation between vertices., It
, can be used with both directed and undirected graphs., The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing., Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a number
, of data sets., As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.",21,5.674418604651163,10.238095238095237
110,"['Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.', 'Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.', ""We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning."", 'Across several benchmark data sets, we find that:', '(i) certain examples are forgotten with high frequency, and some not at all;', ""(ii) a data set's (un)forgettable examples generalize across neural architectures; and"", '(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.']","[0, 0, 0, 0, 0, 0, 1]","[0.15686273574829102, 0.20408162474632263, 0.2545454502105713, 0.05128204822540283, 0.1818181723356247, 0.1428571343421936, 0.4000000059604645]",BJlxm30cKm,"['We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization.', 'Studies the forgetting behavior of training examples during SGD, and shows there exist ""support examples"" in neural network training across different network architectures.', 'This paper analyzes the extent to which networks learn to correctly classify specific examples and then forget these examples over the course of training.', 'The paper studies whether some examples in training neural networks are harder to learn than others. Such examples are forgotten and relearned multiple times through learning.']","['inspired phenomenon catastrophic forgetting  investigate learning dynamic neural network train single classification task ', 'goal understand whether related phenomenon occurs data undergo clear distributional shift ', 'define  forgetting event  occurred individual training example transition classified correctly incorrectly course learning ', 'across several benchmark data set  find ', '  certain example forgotten high frequency  ', ' ii  data set  un  forgettable example generalize across neural architecture ', ' iii  based forgetting dynamic  significant fraction example omitted training data set still maintaining stateoftheart generalization performance ']","Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks., Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift., We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning., Across several benchmark data sets, we find that:, (i) certain examples are forgotten with high frequency, and some not at all;, (ii) a data set's (un)forgettable examples generalize across neural architectures; and, (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.",11,5.901639344262295,11.090909090909092
111,"['Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.', 'This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.', 'In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.', 'These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.', 'The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.', 'We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.', 'We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.', 'Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.', 'An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.1875, 0.12121211737394333, 0.7878788113594055, 0.12903225421905518, 0.1111111044883728, 0.060606054961681366, 0.09090908616781235, 0.1818181723356247, 0.17777776718139648]",B1g6XnCcKQ,"['An unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.', 'Designs a feature representation from video sequences captured from a scene from different view points.', 'Proposal for an unsupervised representation learning method for visual inputs that incorporates a metric learning approach pulling nearest neighbor pairs of image patches close in embedding space while pushing apart other pairs.', 'This paper explores self-supervised learning of object representations, with the main idea to encourage objects with similar features to get further attracted to each other.']","['discovering object attribute great importance autonomous agent effectively operate human environment ', 'task particularly challenging due ubiquitousness object nuance perceptual semantic detail ', 'paper present unsupervised approach learning disentangled representation object entirely unlabeled monocular video ', 'continuous representation biased limited discrete set label determined human labelers ', 'proposed representation trained metric learning loss  object homogeneous feature pushed together  heterogeneous feature pulled apart ', 'show unsupervised embeddings allow discover object attribute enable robot selfsupervise previously unseen environment ', 'quantitatively evaluate performance largescale synthetic dataset 12k object model  well real dataset collected robot show unsupervised object understanding generalizes previously unseen object ', 'specifically  demonstrate effectiveness approach robotic manipulation task  pointing grasping object ', 'interesting perhaps surprising finding approach given limited set object  object correspondence naturally emerge using metric learning without requiring explicit positive pair ']","Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments., This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail., In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos., These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers., The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart., We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments., We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects., Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects., An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.",15,5.976076555023924,13.933333333333334
112,"['Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  ', 'We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  ', 'Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   ', 'We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  ', 'Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.']","[0, 0, 0, 1, 0]","[0.1304347813129425, 0.2745097875595093, 0.2790697515010834, 0.4680851101875305, 0.25]",r1lFYoRcFm,"['We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression.', 'Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space. ', 'This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective.', 'This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.']","['learning scalar reward continuous action space environment difficult often requires million billion interaction ', 'introduce state aligned vector reward  easily defined metric state space allow deep reinforcement learning agent tackle curse dimensionality ', 'agent learns map action distribution state change distribution implicitly defined quantile function neural network ', 'introduce new reinforcement learning technique inspired quantile regression limit agent explicitly parameterized action distribution ', 'result high dimensional state space show training vector reward allows agent learn multiple time faster agent training scalar reward ']","Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  , We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  , Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   , We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  , Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.",6,5.822033898305085,19.666666666666668
113,"['We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.', 'In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.', 'Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.', 'We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.']","[1, 0, 0, 0]","[0.4897959232330322, 0.1818181723356247, 0.1818181723356247, 0.14814814925193787]",BJvWjcgAZ,"['We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.', 'Proposes a new DQN where the targets are computed on a full episode by a backward update (end to start) for faster propagation of rewards by the episode end.', 'The authors propose to modify the DQN algorithm by applying the max Bellman operator recursively on a trajectory with some decay to prevent accumulating errors with the nested max.', 'In deep-Q networks, update Q values starting from the end of the episode in order to facilitate quick propagation of rewards back along the episode.']","['propose episodic backward update  new algorithm boost performance deep reinforcement learning agent fast reward propagation ', 'contrast conventional use replay memory uniform random sampling  agent sample whole episode successively propagates value state previous state ', 'computationally efficient recursive algorithm allows sparse delayed reward propagate effectively throughout sampled episode ', 'evaluate algorithm 2d mnist maze environment 49 game atari 2600 environment show agent improves sample efficiency competitive computational cost ']","We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation., In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states., Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode., We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.",5,5.568627450980392,20.4
114,"['Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc.', 'Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events.', 'This leads to the nonidentifiability of the event times distribution parameters, which makes the problem significantly more challenging.', 'In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.', 'The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks.', 'Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks.', 'Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.']","[0, 0, 0, 1, 0, 0, 0]","[0.290909081697464, 0.16949151456356049, 0.1395348757505417, 0.9473684430122375, 0.20408162474632263, 0.19354838132858276, 0.08163265138864517]",HkjL6MiTb,"['In this work we introduce a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.', 'This paper introduces siamese neural networks to the competing risks framework by optimizing for the c-index directly', 'The authors address issues of estimating risk in a survival analysis setting with competing risks and propose directly optimizing the time-dependent discrimination index using a siamese survival network']","['survival analysis  timetoevent analysis  presence multiple possible adverse event  ie  competing risk  challenging  yet important problem medicine  finance  manufacturing  etc ', 'extending classical survival analysis competing risk trivial since one event  eg  one cause death  observed hence  incidence event interest often obscured related competing event ', 'lead nonidentifiability event time  distribution parameter  make problem significantly challenging ', 'work introduce siamese survival prognosis network  novel siamese deep neural network architecture able effectively learn data presence multiple adverse event ', 'siamese survival network especially crafted issue pairwise concordant timedependent risk  longer event time assigned lower risk ', 'furthermore  architecture able directly optimize approximation cdiscrimination index  rather relying wellknown metric crossentropy etc  able capture unique requirement survival analysis competing risk ', 'result show consistent performance improvement number publicly available medical datasets statistical deep learning stateoftheart method ']","Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc., Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events., This leads to the nonidentifiability of the event times distribution parameters, which makes the problem significantly more challenging., In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events., The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks., Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks., Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.",21,5.908629441624366,8.954545454545455
115,"['The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables.', 'However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables.', 'We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query.', 'We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction.', 'Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary.', 'We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model.', 'We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning.']","[0, 0, 1, 0, 0, 0, 0]","[0.04999999329447746, 0.09999999403953552, 0.2790697515010834, 0.24390242993831635, 0.04651162400841713, 0.2666666507720947, 0.08888888359069824]",BkUDW_lCb,"['We present a type-based pointer network model together with a value-based loss method to effectively train a neural model to translate natural language to SQL.', 'The paper claims to develop a novel method to map natural language queries to SQL by using a grammar to guide decoding and using a new loss function for pointer / copy mechanism']","['digitization data resulted making datasets available million user form relational database spreadsheet table ', 'however  majority user come diverse background lack programming expertise query analyze table ', 'present system allows querying data table using natural language question  system translates question executable sql query ', 'use deep sequence sequence model wich decoder us simple type system sql expression structure output prediction ', 'based type  decoder either copy output token input question using attentionbased copying mechanism generates fixed vocabulary ', 'also introduce valuebased loss function transforms distribution location copy distribution set input token improve training model ', 'evaluate model recently released wikisql dataset show model trained using supervised learning significantly outperforms current stateoftheart seq2sql model us reinforcement learning ']","The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables., However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables., We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query., We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction., Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary., We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model., We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning.",10,5.387978142076503,18.3
116,"['To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.', 'Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.', 'The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space.', 'Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.', 'Python code for reproducible research is publicly available.']","[0, 0, 0, 1, 0]","[0.11764705181121826, 0.2222222238779068, 0.0476190447807312, 0.3499999940395355, 0.10526315122842789]",S1lg0jAcYm,"['An unbiased and low-variance gradient estimator for discrete latent variable models', 'Proposes a new variance-reduction technique to use when computing an expected loss gradient where the expectation is with respect to independent binary random variables.', 'An algorithm combining Rao-Blackwellization and common random numbers for lowering the variance of the score-function gradient estimator in the special case of stochastic binary networks', 'An unbiased and low variance augment-REINFORCE-merge (ARM) estimator for calculating and backpropagating gradients in binary neural networks']","['backpropagate gradient stochastic binary layer  propose augmentreinforcemerge  arm  estimator unbiased  exhibit low variance  low computational complexity ', 'exploiting variable augmentation  reinforce  reparameterization  arm estimator achieves adaptive variance reduction monte carlo integration merging two expectation via common random number ', 'variancereduction mechanism arm estimator also attributed either antithetic sampling augmented space  use optimal antisymmetric  selfcontrol  baseline function together reinforce estimator augmented space ', 'experimental result show arm estimator provides stateoftheart performance autoencoding variational inference maximum likelihood estimation  discrete latent variable model one multiple stochastic binary layer ', 'python code reproducible research publicly available ']","To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity., Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers., The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space., Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers., Python code for reproducible research is publicly available.",13,6.712,9.615384615384615
117,"['Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.', 'The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.', 'To overcome this communication bottleneck recent works propose to reduce the communication frequency.', 'An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.', 'This scheme shows promising results in practice, but eluded thorough theoretical analysis.\n    \n', 'We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.', 'The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.', 'This also holds for asynchronous implementations.\n\n', 'Local SGD can also be used for large scale training of deep learning models.', 'The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.08695651590824127, 0.07407406717538834, 0.20512820780277252, 0.0, 0.3199999928474426, 0.17142856121063232, 0.0, 0.06896550953388214, 0.10526315122842789]",S1g2JnRcFX,"['We prove that parallel local SGD achieves linear speedup with much lesser communication than parallel mini-batch SGD.', 'Provides a convergence proof for local SGD, and proves that local SGD can provide the same speedup gains as minibatch, but may be able to communicate significantly less.', 'This paper presents an analysis of local SGD and bounds on how frequent the estimators obtained by running SGD required to be averaged in order to yield linear parallelization speedups.', 'The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging']","['minibatch stochastic gradient descent  sgd  state art large scale distributed training ', 'scheme reach linear speedup respect number worker  rarely seen practice scheme often suffers large network delay bandwidth limit ', 'overcome communication bottleneck recent work propose reduce communication frequency ', 'algorithm type local sgd run sgd independently parallel different worker average sequence ', 'scheme show promising result practice  eluded thorough theoretical analysis ', 'prove concise convergence rate local sgd convex problem show converges rate minibatch sgd term number evaluated gradient   scheme achieves linear speedup number worker minibatch size ', 'number communication round reduced factor  12   denotes number total step  compared minibatch sgd ', 'also hold asynchronous implementation ', 'local sgd also used large scale training deep learning model ', 'result shown aim serving guideline explore theoretical practical aspect local sgd application ']","Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training., The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits., To overcome this communication bottleneck recent works propose to reduce the communication frequency., An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while., This scheme shows promising results in practice, but eluded thorough theoretical analysis.
    
, We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size., The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD., This also holds for asynchronous implementations.

, Local SGD can also be used for large scale training of deep learning models., The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.",14,5.2274881516587675,15.071428571428571
118,"['Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems.', 'The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams.', 'Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions.', 'We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions.', 'To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation.', 'We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded.', 'We provide theoretical guarantees concerning the convergence of the proposed learning algorithm.', 'To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters.', 'Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems.', 'Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.']","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.0, 0.0, 0.12903225421905518, 0.10810810327529907, 0.05882352590560913, 0.1249999925494194, 0.0, 0.17391304671764374, 0.07692307233810425]",rJgTciR9tm,"['Compact perception of dynamical process', 'Studies the problem of compactly representing the model of a complex dynamic system while preserving information by using an information bottleneck method.', 'This paper studied the Gaussian linear dynamic and proposed an algorithm for computing the Information Bottleneck Hierarchy (IBH).']","['extracting relevant information  causally inferring predicting future state high accuracy crucial task modeling complex system ', 'endeavor address task made even challenging deal highdimensional heterogeneous data stream ', 'data stream often higherorder interdependency across spatial temporal dimension ', 'propose perform softclustering data learn dynamic produce compact dynamical model still ensuring original objective causal inference accurate prediction ', 'efficiently rigorously process dynamic softclustering  advocate information theory inspired approach incorporates stochastic calculus seek determine tradeoff predictive accuracy compactness mathematical representation ', 'cast model construction maximization compression state variable predictive ability causal interdependence  relatedness  constraint original data stream compact model closely bounded ', 'provide theoretical guarantee concerning convergence proposed learning algorithm ', 'test proposed framework  consider highdimensional gaussian case study describe iterative scheme updating new model parameter ', 'using numerical experiment  demonstrate benefit compression prediction accuracy class dynamical system ', 'finally  apply proposed algorithm realworld dataset multimodal sentiment intensity show improvement prediction reduced dimension ']","Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems., The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams., Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions., We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions., To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation., We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded., We provide theoretical guarantees concerning the convergence of the proposed learning algorithm., To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters., Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems., Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.",15,6.140425531914894,15.666666666666666
119,"['We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly.', 'As the density of the connection increases, the number of paths through which the gradient flows can be increased.', 'It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time.', 'Larger gradients, however, can also cause exploding gradient problem.', 'To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows.', 'We describe the relation between the attention gate and the gradient flows by approximation.', 'The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the models performance.']","[1, 0, 0, 0, 0, 0, 0]","[0.7692307829856873, 0.060606054961681366, 0.12121211737394333, 0.0, 0.0555555522441864, 0.0, 0.05405404791235924]",rJVruWZRW,"['Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly.', 'Proposes a new RNN architecture that models long-term dependencies better, can learn multiscale representation of sequential data, and sidestep the gradients problem by using parametrized gating units.', ""This paper proposes a fully connected dense RNN architecture with gated connections to every layer and preceding layer connections, and it's results on PTB charcter-level modelling task.""]","['propose dense rnn  fully connection hidden state multiple preceding hidden state layer directly ', 'density connection increase  number path gradient flow increased ', 'increase magnitude gradient  help prevent vanishing gradient problem time ', 'larger gradient  however  also cause exploding gradient problem ', 'complement tradeoff two problem  propose attention gate  control amount gradient flow ', 'describe relation attention gate gradient flow approximation ', 'experiment language modeling using penn treebank corpus show dense connection attention gate improve model  performance ']","We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly., As the density of the connection increases, the number of paths through which the gradient flows can be increased., It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time., Larger gradients, however, can also cause exploding gradient problem., To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows., We describe the relation between the attention gate and the gradient flows by approximation., The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the models performance.",14,5.520661157024794,8.642857142857142
120,"['We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs).', 'In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose.', 'Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code.', 'Corresponding samples from the real dataset consist of two distinct photographs of the same subject.', 'In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person.', 'We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training.', 'Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithms ability to generate convincing, identity-matched photographs.']","[1, 0, 0, 0, 0, 0, 0]","[0.2790697515010834, 0.11764705181121826, 0.14999999105930328, 0.25806450843811035, 0.15789473056793213, 0.12121211737394333, 0.10810810327529907]",S1nQvfgA-,"['SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).', 'This paper investigates the problem of controlled image generation and proposes an algorithm that produces a pair of images with the same identity.', 'This paper proposes, SD-GAN, a method of training GANs to disentangle the identity and non-identity information in the latent vector input Z.']","['propose new algorithm training generative adversarial network jointly learn latent code identity  eg  individual human  observation  eg  specific photograph  ', 'practice  mean fixing identity portion latent code  generate diverse image subject  fixing observation portion traverse manifold subject maintaining contingent aspect lighting pose ', 'algorithm feature pairwise training scheme sample generator consists two image common identity code ', 'corresponding sample real dataset consist two distinct photograph subject ', 'order fool discriminator  generator must produce image photorealistic  distinct  appear depict person ', 'augment dcgan began approach siamese discriminator accommodate pairwise training ', 'experiment human judge offtheshelf face verification system demonstrate algorithm  ability generate convincing  identitymatched photograph ']","We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs)., In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose., Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code., Corresponding samples from the real dataset consist of two distinct photographs of the same subject., In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person., We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training., Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithms ability to generate convincing, identity-matched photographs.",14,5.8121212121212125,10.3125
121,"['The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains.', 'While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017].', 'In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.', 'Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains.', 'In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods.', 'Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions.', 'Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.']","[0, 0, 0, 0, 1, 0, 0]","[0.09090908616781235, 0.06779660284519196, 0.2666666507720947, 0.09999999403953552, 0.44897958636283875, 0.09999999403953552, 0.0476190410554409]",S1lNELLKuN,"['We propose a learning framework for cross-domain translations which is exactly cycle-consistent and can be learned via adversarial training, maximum likelihood estimation, or a hybrid.', 'Proposes AlignFlow, an efficient way of implementing cycle consistency principle using invertible flows.', 'Flow models for unpaired image to image translation']","['goal unpaired crossdomain translation learn useful mapping two domain  given unpaired set datapoints domain ', 'formulation highly underconstrained  recent work shown possible learn mapping useful downstream task encouraging approximate cycle consistency mapping two domain  zhu et al  2017  ', 'work  propose alignflow  framework unpaired crossdomain translation ensures exact cycle consistency learned mapping ', 'framework us normalizing flow model specify single invertible mapping two domain ', 'contrast prior work cycleconsistent translation  learn alignflow via adversarial training  maximum likelihood estimation  hybrid two method ', 'theoretically  derive consistency result alignflow guarantee recovery desirable mapping suitable assumption ', 'empirically  alignflow demonstrates significant improvement relevant baseline imagetoimage translation unsupervised domain adaptation task benchmark datasets ']","The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains., While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017]., In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings., Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains., In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods., Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions., Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.",17,6.165605095541402,9.235294117647058
122,"['Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.', 'Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.', 'A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.', 'In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.', 'We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.', 'Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.']","[0, 0, 0, 1, 0, 0]","[0.2666666507720947, 0.17777776718139648, 0.23076923191547394, 0.45454543828964233, 0.260869562625885, 0.2857142686843872]",B1CQGfZ0b,"['In a program synthesis context where the input is a set of examples, we reduce the cost by computing a subset of representative examples', 'Proposes a method for identifying representative examples for program synthesis to increase the scalability of existing constraint programming solutions.', 'A method for choosing a subset of examples on which to run a constraint solver in order to solve program synthesis problems.', 'This paper proposes a method for speeding up the general-purpose program synthesizers.']","['program synthesis class regression problem one seek solution  form sourcecode program  map input corresponding output exactly ', 'due precise combinatorial nature  commonly formulated constraint satisfaction problem  inputoutput example expressed constraint  solved constraint solver ', 'key challenge formulation scalability  constraint solver work well wellchosen example  constraining entire set example constitutes significant overhead time memory ', 'paper address challenge constructing representative subset example small able constrain solver sufficiently ', 'build subset one example time  using trained discriminator predict probability unchosen inputoutput example conditioned chosen inputoutput example  adding least probable example subset ', 'experiment diagram drawing domain show approach produce subset example small representative constraint solver ']","Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly., Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver., A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory., In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently., We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset., Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.",14,5.5344827586206895,12.428571428571429
123,"['Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.', 'Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.', 'We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.', 'We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.', 'We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.', 'The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.']","[0, 0, 0, 1, 0, 0]","[0.08510638028383255, 0.290909081697464, 0.2800000011920929, 0.42307692766189575, 0.2745097875595093, 0.3199999928474426]",SkJKHMW0Z,"['We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.', 'Introduced recurrent relational network (RRNs) that can be added to any neural networks to add relational reasoning capacity.', 'Introduction of a deep neural network for structured prediction that achieves state-of-the-art performance on Soduku puzzles and the BaBi task.', 'This paper describes a method called relational network to add relational reasoning capacity to deep neural networks.']","['human posse ability abstractly reason object interaction  ability shared stateoftheart deep learning model ', 'relational network  introduced santoro et al   2017   add capacity relational reasoning deep neural network  limited complexity reasoning task address ', 'introduce recurrent relational network increase suite solvable task require order magnitude step relational reasoning ', 'use recurrent relational network solve sudoku puzzle achieve stateoftheart result solving 966  hardest sudoku puzzle  relational network fail solve ', 'also apply model babi textual qa dataset solving 1920 task competitive stateoftheart sparse differentiable neural computer ', 'recurrent relational network general purpose module augment neural network model capacity manystep relational reasoning ']","Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models., Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address., We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning., We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any., We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers., The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.",11,5.7105263157894735,12.666666666666666
124,"['Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification.', 'In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.', 'We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors.', 'These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data.', 'Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent.', 'Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.']","[0, 0, 0, 0, 0, 1]","[0.0, 0.1428571343421936, 0.2142857164144516, 0.2380952388048172, 0.1860465109348297, 0.2790697515010834]",B1xWcj0qYm,"['Three class priors are all you need to train deep models from only U data, while any two should not be enough.', 'Proposes an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors and discusses theoretical properties of the estimators.', 'A methodology for training any binary classifier from only unlabeled data, and an empirical risk minimization method for two sets of unlabeled data where class priors are given.']","['empirical risk minimization  erm   proper loss function regularization  common practice supervised classification ', 'paper  study training arbitrary  linear deep  binary classifier unlabeled  u  data erm ', 'prove impossible estimate risk arbitrary binary classifier unbiased manner given single set u data  becomes possible given two set u data different class prior ', 'two fact answer fundamental question  minimal supervision training binary classifier u data ', 'following finding  propose ermbased learning method two set u data  prove consistent ', 'experiment demonstrate proposed method could train deep model outperform stateoftheart method learning two set u data ']","Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification., In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM., We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors., These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data., Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent., Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.",12,5.237410071942446,11.583333333333334
125,"['Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions.', 'We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain.', 'Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering.', 'This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features).', 'The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification.', 'Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts.', 'This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.']","[0, 0, 0, 1, 0, 0, 0]","[0.0833333283662796, 0.09756097197532654, 0.1538461446762085, 0.19354838132858276, 0.1818181723356247, 0.18518517911434174, 0.11538460850715637]",SkfMWhAqYQ,"['Aggregating class evidence from many small image patches suffices to solve ImageNet, yields more interpretable models and can explain aspects of the decision-making of popular DNNs.', 'This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features. The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches.']","['deep neural network  dnns  excel many complex perceptual task proven notoriously difficult understand reach decision ', 'introduce highperformance dnn architecture imagenet whose decision considerably easier explain ', 'model  simple variant resnet50 architecture called bagnet  classifies image based occurrence small local image feature without taking account spatial ordering ', 'strategy closely related bagoffeature  bof  model popular onset deep learning reach surprisingly high accuracy imagenet  876  top5 32 x 32 px feature alexnet performance 16 x16 px feature  ', 'constraint local feature make straightforward analyse exactly part image influence classification ', 'furthermore  bagnets behave similar stateofthe art deep neural network vgg16  resnet152 densenet169 term feature sensitivity  error distribution interaction image part ', 'suggests improvement dnns previous bagoffeature classifier last year mostly achieved better finetuning rather qualitatively different decision strategy ']","Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions., We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain., Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering., This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features)., The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification., Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts., This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",12,5.7712765957446805,15.666666666666666
126,"['Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.', 'Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.', 'Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).', 'Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.', 'This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.', 'We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.', 'This method introduces a new domain within bioinformatics and personalized medicine  somatic whole genome mutation calling for liquid biopsy.']","[0, 0, 0, 1, 0, 0, 0]","[0.10526315122842789, 0.1492537260055542, 0.22535210847854614, 0.23333333432674408, 0.1428571343421936, 0.037735845893621445, 0.178571417927742]",H1DkN7ZCZ,"[' Current somatic mutation methods do not work with liquid biopsies (ie low coverage sequencing), we apply a CNN architecture to a unique representation of a read and its ailgnment, we show significant improvement over previous methods in the low frequency setting.', 'Proposes a CNN based solution called Kittyhawk for somatic mutation calling at ultra low allele frequencies.', 'A new algorithm to detect cancer mutations from sequencing cell free DNA that will identify the sequence context that characterize sequencing errors from true mutations.', 'This paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA']","['somatic cancer mutation detection ultralow variant allele frequency  vafs  unmet challenge intractable current stateoftheart mutation calling method ', 'specifically  limit vaf detection closely related depth coverage  due requirement multiple supporting read extant method  precluding detection mutation vafs order magnitude lower depth coverage ', 'nevertheless  ability detect cancerassociated mutation ultra low vafs fundamental requirement lowtumor burden cancer diagnostics application early detection  monitoring  therapy nomination using liquid biopsy method  cellfree dna  ', 'defined spatial representation sequencing information adapted convolutional architecture enables variant detection vafs  manner independent depth sequencing ', 'method enables detection cancer mutation even vafs low 10x4   2 order magnitude current stateoftheart ', 'validated method simulated plasma clinical cfdna plasma sample cancer patient noncancer control ', 'method introduces new domain within bioinformatics personalized medicine  somatic whole genome mutation calling liquid biopsy ']","Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods., Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage., Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA)., Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing., This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art., We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls., This method introduces a new domain within bioinformatics and personalized medicine  somatic whole genome mutation calling for liquid biopsy.",15,5.890625,12.8
127,"['This paper presents the formal release of {\\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts.', 'What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.', 'In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval.', 'To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.']","[1, 0, 0, 0]","[0.4000000059604645, 0.19999998807907104, 0.14999999105930328, 0.04444443807005882]",SylxCx5pTQ,"['The paper introduces a new gold-standard corpus corpus of biomedical scientific literature manually annotated with UMLS concept mentions.', 'Details the construction of a manually annotated dataset covering biomedical concepts that is larger and covered by a larger ontology than previous datasets.', 'This paper uses MedMentions, a TaggerOne semi-Markov model for end-to-end concept recognition and linking on a set of Pubmed abstracts to label papers with biomedical concepts/entities']","['paper present formal release  em medmentions   new manually annotated resource recognition biomedical concept ', 'distinguishes medmentions annotated biomedical corpus size  4000 abstract 350000 linked mention   well size concept ontology  3 million concept umls 2017  broad coverage biomedical discipline ', 'addition full corpus  subcorpus medmentions also presented  comprising annotation subset umls 2017 targeted towards document retrieval ', 'encourage research biomedical named entity recognition linking  data split training testing included release  baseline model metric entity linking also described ']","This paper presents the formal release of {\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts., What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines., In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval., To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.",10,5.520661157024794,12.1
128,"['In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.', 'It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.', 'A clustering network transforms the data into another space and then selects one of the clusters.', 'Next, the autoencoder associated with this cluster is used to reconstruct the data-point.', 'The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.', 'The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.', 'Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.', 'Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.13793103396892548, 0.6666666865348816, 0.1249999925494194, 0.20689654350280762, 0.13333332538604736, 0.32258063554763794, 0.23529411852359772, 0.0]",BJg_fnRqF7,"['We propose a deep clustering method where instead of a centroid each cluster is represented by an autoencoder', 'Presents deep clustering based on a mixture of autoencoders, where data points are allocated to a cluster based the representation error if the autoencoder network were used to represent it.', 'A deep clustering approach that uses an autoencoder framework to learn a low-dimensional embedding of the data simultaneously while clustering data using a deep neural network.', 'A deep clustering method which represents each cluster with different auto-encoders, works in an end-to-end manner, and also can be used to cluster new incoming data without redoing the whole clustering procedure.']","['paper propose deep autoencoder mixture clustering  damic  algorithm ', 'based mixture deep autoencoders cluster represented autoencoder ', 'clustering network transforms data another space selects one cluster ', 'next  autoencoder associated cluster used reconstruct datapoint ', 'clustering algorithm jointly learns nonlinear data representation set autoencoders ', 'optimal clustering found minimizing reconstruction loss mixture autoencoder network ', 'unlike deep clustering algorithm  regularization term needed avoid data collapsing single point ', 'experimental evaluation image text corpus show significant improvement stateoftheart method ']","In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm., It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder., A clustering network transforms the data into another space and then selects one of the clusters., Next, the autoencoder associated with this cluster is used to reconstruct the data-point., The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders., The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network., Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point., Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",10,5.766666666666667,12.0
129,"['We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.', 'The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.', 'We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.', 'The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.', 'Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.', 'We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).', 'We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.', 'Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.3333333432674408, 0.08695651590824127, 0.11764705181121826, 0.0952380895614624, 0.09756097197532654, 0.3243243098258972, 0.1666666567325592, 0.15686273574829102]",SJA7xfb0b,"['We define a new Integral Probability Metric (Sobolev IPM) and show how it can be used for training GANs for text generation and semi-supervised learning.', 'Suggests a novel regularization scheme for GANs based on a Sobolev norm, measuring deviations between L2 norms of derivatives.', 'The authors provide another type of GAN using the typical setup of a GAN but with a different function class, and produce a recipe for training GANs with that sort of function class.', 'The paper proposes a different gradient penalty for GAN critics that forces the expected squared norm of the gradient to be equal to 1']","['propose new integral probability metric  ipm  distribution  sobolev ipm ', 'sobolev ipm compare mean discrepancy two distribution function  critic  restricted sobolev ball defined respect dominant measure mu ', 'show sobolev ipm compare two distribution high dimension based weighted conditional cumulative distribution function  cdf  coordinate leave one basis ', 'dominant measure mu play crucial role defines support conditional cdfs compared ', 'sobolev ipm seen extension one dimensional vonmises cramer statistic high dimensional distribution ', 'show sobolev ipm used train generative adversarial network  gans  ', 'exploit intrinsic conditioning implied sobolev ipm text generation ', 'finally show variant sobolev gan achieves competitive result semisupervised learning cifar10  thanks smoothness enforced critic sobolev gan relates laplacian regularization ']","We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM., The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu., We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis., The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared., Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions., We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs)., We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation., Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.",9,5.481927710843373,18.444444444444443
130,"['We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.', 'The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.', 'The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.', 'A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.', 'Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.', 'The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.', 'We term our method Mixture Generative Adversarial Nets (MGAN).', 'We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators distributions and the empirical data distribution is minimal, whilst the JSD among generators distributions is maximal, hence effectively avoiding the mode collapsing problem.', 'By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.', 'We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.800000011920929, 0.2222222238779068, 0.2380952388048172, 0.2702702581882477, 0.11999999731779099, 0.20408162474632263, 0.07692307233810425, 0.31372547149658203, 0.09999999403953552, 0.16393442451953888]",rkmu5b0a-,"['We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.', 'Address the problem of mode collapse in GANs using a constrained mixture distribution for the generator and an auxiliary classifier which predicts the source mixture component.', 'The paper proposes a mixture of generators to train GANs without extra computational cost', 'The authors present that using MGAN, which aims to overcome model collapsing problem by mixture generators, achieves state-of-art results']","['propose paper new approach train generative adversarial net  gans  mixture generator overcome mode collapsing problem ', 'main intuition employ multiple generator  instead using single one original gan ', 'idea simple  yet proven extremely effective covering diverse data mode  easily overcoming mode collapsing problem delivering stateoftheart result ', 'minimax formulation able establish among classifier  discriminator  set generator similar spirit gan ', 'generator create sample intended come distribution training data  whilst discriminator determines whether sample true data generated generator  classifier specifies generator sample come ', 'distinguishing feature internal sample created multiple generator  one randomly selected final output similar mechanism probabilistic mixture model ', 'term method mixture generative adversarial net  mgan  ', 'develop theoretical analysis prove  equilibrium  jensenshannon divergence  jsd  mixture generator  distribution empirical data distribution minimal  whilst jsd among generator  distribution maximal  hence effectively avoiding mode collapsing problem ', 'utilizing parameter sharing  proposed model add minimal computational cost standard gan  thus also efficiently scale largescale datasets ', 'conduct extensive experiment synthetic 2d data natural image database  cifar10  stl10 imagenet  demonstrate superior performance mgan achieving stateoftheart inception score latest baseline  generating diverse appealing recognizable object different resolution  specializing capturing different type object generator ']","We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem., The main intuition is to employ multiple generators, instead of using a single one as in the original GAN., The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results., A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN., Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from., The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model., We term our method Mixture Generative Adversarial Nets (MGAN)., We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators distributions and the empirical data distribution is minimal, whilst the JSD among generators distributions is maximal, hence effectively avoiding the mode collapsing problem., By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets., We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.",27,5.736301369863014,10.814814814814815
131,"['Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  ', 'Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts.', 'We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.', 'The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty.', 'Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English.', 'The platform also provides a hand-crafted bot agent, which simulates a human teacher.  ', 'We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels.', 'We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.1111111044883728, 0.277777761220932, 0.5405405163764954, 0.20689654350280762, 0.1666666567325592, 0.19999998807907104, 0.22857142984867096, 0.39024388790130615]",rJeXCo0cYX,"['We present the BabyAI platform for studying data efficiency of language learning with a human in the loop', 'Presents a research platform with a bot in the loop for learning to execute language instructions in which language has compositional structures', 'Introduces a platform for grounded language learning that replaces any human in the loop with a heuristic teacher and uses a synthetic language mapped to a 2D grid world']","['allowing human interactively train artificial agent understand language instruction desirable practical scientific reason ', 'though  given lack sample efficiency current learning method  reaching goal may require substantial research effort ', 'introduce babyai research platform  goal supporting investigation towards including human loop grounded language learning ', 'babyai platform comprises extensible suite 19 level increasing difficulty ', 'level gradually lead agent towards acquiring combinatorially rich synthetic language  proper subset english ', 'platform also provides handcrafted bot agent  simulates human teacher ', 'report estimated amount supervision required training neural reinforcement behavioralcloning agent babyai level ', 'put forward strong evidence current deep learning method yet sufficiently sampleefficient context learning language compositional property ']","Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  , Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts., We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning., The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty., Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English., The platform also provides a hand-crafted bot agent, which simulates a human teacher.  , We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels., We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.",13,6.100671140939597,11.461538461538462
132,"['Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance.', 'However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both.', 'In this paper, we propose a novel algorithm that jointly learns and compresses a neural network.', 'The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty.', 'We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints.', 'Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches.']","[0, 0, 0, 0, 0, 1]","[0.0, 0.0, 0.0, 0.0, 0.10526315122842789, 0.190476194024086]",HkinqfbAb,"['A k-means prior combined with L1 regularization yields state-of-the-art compression results.', 'This paper explores soft parameter tying and compression of DNNs/CNNs']","['recently  growing interest method perform neural network compression  namely technique attempt substantially reduce size neural network without significant reduction performance ', 'however  existing method postprocessing approach take learned neural network input output compressed network either forcing several parameter take value  parameter tying via quantization  pruning irrelevant edge  pruning  ', 'paper  propose novel algorithm jointly learns compress neural network ', 'key idea approach change optimization criterion adding  k  independent gaussian prior parameter sparsity penalty ', 'show approach easy implement using existing neural network library  generalizes l1 l2 regularization elegantly enforces parameter tying well pruning constraint ', 'experimentally  demonstrate new algorithm yield stateoftheart compression several standard benchmark minimal loss accuracy requiring little hyperparameter tuning compared related  competing approach ']","Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance., However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both., In this paper, we propose a novel algorithm that jointly learns and compresses a neural network., The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty., We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints., Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches.",13,5.829545454545454,13.538461538461538
133,"['The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success.', 'The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem.', 'We show that naive application of the SVRG technique and related approaches fail, and explore why.']","[0, 1, 0]","[0.07999999821186066, 0.25806450843811035, 0.0833333283662796]",B1MIBs05F7,"['The SVRG method fails on modern deep learning problems', 'This paper presents an analysis of SVRG style methods, showing that dropout, batch norm, data augmentation (random crop/rotation/translations) tend to increase bias and/or variance of the updates.', 'This paper investigates the applicability of SVGD to modern neural networks and shows the naive application of SVGD typically fails.']","['application stochastic variance reduction optimization shown remarkable recent theoretical practical success ', 'applicability technique hard nonconvex optimization problem encountered training modern deep neural network open problem ', 'show naive application svrg technique related approach fail  explore ']","The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success., The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem., We show that naive application of the SVRG technique and related approaches fail, and explore why.",4,6.054545454545455,13.75
134,"['The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks.', 'One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces.', 'In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.', 'A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks.', 'We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.']","[0, 0, 1, 0, 0]","[0.1860465109348297, 0.2926829159259796, 0.8372092843055725, 0.35555556416511536, 0.145454540848732]",rkeSiiA5Fm,"['A method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.', 'Presents a polar anisotropic convolution scheme on a unit sphere by replacing filter translation with filter rotation.', 'This paper explores deep learning of 3D shapes using alt-az anisotropic 2-sphere convolution']","['groundbreaking performance obtained deep convolutional neural network  cnns  image processing task inspiring research effort attempting extend 3d geometric task ', 'one main challenge applying cnns 3d shape analysis define natural convolution operator noneuclidean surface ', 'paper  present method applying deep learning 3d surface using spherical descriptor altaz anisotropic convolution 2sphere ', 'cascade set geodesic disk filter rotate 2sphere collect spherical pattern extract geometric feature various 3d shape analysis task ', 'demonstrate theoretically experimentally proposed method possibility bridge gap 2d image 3d shape desired rotation equivarianceinvariance  effectiveness evaluated application nonrigid rigid shape classification shape retrieval ']","The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks., One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces., In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere., A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks., We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.",7,5.767605633802817,20.285714285714285
135,"['Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs.', 'For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  ', 'One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size.', 'In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters.', 'We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights.', 'Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.']","[0, 0, 0, 1, 0, 0]","[0.06896550953388214, 0.0, 0.17142856121063232, 0.25, 0.1875, 0.06451612710952759]",BySRH6CpW,"['Training binary/ternary networks using local reparameterization with the CLT approximation', 'Trains binary and ternary weight distribution networks using backpropagation to sample neuron pre-activations with reparameterization trick', 'This paper suggests using stochastic parameters in combination with the local reparametrisation trick to train neural networks with binary or ternary weights, which leads to state of the art results.']","['recent breakthrough computer vision make use large deep neural network  utilizing substantial speedup offered gpus ', 'application running limited hardware  however  high precision realtime processing still challenge ', 'one approach solving problem training network binary ternary weight  thus removing need calculate multiplication significantly reducing memory size ', 'work  introduce lrnets  local reparameterization network   new method training neural network discrete weight using stochastic parameter ', 'show simple modification local reparameterization trick  previously used train gaussian distributed weight  enables training discrete weight ', 'using proposed training test binary ternary model mnist  cifar10 imagenet benchmark reach stateoftheart result experiment ']","Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs., For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  , One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size., In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters., We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights., Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.",15,6.038167938931298,8.733333333333333
136,"['We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.', 'OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.', 'Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  ', 'Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.', 'OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively.']","[1, 0, 0, 0, 0]","[0.7142857313156128, 0.045454539358615875, 0.11764705181121826, 0.1702127605676651, 0.1666666567325592]",rkMW1hRqKX,"['Optimal Completion Distillation (OCD) is a training procedure for optimizing sequence to sequence models based on edit distance which achieves state-of-the-art on end-to-end Speech Recognition tasks.', 'Alternative approach to training seq2seq models using a dynamic program to compute optimal continuations of predicted prefixes', 'A training algorithm for auto-regressive models that does not require any MLE pre-training and can directly optimize from the sampling.', 'The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation and propose an approach based on edit distances and the implicit use of given label sequences during training']","['present optimal completion distillation  ocd   training procedure optimizing sequence sequence model based edit distance ', 'ocd efficient  hyperparameters  require pretraining joint optimization conditional loglikelihood ', 'given partial sequence generated model  first identify set optimal suffix minimize total edit distance  using efficient dynamic programming algorithm ', ' position generated sequence  use target distribution put equal probability first token optimal suffix ', 'ocd achieves stateoftheart performance endtoend speech recognition  wall street journal librispeech datasets  achieving  93   wer  45   wer  respectively ']","We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance., OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood., Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  , Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes., OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\%$ WER and $4.5\%$ WER, respectively.",15,5.829059829059829,7.8
137,"['As an emerging field, federated learning has recently attracted considerable attention.\n', 'Compared to distributed learning in the datacenter setting, federated learning\n', 'has more strict constraints on computate efficiency of the learned model and communication\n', 'cost during the training process.', 'In this work, we propose an efficient\n', 'federated learning framework based on variational dropout.', 'Our approach is able\n', 'to jointly learn a sparse model while reducing the amount of gradients exchanged\n', 'during the iterative training process.', 'We demonstrate the superior performance\n', 'of our approach on achieving significant model compression and communication\n', 'reduction ratios with no accuracy loss.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.1818181723356247, 0.21052631735801697, 0.17391303181648254, 0.0, 0.0, 0.23529411852359772, 0.0, 0.17391303181648254, 0.0, 0.0, 0.19999998807907104, 0.0]",BkeAf2CqY7,"['a joint model and gradient sparsification method for federated learning', 'Applies variational dropout to reduce the communication cost of distributed training of neural networks, and does experiments on mnist, cifar10 and svhn datasets. ', 'The authors propose an algorithm that reduces communication costs in federated learning by sending sparse gradients from device to server and back.', 'Combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.']","['emerging field  federated learning recently attracted considerable attention ', 'compared distributed learning datacenter setting  federated learning', 'strict constraint computate efficiency learned model communication', 'cost training process ', 'work  propose efficient', 'federated learning framework based variational dropout ', 'approach able', 'jointly learn sparse model reducing amount gradient exchanged', 'iterative training process ', 'demonstrate superior performance', 'approach achieving significant model compression communication', 'reduction ratio accuracy loss ']","As an emerging field, federated learning has recently attracted considerable attention.
, Compared to distributed learning in the datacenter setting, federated learning
, has more strict constraints on computate efficiency of the learned model and communication
, cost during the training process., In this work, we propose an efficient
, federated learning framework based on variational dropout., Our approach is able
, to jointly learn a sparse model while reducing the amount of gradients exchanged
, during the iterative training process., We demonstrate the superior performance
, of our approach on achieving significant model compression and communication
, reduction ratios with no accuracy loss.",15,6.125,6.4
138,"['We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  ', 'Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  ', 'Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"".', 'We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  ', 'In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  ', 'A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.']","[1, 0, 0, 0, 0, 0]","[0.9743589758872986, 0.06451612710952759, 0.0624999962747097, 0.1395348757505417, 0.4324324131011963, 0.17391303181648254]",SksY3deAW,"['We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.', 'Presents a boosting-style algorithm for training deep residual networks, a convergence analysis for training error, and a analysis of generalization ability.', 'A learning method for ResNet using the boosting framework that decomposes the learning of complex networks and uses less computational costs.', 'Authors propose the deep ResNet as a boosting algorithm, and they claim this is more efficient than standard end-to-end backropagation.']","['prove multiclass boosting theory resnet architecture simultaneously creates new technique multiclass boosting provides new algorithm resnetstyle architecture ', 'proposed training algorithm  boostresnet  particularly suitable nondifferentiable architecture ', 'method requires relatively inexpensive sequential training  shallow resnets  ', 'prove training error decay exponentially depth weak module classifier train perform slightly better weak baseline ', 'word  propose weak learning condition prove boosting theory resnet weak learning condition ', 'generalization error bound based margin theory proved suggests resnet could resistant overfitting using network l1 norm bounded weight ']","We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  , Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  , Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets""., We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  , In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  , A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.",9,5.809523809523809,14.0
139,"['We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector.', 'We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\n', 'Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1.', 'All local minima of $G$ are also global minima.\n', '2.', 'All global minima of $G$ correspond to the ground truth parameters.\n', '3.', 'The value and gradient of $G$ can be estimated using samples.\n\t\n', 'With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters.', 'We also prove finite sample complexity results and validate the results by simulations.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.21052631735801697, 0.23529411852359772, 0.1860465109348297, 0.1875, 0.17142856121063232, 0.22857142984867096, 0.19512194395065308, 0.11428570747375488]",BkwHObbRZ,"['The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ', 'This paper studies the problem of learning one-hidden layer neural networks, establishes a connection between least squares population loss and Hermite polynomials, and proposes a new loss function.', 'A tensor factorization-type method for leaning one hidden-layer neural netowrk']","['consider problem learning onehiddenlayer neural network  assume input x gaussian distribution label   sigma  bx   xi   nonnegative vector  b  fullrank weight matrix   xi  noise vector ', 'first give analytic formula population risk standard squared loss demonstrate implicitly attempt decompose sequence lowrank tensor simultaneously ', 'inspired formula  design nonconvex objective function  g  whose landscape guaranteed following property  1 ', 'local minimum  g  also global minimum ', '2 ', 'global minimum  g  correspond ground truth parameter ', '3 ', 'value gradient  g  estimated using sample ', 'property  stochastic gradient descent  g  provably converges global minimum learn groundtruth parameter ', 'also prove finite sample complexity result validate result simulation ']","We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \sigma(Bx) + \xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\xi$ is a noise vector., We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. 
	
, Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:	

1., All local minima of $G$ are also global minima.
, 2., All global minima of $G$ correspond to the ground truth parameters.
, 3., The value and gradient of $G$ can be estimated using samples.
	
, With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters., We also prove finite sample complexity results and validate the results by simulations.",14,5.154320987654321,11.571428571428571
140,"['Open information extraction (OIE) systems extract relations and their\n  arguments from natural language text in an unsupervised manner.', 'The resulting\n  extractions are a valuable resource for downstream tasks such as knowledge\n  base construction, open question answering, or event schema induction.', 'In this\n  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was\n  extracted from the text of English Wikipedia.', 'OPIEC complements the available\n  OIE resources: It is the largest OIE corpus publicly available to date (over\n  340M triples) and contains valuable metadata such as provenance information,\n  confidence scores, linguistic annotations, and semantic annotations including\n  spatial and temporal information.', 'We analyze the OPIEC corpus by comparing its\n  content with knowledge bases such as DBpedia or YAGO, which are also based on\n  Wikipedia.', 'We found that most of the facts between entities present in OPIEC\n  cannot be found in DBpedia and/or YAGO, that OIE facts \n  often differ in the level of specificity compared to knowledge base facts, and\n  that OIE open relations are generally highly polysemous.', 'We believe that the\n  OPIEC corpus is a valuable resource for future research on automated knowledge\n  base construction.']","[1, 0, 0, 0, 0, 0, 0]","[0.14814814925193787, 0.0, 0.06451612710952759, 0.04651162400841713, 0.0624999962747097, 0.04651162400841713, 0.0]",HJxeGb5pTm,"['An Open Information Extraction Corpus and its in-depth analysis', 'Builds a new corpus for information extraction which is larger than the prior public corpora and contains information not existing in current corpora.', 'Presents a dataset of open-IE triples that were collected from Wikipedia with the help of a recent extraction system. ', 'The paper describes the creation of an Open IE corpus over English Wikipedia through an automatic manner']","['open information extraction  oie  system extract relation argument natural language text unsupervised manner ', 'resulting extraction valuable resource downstream task knowledge base construction  open question answering  event schema induction ', 'paper  release  describe  analyze oie corpus called opiec  extracted text english wikipedia ', 'opiec complement available oie resource  largest oie corpus publicly available date  340m triple  contains valuable metadata provenance information  confidence score  linguistic annotation  semantic annotation including spatial temporal information ', 'analyze opiec corpus comparing content knowledge base dbpedia yago  also based wikipedia ', 'found fact entity present opiec found dbpedia andor yago  oie fact often differ level specificity compared knowledge base fact  oie open relation generally highly polysemous ', 'believe opiec corpus valuable resource future research automated knowledge base construction ']","Open information extraction (OIE) systems extract relations and their
  arguments from natural language text in an unsupervised manner., The resulting
  extractions are a valuable resource for downstream tasks such as knowledge
  base construction, open question answering, or event schema induction., In this
  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was
  extracted from the text of English Wikipedia., OPIEC complements the available
  OIE resources: It is the largest OIE corpus publicly available to date (over
  340M triples) and contains valuable metadata such as provenance information,
  confidence scores, linguistic annotations, and semantic annotations including
  spatial and temporal information., We analyze the OPIEC corpus by comparing its
  content with knowledge bases such as DBpedia or YAGO, which are also based on
  Wikipedia., We found that most of the facts between entities present in OPIEC
  cannot be found in DBpedia and/or YAGO, that OIE facts 
  often differ in the level of specificity compared to knowledge base facts, and
  that OIE open relations are generally highly polysemous., We believe that the
  OPIEC corpus is a valuable resource for future research on automated knowledge
  base construction.",18,5.610810810810811,10.277777777777779
141,"['The process of designing neural architectures requires expert knowledge and extensive trial and error.\n', 'While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\n', 'We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\n', 'The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  ', 'Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\n', 'The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.']","[0, 0, 0, 1, 0, 0]","[0.16326530277729034, 0.16393442451953888, 0.27586206793785095, 0.3333333134651184, 0.3030303120613098, 0.19354838132858276]",SkOb1Fl0Z,"['We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.', 'Introduces a new method to generate RNNs architectures using a domain-specific language for two types of generators (random and RL-based) together with a ranking function and evaluator.', 'This paper casts the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and scored based on learnt functions or generated by a RL agent.', 'This paper investigates meta-learning strategy for automated architecture search in the context of RNN by using a DSL that specifies RNN recurrent operations.']","['process designing neural architecture requires expert knowledge extensive trial error ', 'automated architecture search may simplify requirement  recurrent neural network  rnn  architecture generated existing method limited flexibility component ', 'propose domainspecific language  dsl  use automated architecture search produce novel rnns arbitrary depth width ', 'dsl flexible enough define standard architecture gated recurrent unit long short term memory allows introduction nonstandard rnn component trigonometric curve layer normalization ', 'using two different candidate generation technique  random search ranking function reinforcement learning  explore novel architecture produced rnn dsl language modeling machine translation domain ', 'resulting architecture follow human intuition yet perform well targeted task  suggesting space usable rnn architecture far larger previously assumed ']","The process of designing neural architectures requires expert knowledge and extensive trial and error.
, While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.
, We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.
, The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  , Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, 
we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.
, The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.",10,6.006410256410256,15.6
142,"['Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics.', 'Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously.', 'In this work, we develop a new method termed as ``""WAGE"" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers.', 'To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation.', 'Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization.', 'Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.']","[0, 1, 0, 0, 0, 0]","[0.19999998807907104, 0.3529411852359772, 0.17777776718139648, 0.052631575614213943, 0.0, 0.1666666567325592]",HJGXzmspb,"['We apply training and inference with only low-bitwidth integers in DNNs', 'A method called WAGE which quantizes all operands and operators in a neural network to reduce the number of bits for representation in a network.', 'The authors propose discretized weights, activations, gradients, and errors at both training and testing time on neural networks']","['research deep neural network discrete parameter deployment embedded system active promising topic ', 'although previous work successfully reduced precision inference  transferring training inference process lowbitwidth integer demonstrated simultaneously ', 'work  develop new method termed   wage  discretize training inference  weight  w   activation    gradient  g  error  e  among layer shifted linearly constrained lowbitwidth integer ', 'perform pure discrete dataflow fixedpoint device  replace batch normalization constant scaling layer simplify component arduous integer implementation ', 'improved accuracy obtained multiple datasets  indicates wage somehow act type regularization ', 'empirically  demonstrate potential deploy training hardware system integerbased deep learning accelerator neuromorphic chip comparable accuracy higher energy efficiency  crucial future ai application variable scenario transfer continual learning demand ']","Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics., Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously., In this work, we develop a new method termed as ``""WAGE"" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers., To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation., Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization., Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.",15,6.117647058823529,11.333333333333334
143,"['Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters.', 'Their deployment exerts computational, storage and energy demands, particularly on embedded platforms.', 'Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy.', 'Such retraining is not feasible in some contexts.', 'In this paper, we explore the sparsification of CNNs by proposing three model-independent methods.', 'Our methods are applied on-the-fly and require no retraining.', ""We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy."", 'Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.0624999962747097, 0.0, 0.060606054961681366, 0.13793103396892548, 0.4571428596973419, 0.13333332538604736, 0.20408162474632263, 0.2702702581882477]",rkz1YD0vjm,"['In this paper, we develop fast retraining-free  sparsification methods that can be deployed for on-the-fly sparsification of CNNs in many industrial contexts.', 'This paper proposes approaches for pruning CNNs without retraining by introducing three schemes to determine the thresholds of pruning weights.', 'This paper describes a method for sparsification of CNNs without retraining.']","['modern convolutional neural network  cnns  complex  encompassing million parameter ', 'deployment exerts computational  storage energy demand  particularly embedded platform ', 'existing approach prune sparsify cnns require retraining maintain inference accuracy ', 'retraining feasible context ', 'paper  explore sparsification cnns proposing three modelindependent method ', 'method applied onthefly require retraining ', 'show stateoftheart model  weight reduced 73   compression factor 37x  without incurring 5  loss top5 accuracy ', 'additional finetuning gain 8  sparsity  indicates fast onthefly method effective ']","Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters., Their deployment exerts computational, storage and energy demands, particularly on embedded platforms., Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy., Such retraining is not feasible in some contexts., In this paper, we explore the sparsification of CNNs by proposing three model-independent methods., Our methods are applied on-the-fly and require no retraining., We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy., Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.",13,6.054545454545455,8.461538461538462
144,"['Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels.', 'Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance.', 'In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.', 'We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions.', 'Results of the statistical tests show that the proposed method is better than classical method and similar with the others.', 'These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.']","[0, 1, 0, 0, 0, 0]","[0.10810810327529907, 0.15789473056793213, 0.05882352590560913, 0.12903225421905518, 0.12903225421905518, 0.10256409645080566]",SJ1fQYlCZ,"['We propose that training with growing sets stage-by-stage provides an optimization for neural networks.', 'The authors compare curriculum learning to learning in a random order with stages that add a new sample of examples to the previously, randomly constructed set', 'This paper studies the influence of ordering in the Curriculum and Self paced learning, and shows that to some extent the ordering of training instances is not important.']","['curriculum learning self paced learning popular topic machine learning suggest put training sample order considering difficulty level ', 'study topic show starting small training set adding new sample according difficulty level improves learning performance ', 'paper experimented also obtain good result adding sample randomly without meaningful order ', 'compared method classical training  curriculum learning  self paced learning reverse ordered version ', 'result statistical test show proposed method better classical method similar others ', 'result point new training regime remove process difficulty level determination curriculum self paced learning successful method ']","Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels., Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance., In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order., We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions., Results of the statistical tests show that the proposed method is better than classical method and similar with the others., These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.",8,5.492647058823529,17.0
145,"['We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\\vb \\in B$ contain all the information that exists in samples $\\va\\in A$ and some additional information.', 'For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information.', 'When mapping a sample $\\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\\vb\\in B$.', 'Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. \n\n', 'Our solution employs a single two-pathway encoder and a single decoder for both domains.', 'The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$.', 'The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term.', 'Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains.', 'We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.12765957415103912, 0.060606054961681366, 0.1621621549129486, 0.1621621549129486, 0.0714285671710968, 0.1111111044883728, 0.12121211737394333, 0.1463414579629898, 0.10526315122842789]",BylE1205Fm,"['An image to image translation method which adds to one image the content of another thereby creating a new image.', 'This paper tackles the task of content transfer, with the novalty being on the loss.']","['study problem learning map  unsupervised way  domain    b   sample  vb b  contain information exists sample  vain  additional information ', 'example  ignoring occlusion   b  people glass    people without  glass  would added information ', 'mapping sample  va  first domain domain  missing information replicated independent reference sample  vbin b  ', 'thus  example  create  every person without glass version glass observed face image ', 'solution employ single twopathway encoder single decoder domain ', 'common part two domain separate part encoded two vector  separate part fixed zero domain   ', 'loss term minimal involve reconstruction loss two domain domain confusion term ', 'analysis show mild assumption  architecture  much simpler literature guidedtranslation method  enough ensure disentanglement two domain ', 'present convincing result visual domain  noglasses glass  adding facial hair based reference image  etc ']","We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\vb \in B$ contain all the information that exists in samples $\va\in A$ and some additional information., For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information., When mapping a sample $\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\vb\in B$., Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. 

, Our solution employs a single two-pathway encoder and a single decoder for both domains., The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$., The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term., Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains., We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.",28,5.032407407407407,7.714285714285714
146,"['Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules.', 'In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format.', 'The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes.', 'Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.\n']","[0, 0, 0, 1]","[0.1090909019112587, 0.12244897335767746, 0.039215683937072754, 0.14814814925193787]",H1gR5iR5FX,"['A dataset for testing mathematical reasoning (and algebraic generalization), and results on current sequence-to-sequence models.', 'Presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models, and uses it to evaluate several models.', 'Model for solving basic math problems.']","['mathematical reasoning  core ability within human intelligence  present unique challenge domain  come understand solve mathematical problem primarily back experience evidence  basis inferring  learning  exploiting law  axiom  symbol manipulation rule ', 'paper  present new challenge evaluation  eventually design  neural architecture similar system  developing task suite mathematics problem involving sequential question answer freeform textual inputoutput format ', 'structured nature mathematics domain  covering arithmetic  algebra  probability calculus  enables construction training test spit designed clearly illuminate capability failuremodes different architecture  well evaluate ability compose relate knowledge learned process ', 'described data generation process potential future expansion  conduct comprehensive analysis model two broad class powerful sequencetosequence architecture find notable difference ability resolve mathematical problem generalize knowledge ']","Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules., In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format., The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes., Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.
",17,6.114942528735632,10.235294117647058
147,"['Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.\n', 'Commonly, the convolution operators couple features from all channels.\n', 'For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.\n', 'In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.\n', 'We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.\n', 'Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.', 'Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.', 'Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.', 'We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.\n']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.10256409645080566, 0.0, 0.11764705181121826, 0.09756097197532654, 0.09999999403953552, 0.1395348757505417, 0.11428570747375488, 0.09999999403953552, 0.045454539358615875]",H1eRIoA5Y7,"['This paper introduces efficient and economic parametrizations of convolutional neural networks motivated by partial differential equations ', 'Introduces four ""low cost"" alternatives to the standard convolution operation that can be used in place of the standard convolution operation to reduce their computational complexity.', 'This paper introduces methods for reducing the computational cost of CNN implementations, and introduces new parameterizations of CNN like architectures that limit parameter coupling.', 'The paper proposes a PDE-based perspective to understand and parameterize CNNs']","['convolutional neural network  cnns  filter input data using series spatial convolution operator compactly supported stencil pointwise nonlinearities ', 'commonly  convolution operator couple feature channel ', 'wide network  lead immense computational cost training prediction cnns ', 'paper  present novel way parameterize convolution efficiently  aiming decrease number parameter cnns computational complexity ', 'propose new architecture use sparser coupling channel thereby reduce number trainable weight computational cost cnn ', 'architecture arise new type residual neural network  resnet  seen discretizations partial differential equation  pdes  thus predictable theoretical property ', 'first architecture involves convolution operator special sparsity structure  applicable large class cnns ', 'next  present architecture seen discretization diffusion reaction pde  use three different convolution operator ', 'outline experiment proposed architecture  although considerably reducing number trainable weight  yield comparable accuracy existing cnns fully coupled channel dimension ']","Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.
, Commonly, the convolution operators couple features from all channels.
, For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.
, In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.
, We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.
, Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties., Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs., Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators., We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.
",18,5.711538461538462,11.555555555555555
148,"['In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?', 'One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.', 'The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.', 'Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.', 'We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.']","[0, 0, 0, 0, 1]","[0.25531914830207825, 0.2800000011920929, 0.2142857164144516, 0.19607843458652496, 0.31578946113586426]",rkemqsC9Fm,"['Use rate-distortion theory to bound how much a latent variable model can be improved', 'Addresses problems of optimization of the prior in the latent variable model and the selection of the likelihood function by proposing criteria based on a lower-bound on the negative log-likelihood.', 'Presents a theorem which gives a lower bound on negative log likelihood of rate-distortion for latent-variable modeling', 'The authors argue that the rate-distortion theory for lossy compression provides a natural toolkit for studying latent variable models proposes a lower bound.']","['article use ratedistortion theory  branch information theory devoted problem lossy compression  shed light important problem latent variable modeling data  room improve model ', 'one way address question find upper bound probability  equivalently lower bound negative log likelihood  model assign data one varies prior andor likelihood function latent variable model ', 'core contribution formally show problem optimizing prior latent variable model exactly instance variational optimization problem information theorist solve computing ratedistortion function  use derive lower bound negative log likelihood ', 'moreover  show changing prior improve log likelihood  way change likelihood function instead attain log likelihood  thus ratedistortion theory relevance optimizing prior well optimizing likelihood function ', 'experimentally argue usefulness quantity derived ratedistortion theory latent variable modeling applying problem image modeling ']","In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?, One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model., The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood., Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions., We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.",11,5.148325358851674,19.0
149,"['Backprop is the primary learning algorithm used in many machine learning algorithms.', 'In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints.', 'One set of constraints is to avoid weights that lead to saturated units.', 'The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt.', 'Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow.', 'Here we investigate backpropagating error terms only linearly.', 'That is, we ignore the saturation that arise by ensuring gradients always flow.', 'We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear.', 'In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed.', 'Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.13333332538604736, 0.09999999403953552, 0.06451612710952759, 0.1621621549129486, 0.2978723347187042, 0.0, 0.3125, 0.3243243098258972, 0.1538461446762085, 0.05714285373687744]",ByfPDyrYim,"['We ignore non-linearities and do not compute gradients in the backward pass to save computation and to ensure gradients always flow. ', 'The author proposed linear backprop algorithms to ensure gradients flow for all parts during backpropagation.']","['backprop primary learning algorithm used many machine learning algorithm ', 'practice  however  backprop deep neural network highly sensitive learning algorithm successful learning depends numerous condition constraint ', 'one set constraint avoid weight lead saturated unit ', 'motivation avoiding unit saturation gradient vanish result learning come halt ', 'careful weight initialization rescaling scheme batch normalization ensure input activity neuron within linear regime gradient vanished flow ', 'investigate backpropagating error term linearly ', ' ignore saturation arise ensuring gradient always flow ', 'refer learning rule linear backprop since backward pas network appears linear ', 'addition ensuring persistent gradient flow  linear backprop also favorable computation expensive since gradient never computed ', 'early result suggest learning linear backprop competitive backprop save expensive gradient computation ']","Backprop is the primary learning algorithm used in many machine learning algorithms., In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints., One set of constraints is to avoid weights that lead to saturated units., The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt., Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow., Here we investigate backpropagating error terms only linearly., That is, we ignore the saturation that arise by ensuring gradients always flow., We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear., In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed., Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.",14,5.593220338983051,12.642857142857142
150,"['Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks.', 'There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts.', 'Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.', 'In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.', 'Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.\n']","[0, 0, 0, 0, 1]","[0.17777776718139648, 0.19230768084526062, 0.0833333283662796, 0.0952380895614624, 0.4333333373069763]",HkGGfhC5Y7,"['Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline.', 'This paper introduces a new way of interpreting the VQ-VAE and proposes a new training algorithm based on the soft EM clustering.', 'The paper presents an alternative view on the training procedure for the VQ-VAE using the soft EM algorithm']","['deep neural network discrete latent variable offer promise better symbolic reasoning  learning abstraction useful new task ', 'surge interest discrete latent variable model  however  despite several recent improvement  training discrete latent variable model remained challenging performance mostly failed match continuous counterpart ', 'recent work vector quantized autoencoders  vqvae  made substantial progress direction  perplexity almost matching vae datasets cifar10 ', 'work  investigate alternate training technique vqvae  inspired connection expectation maximization  em  algorithm ', 'training discrete autoencoder em combining sequence level knowledge distillation alows u develop nonautoregressive machine translation model whose accuracy almost match strong greedy autoregressive baseline transformer  33 time faster inference ']","Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks., There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts., Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10., In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm., Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.
",13,5.947019867549669,11.615384615384615
151,"['Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution.', 'In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies.', 'In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution.', 'We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks.', 'In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations.', 'And our ODN\n', 'model also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.']","[0, 0, 1, 0, 0, 0, 0]","[0.15094339847564697, 0.2857142686843872, 0.4680851101875305, 0.21276594698429108, 0.09302324801683426, 0.0, 0.19512194395065308]",HygcvsAcFX,"['This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.', 'Presents a PAC-Bayesian bound for a margin loss']","['recent research margin theory proved maximizing minimum margin like support vector machine necessarily lead better performance  instead  crucial optimize margin distribution ', 'meantime  margin theory used explain empirical success deep network recent study ', 'paper  present odn  optimal margin distribution network   network embeds loss function regard optimal margin distribution ', 'give theoretical analysis method using pacbayesian framework  confirms significance margin distribution classification within framework deep network ', 'addition  empirical result show odn model always outperforms baseline crossentropy loss model consistently across different regularization situation ', 'odn', 'model also outperforms crossentropy loss  xent   hinge loss soft hinge loss model generalization task limited training data ']","Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution., In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies., In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution., We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks., In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations., And our ODN
, model also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.",15,5.733333333333333,10.0
152,"['Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  ', 'Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  ', 'While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  ', 'In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\n', 'We propose a suite of triage methods and compare them on problem spaces of varying complexity.  ', 'We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  ', 'Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.']","[0, 0, 0, 0, 0, 1, 0]","[0.1111111044883728, 0.060606054961681366, 0.10526315122842789, 0.23880596458911896, 0.11764705181121826, 0.25, 0.1538461446762085]",HJWpQCa7z,"['We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage', 'Compares various initialization and training methods of transferring knowledge from VGG network to a smaller student network by replacing blocks of layers with single layers.', 'This paper presents five methods for doing triaging or block layer compression for deep networks.', 'The paper proposes a method to compress a block of layers in a NN that evaluates several different sub-approaches']","['deep network compression seek reduce number parameter network maintaining certain level performance ', 'deep network distillation seek train smaller network match softmax performance larger network ', 'regime led impressive performance respective goal  neither provide insight importance given layer original model  useful improve understanding highly parameterized model ', 'paper  present concept deep net triage  individually ass small block convolution layer understand collective contribution overall performance  call emph  criticality   call triage ass criticality answering question  impact health overall network compress block layer single layer ', 'propose suite triage method compare problem space varying complexity ', 'ultimately show  across problem space  deep net triage able indicate relative importance different layer ', 'surprisingly  local structural compression technique also lead improvement overall accuracy final model finetuned globally ']","Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  , Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  , While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  , In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.
, We propose a suite of triage methods and compare them on problem spaces of varying complexity.  , We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  , Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.",15,5.357843137254902,12.75
153,"[""In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   "", 'The existence of super-convergence is relevant to understanding why deep networks generalize well.  ', 'One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  ', 'Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.', 'In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  ', 'We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  ', 'The architectures to replicate this work will be made available upon publication.\n']","[0, 0, 0, 0, 1, 0, 0]","[0.11320754140615463, 0.2222222238779068, 0.25, 0.052631575614213943, 0.25531914830207825, 0.19512194395065308, 0.05714285373687744]",H1A5ztj3b,"['Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.', 'The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates', 'The authors analyze training of residual networks using large cyclic learning rates, and demonstrate fast convergence with cyclic learning rates and evidence of large learning rates acting as regularization.']","['paper  show phenomenon  named  superconvergence   residual network trained using order magnitude fewer iteration used standard training method ', 'existence superconvergence relevant understanding deep network generalize well ', 'one key element superconvergence training cyclical learning rate large maximum learning rate ', 'furthermore  present evidence training large learning rate improves performance regularizing network ', 'addition  show superconvergence provides greater boost performance relative standard training amount labeled training data limited ', 'also derive simplification hessian free optimization method compute estimate optimal learning rate ', 'architecture replicate work made available upon publication ']","In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   , The existence of super-convergence is relevant to understanding why deep networks generalize well.  , One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  , Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network., In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  , We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  , The architectures to replicate this work will be made available upon publication.
",12,5.735294117647059,11.333333333333334
154,"['Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks.', 'Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers.', 'To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.', 'Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks.', 'The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way.', 'Furthermore, we examine the practical implications of this construction for standard, finite-width networks.', 'In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand.', 'We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.1702127605676651, 0.0952380895614624, 0.25925925374031067, 0.4838709533214569, 0.1875, 0.3414634168148041, 0.4727272689342499, 0.35555556416511536]",SkGT6sRcFX,"['We propose a method for the construction of arbitrarily deep infinite-width networks, based on which we derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance.', 'Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets.', 'Proposes deep neural networks of infinite width.']","['infinitewidth neural network extensively used study theoretical property underlying extraordinary empirical success standard  finitewidth neural network ', 'nevertheless   infinitewidth network limited two hidden layer ', 'address shortcoming  study initialisation requirement network show main challenge constructing defining appropriate sampling distribution weight ', 'based observation  propose principled approach weight initialisation correctly account functional nature hidden layer activation facilitates construction arbitrarily many infinitewidth layer  thus enabling construction arbitrarily deep infinitewidth network ', 'main idea approach iteratively reparametrise hiddenlayer activation appropriately defined reproducing kernel hilbert space use canonical way constructing probability distribution space specifying required weight distribution principled way ', 'furthermore  examine practical implication construction standard  finitewidth network ', 'particular  derive novel weight initialisation scheme standard  finitewidth network take account structure data information task hand ', 'demonstrate effectiveness weight initialisation approach mnist  cifar10 year prediction msd datasets ']","Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks., Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers., To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights., Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks., The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way., Furthermore, we examine the practical implications of this construction for standard, finite-width networks., In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand., We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.",19,6.314285714285714,11.052631578947368
155,"['Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.', 'This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds.', 'Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations.', ""Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities."", 'The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network.', 'To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.', 'Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.']","[0, 0, 0, 0, 0, 1, 0]","[0.0555555522441864, 0.05882352590560913, 0.1538461446762085, 0.13793103396892548, 0.20512819290161133, 0.2978723347187042, 0.16326530277729034]",Syl3_2JCZ,"['We derived biologically plausible synaptic plasticity learning rules for a recurrent neural network to store stimulus representations. ', 'A neural network model consisting of recurrently connected neurons and one or more redouts which aims to retain some output over time.', 'This paper presents a self-organizing memory mechanism in a neural model, and introduces an objective function that minimizes changes in the signal to be memorized.']","['working memory requires information external stimulus represented brain even stimulus go away ', 'information encoded activity neuron  neural activity change timescales ten millisecond ', 'information working memory  however  retained ten second  suggesting question timevarying neural activity maintain stable representation ', 'prior work show  neural dynamic  null space  representation  change neural activity affect downstream readout stimulus information  information retained period much longer timescale individualneuronal activity ', 'prior work  however  requires precisely constructed synaptic connectivity matrix  without explaining would arise biological neural network ', 'identify mechanism biological network selforganize learn memory function  derived biologically plausible synaptic plasticity rule dynamically modify connectivity matrix enable information storing ', 'network implementing plasticity rule successfully learn form memory representation even 10  synapsis plastic  robust synaptic noise  represent information multiple stimulus ']","Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away., This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds., Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations., Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities., The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network., To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing., Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.",18,5.933333333333334,10.833333333333334
156,"['Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models.', 'While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice.', 'In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions immediate neighborhood.\n\n', 'To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself.', 'To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.\n\n', 'This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse.', 'This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice.', 'Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.04651162400841713, 0.11999999731779099, 0.307692289352417, 0.1666666567325592, 0.19230768084526062, 0.19999998807907104, 0.11428570747375488]",HJYQLb-RW,"['To understand GAN training, we define simple GAN dynamics, and show quantitative differences between optimal and first order updates in this model.', 'The authors study the impact of GANs in settings where at each iteration, the discriminator trains to convergence and the generator updates with gradient steps, or where a few gradient steps are done for the disciminator and generator.', 'This paper studies the dynamics of adversarial training of GANs on a Gaussian mixture model']","['generative adversarial network  gans  proposed approach learning generative model ', 'gans demonstrated promising performance multiple vision task  learning dynamic yet well understood  neither theory practice ', 'particular  work domain focused far understanding property stationary solution dynamic might converge  behavior dynamic solution  immediate neighborhood ', 'address issue  work take first step towards principled study gan dynamic ', 'end  propose model  one hand  exhibit several common problematic convergence behavior  eg  vanishing gradient  mode collapse  diverging oscillatory behavior   hand  sufficiently simple enable rigorous convergence analysis ', 'methodology enables u exhibit interesting phenomenon  gan optimal discriminator provably converges  guiding gan training using first order approximation discriminator lead unstable gan dynamic mode collapse ', 'suggests usage first order approximation discriminator  defacto standard existing gan dynamic  might one factor make gan training challenging practice ', 'additionally  convergence result constitutes first rigorous analysis dynamic concrete parametric gan ']","Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models., While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice., In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions immediate neighborhood.

, To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself., To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.

, This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse., This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice., Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.",25,5.446351931330472,9.32
157,"['The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network.', 'However, it is expensive and time-consuming to manually label sufficient amount of training data.', 'Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task.', 'While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting.', 'We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule.', 'GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training.', 'Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset.', 'In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain.', 'We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.16393442451953888, 0.13333332538604736, 0.1111111044883728, 0.3589743673801422, 0.3870967626571655, 0.24242423474788666, 0.17391303181648254, 0.22727271914482117, 0.21052631735801697]",H1xL_iR9Km,"['We propose a gradient-based method to transfer knowledge from multiple sources across different domains and tasks.', 'This paper proposes to combine the gradients of source domains to help the learning in the target domain. ']","['machine learning computer vision community witnessing unprecedented rate new task proposed addressed  thanks power deep convolutional network find complex mapping x  advent task often accompanies release largescale humanlabeled dataset  supervised training deep network ', 'however  expensive timeconsuming manually label sufficient amount training data ', 'therefore  important develop algorithm leverage offtheshelf labeled dataset learn useful knowledge target task ', 'previous work mostly focus transfer learning single source  study multisource transfer across domain task  msdtt   semisupervised setting ', 'propose gradmix  modelagnostic method applicable model trained gradientbased learning rule ', 'gradmix transfer knowledge via gradient descent  weighting mixing gradient source training ', 'method follows metalearning objective  assigning layerwise weight source gradient  combined gradient follows direction minimize loss small set sample target dataset ', 'addition  propose adaptively adjust learning rate minibatch based importance target task  pseudolabeling method leverage unlabeled sample target domain ', 'perform experiment two msdtt task  digit recognition action recognition  demonstrate advantageous performance proposed method multiple baseline ']","The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network., However, it is expensive and time-consuming to manually label sufficient amount of training data., Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task., While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting., We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule., GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training., Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset., In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain., We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.",22,5.679012345679013,10.565217391304348
158,"['Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates.', 'In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis.', 'We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions.', 'We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models.', 'We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference.', 'Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling.', 'Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.']","[0, 0, 0, 0, 0, 0, 1]","[0.1538461446762085, 0.17142856121063232, 0.09090908616781235, 0.17391303181648254, 0.11999999731779099, 0.0476190410554409, 0.21052631735801697]",SJVmjjR9FX,"['The first variational Bayes formulation of phylogenetic inference, a challenging inference problem over structures with intertwined discrete and continuous components', 'Explores an approximate inference solution to the problem of Bayesian inference of phylogenetic trees by leveraging recently proposed subsplit Bayesian networks and modern gradient estimators for VI.', 'Proposes a variational approach to Bayesian posterior inference in phylogenetic trees.']","['bayesian phylogenetic inference currently done via markov chain monte carlo simple mechanism proposing new state  hinders exploration efficiency often requires long run deliver accurate posterior estimate ', 'paper present alternative approach  variational framework bayesian phylogenetic analysis ', 'approximate true posterior using expressive graphical model tree distribution  called subsplit bayesian network  together appropriate branch length distribution ', 'train variational approximation via stochastic gradient ascent adopt multisample based gradient estimator different latent variable separately handle composite latent space phylogenetic model ', 'show structured variational approximation flexible enough provide comparable posterior estimation mcmc  requiring le computation due efficient tree exploration mechanism enabled variational inference ', 'moreover  variational approximation readily used statistical analysis marginal likelihood estimation model comparison via importance sampling ', 'experiment synthetic data real data bayesian phylogenetic inference problem demonstrate effectiveness efficiency method ']","Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates., In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis., We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions., We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models., We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference., Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling., Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.",12,6.511363636363637,14.666666666666666
159,"['This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\n', 'models for raw audio waveform generation.', 'As an example, we propose\n', 'a hybrid model that combines an autoregressive network named WaveNet and a\n', 'conventional LSTM model to address speech synthesis.', 'Instead of generating\n', 'one sample per time-step, the proposed HybridNet generates multiple samples per\n', 'time-step by exploiting the long-term memory utilization property of LSTMs.', 'In\n', 'the evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\n', 'HybridNet achieves a 3.83 subjective 5-scale mean opinion score on\n', 'US English, largely outperforming the same size WaveNet in terms of naturalness\n', 'and provide 2x speed up at inference.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.5454545617103577, 0.0, 0.0, 0.3636363446712494, 0.2222222238779068, 0.0, 0.0, 0.0, 0.1818181723356247, 0.09090908616781235, 0.0, 0.0]",rJoXrxZAZ,"['It is a hybrid neural architecture to speed-up autoregressive model. ', 'Concludes that in order to scale up the model size without increasing inference time for sequential prediction, use a model that predicts multiple timesteps at once.', 'This paper presents HybridNet, a neural speech and other audio synthesis system that combines the WaveNet model with an LSTM with the goal of offering a model with faster inference-time audio generation.']","['paper introduces hybridnet  hybrid neural network speedup autoregressive', 'model raw audio waveform generation ', 'example  propose', 'hybrid model combine autoregressive network named wavenet', 'conventional lstm model address speech synthesis ', 'instead generating', 'one sample per timestep  proposed hybridnet generates multiple sample per', 'timestep exploiting longterm memory utilization property lstms ', '', 'evaluation  applied texttospeech  hybridnet yield stateofart performance ', 'hybridnet achieves 383 subjective 5scale mean opinion score', 'u english  largely outperforming size wavenet term naturalness', 'provide 2x speed inference ']","This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive
, models for raw audio waveform generation., As an example, we propose
, a hybrid model that combines an autoregressive network named WaveNet and a
, conventional LSTM model to address speech synthesis., Instead of generating
, one sample per time-step, the proposed HybridNet generates multiple samples per
, time-step by exploiting the long-term memory utilization property of LSTMs., In
, the evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.
, HybridNet achieves a 3.83 subjective 5-scale mean opinion score on
, US English, largely outperforming the same size WaveNet in terms of naturalness
, and provide 2x speed up at inference.",19,5.961904761904762,5.526315789473684
160,"['Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them.', 'In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations.', 'We interpret the model through average visualizations of this reduced set of features.', 'Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features.', 'In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations.', 'Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.', 'Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.']","[0, 1, 0, 0, 0, 0, 0]","[0.12765957415103912, 0.2461538463830948, 0.1904761791229248, 0.19607841968536377, 0.1702127605676651, 0.1702127605676651, 0.24561403691768646]",H1ziPjC5Fm,"['Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset.', 'This paper proposes a method for producing visual explanations for deep neural network outputs and releases a new synthetic dataset.', 'A method for Deep Neural Networks that identifies automatically relevant features of the set of the classes, supporting interpretation and explanation without relying on additional annotations.']","['visual interpretation explanation deep model critical towards wide adoption system rely ', 'paper  propose novel scheme interpretation well explanation  given pretrained model  automatically identify internal feature relevant set class considered model  without relying additional annotation ', 'interpret model average visualization reduced set feature ', ' test time  explain network prediction accompanying predicted class label supporting visualization derived identified feature ', 'addition  propose method address artifact introduced strided operation deconvnetbased visualization ', 'moreover  introduce an8flower  dataset specifically designed objective quantitative evaluation method visual explanation ', 'experiment mnist  ilsvrc 12  fashion 144k an8flower datasets show method produce detailed explanation good coverage relevant feature class interest ']","Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them., In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations., We interpret the model through average visualizations of this reduced set of features., Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features., In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations., Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation., Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.",18,5.860759493670886,8.777777777777779
161,"['In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data.', 'Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem.', 'Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations.', 'This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations.', 'We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.']","[1, 0, 0, 0, 0]","[0.4000000059604645, 0.12121211737394333, 0.13793103396892548, 0.27586206793785095, 0.14999999105930328]",rJvJXZb0W,"['A framework for learning high-quality sentence representations efficiently.', 'Proposes a faster algorithm for learning SkipThought-style sentence representations from corpora of ordered sentences that swaps the word-level decoder for a contrastive classification loss.', 'This paper proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences']","['work propose simple efficient framework learning sentence representation unlabelled data ', 'drawing inspiration distributional hypothesis recent work learning sentence representation  reformulate problem predicting context sentence appears classification problem ', 'given sentence context appears  classifier distinguishes context sentence contrastive sentence based vector representation ', 'allows u efficiently learn different type encoding function  show model learns highquality sentence representation ', 'demonstrate sentence representation outperform stateoftheart unsupervised supervised representation learning method several downstream nlp task involve understanding sentence semantics achieving order magnitude speedup training time ']","In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data., Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem., Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations., This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations., We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",8,6.388888888888889,15.75
162,"['Many regularization methods have been proposed to prevent overfitting in neural networks.', 'Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian.', 'However, this method cannot be generalized to regular neural network architectures.', 'We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.', 'Unlike in previous literature, it can be applied to any general neural network.', 'We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification.', 'We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.']","[0, 0, 0, 1, 0, 0, 0]","[0.07407406717538834, 0.1875, 0.1538461446762085, 0.4444444477558136, 0.1428571343421936, 0.21052631735801697, 0.34285715222358704]",SySpa-Z0Z,"['We derive a norm penalty on the output of the neural network from the information bottleneck perspective', 'Puts forward Activation Norm Penalty, an L_2 type regularization on the activations, deriving it from the Information Bottleneck principle', 'This paper creates a mapping between activation norm penalties and information bottleneck framework using variational dropout framework.']","['many regularization method proposed prevent overfitting neural network ', 'recently  regularization method proposed optimize variational lower bound information bottleneck lagrangian ', 'however  method generalized regular neural network architecture ', 'present activation norm penalty derived information bottleneck principle theoretically grounded variation dropout framework ', 'unlike previous literature  applied general neural network ', 'demonstrate penalty give consistent improvement different state art architecture language modeling image classification ', 'present analysis property penalty compare method also reduce mutual information ']","Many regularization methods have been proposed to prevent overfitting in neural networks., Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian., However, this method cannot be generalized to regular neural network architectures., We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework., Unlike in previous literature, it can be applied to any general neural network., We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification., We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.",10,5.925,12.0
163,"['Unsupervised learning of timeseries data is a challenging problem in machine learning.', 'Here, \nwe propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.', 'The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.', 'Then it jointly optimizes the clustering objective and the dimensionality reduction objective.', 'Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.', 'Several similarity metrics are considered and compared.  ', 'To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.', 'The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.', 'In each case, we show that our algorithm outperforms traditional methods.', 'This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.13793103396892548, 0.7727272510528564, 0.31578946113586426, 0.2857142686843872, 0.1764705777168274, 0.07692307233810425, 0.09090908616781235, 0.0555555522441864, 0.0, 0.4516128897666931]",SJFM0ZWCb,"['A fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.', 'Proposes an algorithm that integrates autoencoder with time-series data clustering using a network structure that suits time-series data.', 'An algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context, utilizing an autoencoder and clustering objective.', 'The authors proposed an unsupervised time series clustering methods built with deep neural networks and equipped with an encoder-decoder and a clustering mode to shorten the time series, extract local temporal features, and to get the encoded representations.']","['unsupervised learning timeseries data challenging problem machine learning ', ' propose novel algorithm  deep temporal clustering  dtc   fully unsupervised method  naturally integrate dimensionality reduction temporal clustering single end end learning framework ', 'algorithm start initial cluster estimate using autoencoder dimensionality reduction novel temporal clustering layer cluster assignment ', 'jointly optimizes clustering objective dimensionality reduction objective ', 'based requirement application  temporal clustering layer customized temporal similarity metric ', 'several similarity metric considered compared ', 'gain insight feature network learned clustering  apply visualization method generates heat map region interest timeseries ', 'viability algorithm demonstrated using timeseries data diverse domain  ranging earthquake sensor data spacecraft ', 'case  show algorithm outperforms traditional method ', 'performance attributed fully integrated temporal dimensionality reduction clustering criterion ']","Unsupervised learning of timeseries data is a challenging problem in machine learning., Here, 
we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework., The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment., Then it jointly optimizes the clustering objective and the dimensionality reduction objective., Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric., Several similarity metrics are considered and compared.  , To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries., The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft., In each case, we show that our algorithm outperforms traditional methods., This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.",18,6.079545454545454,9.777777777777779
164,"['We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.', 'Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied.', 'MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training.', ""In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning."", ""It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain."", 'MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes.', ""While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem."", 'We design different training strategies of MahiNet for supervised learning and meta-learning.', ""Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem."", 'In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.5625, 0.25, 0.0476190410554409, 0.12903225421905518, 0.23529411852359772, 0.17777776718139648, 0.11764705181121826, 0.25806450843811035, 0.09999999403953552, 0.3499999940395355]",rJlcV2Actm,"['A memory-augmented neural network that addresses many-class few-shot problem by leveraging class hierarchy in both supervised learning and meta-learning.', 'This paper presents methods for adding inductive bias to a classifier through coarse-to-fine prediction along a class hierarchy and learning a memory-based KNN classifier that keeps track of mislabeled instances during learning.', 'This paper formulates the many-class-few-shot classification problem from a supervised learning perspective and a meta-learning perspective.']","['study manyclass fewshot  mcfs  problem supervised learning metalearning scenario ', 'compared wellstudied manyclass manyshot fewclass fewshot problem  mcfs problem commonly occurs practical application rarely studied ', 'mcfs brings new challenge need distinguish many class  sample per class available training ', 'paper  propose  memoryaugmented hierarchicalclassification network  mahinet   mcfs learning ', 'address  manyclass  problem exploring class hierarchy  eg  coarseclass label cover subset fine class  help narrow candidate fine class cheaper obtain ', 'mahinet us convolutional neural network  cnn  extract feature  integrates memoryaugmented attention module multilayer perceptron  mlp  produce probability coarse fine class ', 'mlp extends linear classifier  attention module extends knn classifier  together targeting   fewshot  problem ', 'design different training strategy mahinet supervised learning metalearning ', 'moreover  propose two novel benchmark datasets  mcfsimagenet   subset imagenet   mcfsomniglot   resplitted omniglot  specifically mcfs problem ', 'experiment  show mahinet outperforms several stateoftheart model mcfs classification task supervised learning metalearning scenario ']","We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios., Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied., MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training., In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning., It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain., MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes., While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem., We design different training strategies of MahiNet for supervised learning and meta-learning., Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem., In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.",21,6.247619047619048,10.0
165,"['Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.', 'In this work, we focus on learning a representation that would be useful in a clustering task.', 'We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.', 'We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.']","[0, 0, 1, 0]","[0.1904761791229248, 0.277777761220932, 0.2800000011920929, 0.1249999925494194]",S17mtzbRb,"['A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.', 'This paper proposes two regularization terms based on a compound hinge loss over the KL divergence between two softmax-normalized input arguments to encourage learning disentangled representations', 'Proposal for two regularizers intended to make the representations learned in the penultimate layer of a classifier more conforming to inherent structure in the data.']","['learning better representation neural network challenging problem  tackled different perspective past year ', 'work  focus learning representation would useful clustering task ', 'introduce two novel loss component substantially improve quality produced cluster  simple apply arbitrary model cost function  require complicated training procedure ', 'perform extensive set experiment  supervised unsupervised  evaluate proposed loss component two common type model  recurrent neural network convolutional neural network  showing approach propose consistently improves quality kmeans clustering term mutual information score outperforms previously proposed method ']","Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years., In this work, we focus on learning a representation that would be useful in a clustering task., We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure., We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.",12,5.674603174603175,10.5
166,"['In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.\n', 'While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.\n', 'We study the interaction between nearest neighbor algorithms and neural networks in more detail.\n', 'We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.\n', 'Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.\n', 'Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.']","[0, 0, 0, 1, 0, 0]","[0.13333332538604736, 0.1666666567325592, 0.13793103396892548, 0.17142856121063232, 0.1111111044883728, 0.1428571343421936]",SkrHeXbCW,"['We show how to get good representations from the point of view of Simiarity Search.', 'Studies the impact of changing the image classification part on top of the DNN on the ability to index the descriptors with a LSH or a kd-tree algorithm.', 'Proposes to use softmax cross-entropy loss to learn a network that tries to reduce the angles between inputs and the corresponding class vectors in a supervised framework using.']","['high dimension  performance nearest neighbor algorithm depends crucially structure data ', 'traditional nearest neighbor datasets consisted mostly handcrafted feature vector  increasing number datasets come representation learned neural network ', 'study interaction nearest neighbor algorithm neural network detail ', 'find network architecture significantly influence efficacy nearest neighbor algorithm even classification accuracy unchanged ', 'based experiment  propose number training modification lead significantly better datasets nearest neighbor algorithm ', 'modification lead learned representation accelerate nearest neighbor query 5x ']","In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.
, While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.
, We study the interaction between nearest neighbor algorithms and neural networks in more detail.
, We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.
, Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.
, Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.",9,6.281818181818182,12.222222222222221
167,"['Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.', 'In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.', 'Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.', 'These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.', 'We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.', 'We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.']","[0, 1, 0, 0, 0, 0]","[0.0555555522441864, 0.29411762952804565, 0.060606054961681366, 0.1428571343421936, 0.23255813121795654, 0.13793103396892548]",HkxjYoCqKX,"['We introduce a technique that allows for gradient based training of quantized neural networks.', 'Proposes a unified and general way of training neural networks with reduced precision quantized synaptic weights and activations.', 'A new approach to quantizing activations which is state of the art or competitive on several real image problems.', 'A method for learning neural networks with quantized weights and activations by stochastically quantizing values and replacing the resulting categotical distribution with a continuous relaxation']","['neural network quantization become important research area due great impact deployment large model resource constrained device ', 'order train network effectively discretized without loss performance  introduce differentiable quantization procedure ', 'differentiability achieved transforming continuous distribution weight activation network categorical distribution quantization grid ', 'subsequently relaxed continuous surrogate allow efficient gradientbased optimization ', 'show stochastic rounding seen special case proposed approach formulation quantization grid also optimized gradient descent ', 'experimentally validate performance method mnist  cifar 10 imagenet classification ']","Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices., In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure., Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid., These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization., We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent., We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.",8,6.015625,16.0
168,"['In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations.', 'In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models.', 'Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations.', 'We show how they can be easily implemented on top of existing models.', 'Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators.', 'In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.']","[0, 0, 0, 0, 1, 0]","[0.20338982343673706, 0.18518517911434174, 0.23529411852359772, 0.1666666567325592, 0.2769230604171753, 0.19230768084526062]",SyKoKWbC-,"['We show that the mode collapse problem in GANs may be explained by a lack of information sharing between observations in a training batch, and propose a distribution-based framework for globally sharing information between gradients that leads to more stable and effective adversarial training.', 'Proposes to replace single-sample discriminators in adversarial training with discriminators that explicitly operate on distributions of examples.', 'Theory on two-sample tests and MMD and how can be beneficially incorporated into GAN framework.']","['current formulation adversarial training  discriminator expressed singleinput operator   mapping define separable observation ', 'work  argue property might help explain infamous mode collapse phenomenon adversariallytrained generative model ', 'inspired discrepancy measure twosample test probability distribution  propose distributional adversary operate sample  ie  set multiple point drawn distribution  rather single observation ', 'show easily implemented top existing model ', 'various experimental result show generator trained combination distributional adversary much stable remarkably le prone mode collapse traditional model trained observationwise prediction discriminator ', 'addition  application framework domain adaptation result strong improvement recent stateoftheart ']","In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations., In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models., Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations., We show how they can be easily implemented on top of existing models., Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators., In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.",15,6.226950354609929,9.4
169,"['Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization.', 'Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula.', 'However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task.', 'In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model.', 'Our framework is trained end to end and is fully data-driven.', 'Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.']","[0, 0, 0, 1, 0, 0]","[0.1538461446762085, 0.1111111044883728, 0.1111111044883728, 0.40909090638160706, 0.17391303181648254, 0.1764705777168274]",rJg_NjCqtX,"['We designed an end-to-end framework using sequence to sequence model to do the  chemical names standardization.', 'Standardizes non systematic names in chemical information extraction by creating a parallel corpus of non-systematic and systematic names and building a seq2seq model.', 'This work presents a method to translate non-systematic names of chemical compounds into their systematic equivalents using a combination of mechanisms']","['chemical information extraction convert chemical knowledge text true chemical database  text processing task heavily relying chemical compound name identification standardization ', 'systematic name chemical compound given  naturally much simply convert name eventually required molecular formula ', 'however  many chemical substance  shown many name besides systematic name pose great challenge task ', 'paper  propose framework auto standardization nonsystematic name corresponding systematic name using spelling error correction  byte pair encoding tokenization neural sequence sequence model ', 'framework trained end end fully datadriven ', 'standardization accuracy test dataset achieves 5404  great improvement compared previous stateoftheart result ']","Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization., Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula., However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task., In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model., Our framework is trained end to end and is fully data-driven., Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.",12,5.636986301369863,12.166666666666666
170,"['The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss.', 'This was found to correlate with a good final generalization performance.  ', 'In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.', 'We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD.', 'At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions).', 'To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD.', 'Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.']","[0, 0, 0, 0, 0, 1, 0]","[0.2181818187236786, 0.17142856121063232, 0.1304347813129425, 0.17777776718139648, 0.1860465109348297, 0.29032257199287415, 0.20338982343673706]",SkgEaj05t7,"['SGD is steered early on in training towards a region in which its step is too large compared to curvature, which impacts the rest of training. ', 'Analyzes the relationship between the convergence/generalization and the update on largest eigenvectors of Hessian of the empirical losses of DNNs.', 'This work studies the relationship between the SGD step size and the curvature of the loss surface']","['training deep neural network stochastic gradient descent  sgd  large learning rate small batchsize typically end flat region weight space  indicated small eigenvalue hessian training loss ', 'found correlate good final generalization performance ', 'paper extend previous work investigating curvature loss surface along whole training trajectory  rather endpoint ', 'find initially sgd visit increasingly sharp region  reaching maximum sharpness determined learning rate batchsize sgd ', 'peak value sgd start fail minimize loss along direction loss surface corresponding largest curvature  sharpest direction  ', 'investigate effect dynamic training process  study variant sgd using reduced learning rate along sharpest direction show improve training speed finding sharper better generalizing solution  compared vanilla sgd ', 'overall  result show sgd dynamic subspace sharpest direction influence region sgd steer  larger learning rate smaller batch size result wider region visited   overall training speed  generalization ability final model ']","The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss., This was found to correlate with a good final generalization performance.  , In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint., We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD., At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions)., To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD., Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.",15,5.236363636363636,14.666666666666666
171,"['We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems.', 'Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution.', 'Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state.', 'This allows the agent to learn a simple stochastic policy that has an easy to compute expected return.', 'In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.']","[1, 0, 0, 0, 0]","[0.3870967626571655, 0.03999999538064003, 0.14999999105930328, 0.1249999925494194, 0.12121211737394333]",SJgf6Z-0W,"['We introduce a novel reinforcement learning algorithm, that predicts multiple actions and samples from them.', 'This work introduces a uniform mixture of deterministic policies, and find that this parametrization of stochastic policies outperforms DDPG on several OpenAI gym benchmarks.', 'The authors investigate a method for improving the performance of networks trained with DDPG, and show improved performance on a large number of standard continuous control environment.']","['introduce new approach estimate continuous action using actorcritic algorithm reinforcement learning problem ', 'policy gradient method usually predict one continuous action estimate parameter presumed distribution  commonly gaussian  given state might optimal may capture complete description target distribution ', 'approach instead predicts action policy network  actor  uniformly sample one action training well testing state ', 'allows agent learn simple stochastic policy easy compute expected return ', 'experiment  facilitates better exploration state space training converges better policy ']","We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems., Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution., Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state., This allows the agent to learn a simple stochastic policy that has an easy to compute expected return., In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.",6,5.533898305084746,19.666666666666668
172,"['Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks.', 'DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse.', 'However, like other CNN models, DenseNets also face overfitting problem if not severer.', 'Existing dropout method can be applied but not as effective due to the introduced nonlinear connections.', 'In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps.', 'To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability.', 'The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections.', 'Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.0, 0.19512194395065308, 0.1538461446762085, 0.2380952388048172, 0.1702127605676651, 0.43478259444236755, 0.3199999928474426, 0.15094339847564697]",r1gOe209t7,"['Realizing the drawbacks when applying original dropout on DenseNet, we craft the design of dropout method from three aspects, the idea of which could also be applied on other CNN models.', 'Application of different binary dropout structures and schedules with the specific aim to regularise the DenseNet architecture.', 'Proposes a pre-dropout technique for densenet which implements the dropout before the non-linear activation function.']","['recently convolutional neural network  cnns  achieve great accuracy visual recognition task ', 'densenet becomes one popular cnn model due effectiveness featurereuse ', 'however  like cnn model  densenets also face overfitting problem severer ', 'existing dropout method applied effective due introduced nonlinear connection ', 'particular  property featurereuse densenet impeded  dropout effect weakened spatial correlation inside feature map ', 'address problem  craft design specialized dropout method three aspect  dropout location  dropout granularity  dropout probability ', 'insight attained could potentially applied general approach boosting accuracy cnn model similar nonlinear connection ', 'experimental result show densenets specialized dropout method yield better accuracy compared vanilla densenet stateoftheart cnn model  accuracy boost increase model depth ']","Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks., DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse., However, like other CNN models, DenseNets also face overfitting problem if not severer., Existing dropout method can be applied but not as effective due to the introduced nonlinear connections., In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps., To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability., The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections., Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.",17,5.829113924050633,9.294117647058824
173,"['While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models.', 'Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples.', 'Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.', 'Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.']","[0, 0, 1, 0]","[0.1538461446762085, 0.11428570747375488, 0.21621620655059814, 0.0]",HJeOMhA5K7,"['Guiding relation-aware deep models towards better learning with human knowledge.', 'This work proposes a variant of the column network based on the injection of human guidance by modifying calculations in the network.', 'A method to incorporate human advices to deep learning by extending Column Network, a graph neural network for collective classification.']","['extremely successful several application  especially lowlevel representation  sparse  noisy sample structured domain  multiple object interaction  open challenge deep model ', 'column network  deep architecture  succinctly capture domain structure interaction  may still prone suboptimal learning sparse noisy sample ', 'inspired success humanadvice guided learning ai  especially datascarce domain  propose knowledgeaugmented column network leverage human adviceknowledge better learning noisysparse sample ', 'experiment demonstrate approach lead either superior overall performance faster convergence ']","While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models., Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples., Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples., Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.",11,6.405940594059406,9.181818181818182
174,"['Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent.', 'However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations.', 'Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors.', 'In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved.', 'Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  ', 'Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations.', 'Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks.', 'Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.1666666567325592, 0.20512819290161133, 0.2790697515010834, 0.19512194395065308, 0.10256409645080566, 0.11764705181121826, 0.052631575614213943, 0.10526315122842789]",B1IDRdeCW,"['Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors', 'Investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks.', 'This paper analyzes the effectiveness of binary neural networks and why binarization is able to preserve model performance.']","['recent research shown one train neural network binary weight activation train time augmenting weight highprecision continuous latent variable accumulates small change stochastic gradient descent ', 'however  dearth work explain one effectively capture feature data binary weight activation ', 'main result neural network binary weight activation trained using method courbariaux  hubara et al   2016  work highdimensional geometry binary vector ', 'particular  ideal continuous vector extract feature intermediate representation bnns wellapproximated binary vector sense dot product approximately preserved ', 'compared previous research demonstrated good classification performance bnns  work explains bnns work term hd geometry ', 'furthermore  result analysis used bnns shown generalize neural network ternary weight activation ', 'theory serf foundation understanding bnns variety method seek compress traditional neural network ', 'furthermore  better understanding multilayer binary neural network serf starting point generalizing bnns neural network architecture recurrent neural network ']","Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent., However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations., Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors., In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved., Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  , Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations., Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks., Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.",14,5.62962962962963,14.4
175,"['In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR).', 'In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.', 'We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator.', 'Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries.', 'In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.']","[0, 0, 1, 0, 0]","[0.045454539358615875, 0.2857142686843872, 0.3333333432674408, 0.21052631735801697, 0.2535211145877838]",SyqAPeWAZ,"['After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art.', 'Discusses using neural networks for super-resolution', 'A new architecture for solving image super-resolution tasks, and an analysis aiming to establish a connection between CNNs for solving super resolution and solving sparse regularized inverse problems.']","['recent year convolutional neural network  cnn  used extensively superresolution  sr  ', 'paper  use inverse problem sparse representation solution form mathematical basis cnn operation ', 'show single neuron able provide optimum solution inverse problem  given low resolution image dictionary operator ', 'introducing new concept called representation dictionary duality  show cnn element  filter  trained representation vector  reconstruction  used dictionary ', 'light theoretical work  propose new algorithm us two network different structure separately trained low high coherency image patch show performs faster compared stateoftheart algorithm sacrificing performance ']","In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR)., In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations., We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator., Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries., In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.",11,5.653846153846154,11.818181818181818
176,"['We consider the learning of algorithmic tasks by mere observation of input-output\n', 'pairs.', 'Rather than studying this as a black-box discrete regression problem with\n', 'no assumption whatsoever on the input-output mapping, we concentrate on tasks\n', 'that are amenable to the principle of divide and conquer, and study what are its\n', 'implications in terms of learning.\n', 'This principle creates a powerful inductive bias that we leverage with neural\n', 'architectures that are defined recursively and dynamically, by learning two scale-\n', 'invariant atomic operations: how to split a given input into smaller sets, and how\n', 'to merge two partially solved tasks into a larger partial solution.', 'Our model can be\n', 'trained in weakly supervised environments, namely by just observing input-output\n', 'pairs, and in even weaker environments, using a non-differentiable reward signal.\n', 'Moreover, thanks to the dynamic aspect of our architecture, we can incorporate\n', 'the computational complexity as a regularization term that can be optimized by\n', 'backpropagation.', 'We demonstrate the flexibility and efficiency of the Divide-\n', 'and-Conquer Network on several combinatorial and geometric tasks: convex hull,\n', 'clustering, knapsack and euclidean TSP.', 'Thanks to the dynamic programming\n', 'nature of our model, we show significant improvements in terms of generalization\n', 'error and computational complexity.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.09090908616781235, 0.0, 0.0, 0.25, 0.0, 0.08695651590824127, 0.27272728085517883, 0.0833333283662796, 0.0, 0.13333332538604736, 0.0952380895614624, 0.08695651590824127, 0.0, 0.17391303181648254, 0.10526315122842789, 0.0952380895614624, 0.1249999925494194, 0.0, 0.0, 0.13333332538604736]",B1jscMbAW,"['Dynamic model that learns divide and conquer strategies by weak supervision.', 'Proposes to add new inductive bias to neural network architecture by using a divide and conquer strategy.', 'This paper studies problems that can be solved using a dynamic programming approach, and proposes a neural network architecture to solve such problems that beats sequence to sequence baselines.', 'The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks.']","['consider learning algorithmic task mere observation inputoutput', 'pair ', 'rather studying blackbox discrete regression problem', 'assumption whatsoever inputoutput mapping  concentrate task', 'amenable principle divide conquer  study', 'implication term learning ', 'principle creates powerful inductive bias leverage neural', 'architecture defined recursively dynamically  learning two scale', 'invariant atomic operation  split given input smaller set ', 'merge two partially solved task larger partial solution ', 'model', 'trained weakly supervised environment  namely observing inputoutput', 'pair  even weaker environment  using nondifferentiable reward signal ', 'moreover  thanks dynamic aspect architecture  incorporate', 'computational complexity regularization term optimized', 'backpropagation ', 'demonstrate flexibility efficiency divide', 'andconquer network several combinatorial geometric task  convex hull ', 'clustering  knapsack euclidean tsp ', 'thanks dynamic programming', 'nature model  show significant improvement term generalization', 'error computational complexity ']","We consider the learning of algorithmic tasks by mere observation of input-output
, pairs., Rather than studying this as a black-box discrete regression problem with
, no assumption whatsoever on the input-output mapping, we concentrate on tasks
, that are amenable to the principle of divide and conquer, and study what are its
, implications in terms of learning.
, This principle creates a powerful inductive bias that we leverage with neural
, architectures that are defined recursively and dynamically, by learning two scale-
, invariant atomic operations: how to split a given input into smaller sets, and how
, to merge two partially solved tasks into a larger partial solution., Our model can be
, trained in weakly supervised environments, namely by just observing input-output
, pairs, and in even weaker environments, using a non-differentiable reward signal.
, Moreover, thanks to the dynamic aspect of our architecture, we can incorporate
, the computational complexity as a regularization term that can be optimized by
, backpropagation., We demonstrate the flexibility and efficiency of the Divide-
, and-Conquer Network on several combinatorial and geometric tasks: convex hull,
, clustering, knapsack and euclidean TSP., Thanks to the dynamic programming
, nature of our model, we show significant improvements in terms of generalization
, error and computational complexity.",33,5.787878787878788,6.0
177,"['Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$.', 'Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.', 'To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.', 'We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).', 'Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.']","[0, 0, 0, 0, 1]","[0.1428571343421936, 0.08695651590824127, 0.15686273574829102, 0.04651162400841713, 0.19999998807907104]",ryf6Fs09YX,"['a Rep-like gradient for non-reparameterizable continuous/discrete distributions; further generalized to deep probabilistic models, yielding statistical back-propagation', 'Presents a gradient estimator for expectation-based objectives that is unbiased, has low variance, and applies to either continuous and discrete random variables.', 'An improved method for computing derivates of the expectation, and a new gradient estimator of low variance that allows training of generative models in which observations or latent variables are discrete.', 'Designs a low variance gradient for distributions associated with continuous or discrete random variables.']","['within many machine learning algorithm  fundamental problem concern efficient calculation unbiased gradient wrt parameter  boldsymbol  gamma   expectationbased objective  mathbb  e    q  boldsymbol  gamma    boldsymbol      f  boldsymbol      ', 'existing method either     suffer high variance  seeking help  often  complicated variancereduction technique    ii   apply reparameterizable continuous random variable employ reparameterization trick ', 'address limitation  propose general onesample  go  gradient     applies many distribution associated nonreparameterizable continuous  em  discrete random variable    ii   lowvariance reparameterization trick ', 'find go gradient often work well practice based one monte carlo sample  although one course use sample desired  ', 'alongside go gradient  develop mean propagating chain rule distribution  yielding statistical backpropagation  coupling neural network common random variable ']","Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\boldsymbol{\gamma}$ for expectation-based objectives $\mathbb{E}_{q_{\boldsymbol{\gamma}} (\boldsymbol{y})} [f (\boldsymbol{y}) ]$., Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick., To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick., We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired)., Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.",12,6.653061224489796,12.25
178,"['Quantum computers promise significant advantages over classical computers for a number of different applications.', 'We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.', 'We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.', 'We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.', 'We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.']","[0, 1, 0, 0, 0]","[0.10810810327529907, 0.35555556416511536, 0.307692289352417, 0.21739129722118378, 0.1428571343421936]",SyxvSiCcFQ,"['We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training.', 'Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network.', 'This paper proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks.', 'Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).']","['quantum computer promise significant advantage classical computer number different application ', 'show complete loss function landscape neural network represented quantum state output quantum computer ', 'demonstrate explicitly binary neural network   show quantum computer train network manipulating state using wellknown algorithm known quantum amplitude amplification ', 'show minor adaptation  method also represent metaloss landscape number neural network architecture simultaneously ', 'search metaloss landscape method simultaneously train design binary neural network ']","Quantum computers promise significant advantages over classical computers for a number of different applications., We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer., We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification., We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously., We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.",8,5.669642857142857,14.0
179,"['Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations.', 'These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.', ""We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others."", 'We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness.', 'Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.']","[1, 0, 0, 0, 0]","[0.41379308700561523, 0.06666666269302368, 0.1818181723356247, 0.3255814015865326, 0.0952380895614624]",BygANhA9tQ,"['A general method for training certified cost-sensitive robust classifier against adversarial perturbations', 'Calculates and plugs in the costs of adversarial attack into the objective of optimization to get a model that is cost-sensitively robust against adversarial attacks. ', 'Build on semnial work by Dalvi et al. and extends approach to certifiable robustness with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples.']","['several recent work developed method training classifier certifiably robust normbounded adversarial perturbation ', 'method assume adversarial transformation equally important  seldom case realworld application ', 'advocate costsensitive robustness criterion measuring classifier performance task adversarial transformation important others ', 'encode potential harm adversarial transformation cost matrix  propose general objective function adapt robust training method wong  kolter  2018  optimize costsensitive robustness ', 'experiment simple mnist cifar10 model variety cost matrix show proposed approach produce model substantially reduced costsensitive robust error  maintaining classification accuracy ']","Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations., These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications., We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others., We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness., Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.",8,6.244094488188976,15.875
180,"['Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain.', 'However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina.', 'Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response.', 'This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed.', 'Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina.', 'The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input.', 'Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.']","[0, 0, 0, 0, 0, 0, 1]","[0.15789473056793213, 0.19354838132858276, 0.23529411852359772, 0.25641024112701416, 0.3720930218696594, 0.3030303120613098, 0.47058823704719543]",HJhIM0xAW,"['Using triplets to learn a metric for comparing neural responses and improve the performance of a prosthesis.', 'Authors develop new spike train distance metrics, including neural networks and quadratic metrics. These metrics are shown to outperform the naive Hamming distance metric, and implicitly captures some structure in neural code.', 'With the application of improving neural prosthesis in mind, the authors propose to learn a metric between neural responses by either optimizing a quadratic form or a deep neural network .']","['retinal prosthesis treating incurable blindness designed electrically stimulate surviving retinal neuron  causing send artificial visual signal brain ', 'however  electrical stimulation generally precisely reproduce normal pattern neural activity retina ', 'therefore  electrical stimulus must selected produce neural response close possible desired response ', 'requires technique computing distance desired response achievable response meaningful term visual signal conveyed ', 'propose method learn metric neural response  directly recorded light response population retinal ganglion cell  rgcs  primate retina ', 'learned metric produce measure similarity rgc population response accurately reflects similarity visual input ', 'using data electrical stimulation experiment  demonstrate metric may improve performance prosthesis ']","Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain., However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina., Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response., This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed., Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina., The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input., Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.",12,5.741935483870968,12.916666666666666
181,"['We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.', 'Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem.', 'The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth).', 'Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic.', 'We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries.', 'In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention.', 'Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.']","[0, 0, 0, 0, 1, 0, 0]","[0.1428571343421936, 0.1764705777168274, 0.10810810327529907, 0.15686273574829102, 0.19512194395065308, 0.11764705181121826, 0.05882352590560913]",r5vnRRwrgX,"['This paper introduces a method to generate questions (cues) and queries (suggestions) to help users perform mind-mapping.', 'Presents a tool to assist mind-mapping through suggested context related to existing nodes and through questions that expand on less developed branches.', 'This paper presents an approach for assisting people with mindmapping tasks, designing an interface and algorithmic features to suppport mindmapping, and contributes a evaluative study.']","['introduce novel workflow  qcue  providing textual stimulation mindmapping ', 'mindmapping powerful tool whose intent allow one externalize idea relationship surrounding central problem ', 'key challenge mindmapping difficulty balancing exploration different aspect problem  breadth  detailed exploration aspect  depth  ', 'idea behind qcue based two mechanism   1  computergenerated automatic cue stimulate user explore breadth topic based temporal topological evolution mindmap  2  userelicited query helping user explore depth given topic ', 'present twophase study wherein first phase provided insight led development workflow stimulating user cue query ', 'second phase  present betweensubjects evaluation comparing qcue digital mindmapping workflow without computer intervention ', 'finally  present expert rater evaluation mindmaps created user conjunction user feedback ']","We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping., Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem., The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth)., Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic., We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries., In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention., Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.",11,5.522988505747127,15.818181818181818
182,"['The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.', 'In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.', 'Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.', 'Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  ', 'The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.', 'Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.', 'The source code of our method will be made publicly available.']","[0, 1, 0, 0, 0, 0, 0]","[0.0, 0.19512194395065308, 0.1818181723356247, 0.0, 0.10526315122842789, 0.08695651590824127, 0.0]",rkgpCoRctm,"['Detecting out-of-distribution samples by using low-order feature statistics without requiring any change in underlying DNN.', 'Presents an algorithm to detect out-of-distribution samples by using the running estimate of mean and variance within BatchNorm layers to construct feature representations later fed into a linear classifier.', 'An approach for detecting out-of-distribution samples in which the authors propose to use logistic regression over simple statistics of each batch normalization layer of CNN.', 'The paper suggests using Z-scores for comparing ID and OOD samples to evaluate what deep nets are trying to do.']","['ability detect input sample drawn training distribution important desirable property deep neural network ', 'paper  show simple ensembling first second order deep feature statistic exploited effectively differentiate indistribution outofdistribution sample ', 'specifically  observe mean standard deviation within feature map differs greatly indistribution outofdistribution sample ', 'based observation  propose simple efficient plugandplay detection procedure require retraining  preprocessing change model ', 'proposed method outperforms stateoftheart large margin standard benchmarking task  much simpler implement execute ', 'notably  method improves true negative rate 396  953  95  indistribution  cifar100  correctly detected using densenet outofdistribution dataset tinyimagenet resize ', 'source code method made publicly available ']","The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks., In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples., Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples., Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  , The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute., Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize., The source code of our method will be made publicly available.",13,5.917721518987341,12.153846153846153
183,"['Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security.', 'However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts.', 'In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder.', 'In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries.', 'We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.']","[0, 0, 1, 0, 0]","[0.11764705181121826, 0.21739129722118378, 0.3404255211353302, 0.0, 0.08888888359069824]",ByloIiCqYQ,"['We propose a novel method named Maximal Divergence Sequential Auto-Encoder that leverages Variational AutoEncoder representation for binary code vulnerability detection.', 'This paper proposes a variational autoencoder-based architecture for code embeddings for binary software vulnerability detection, with learned embeddings more effective at distinguishing between vulnerable and non-vulnerable binary code than baselines.', 'This paper proposes a model to automatically extract features for vulnerability detection using deep learning technique. \n']","['due sharp increase severity threat imposed software vulnerability  detection vulnerability binary code become important concern software industry  embedded system industry  field computer security ', 'however  work binary code vulnerability detection relied handcrafted feature manually chosen select  knowledgeable domain expert ', 'paper  attempt alleviate severe binary vulnerability detection bottleneck leveraging recent advance deep learning representation propose maximal divergence sequential autoencoder ', 'particular  latent code representing vulnerable nonvulnerable binary encouraged maximally divergent  still able maintain crucial information original binary ', 'conducted extensive experiment compare contrast proposed method baseline  result show proposed method outperform baseline performance measure interest ']","Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security., However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts., In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder., In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries., We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.",14,5.891025641025641,11.142857142857142
184,"['Modern neural architectures critically rely on attention for mapping structured inputs to sequences.', 'In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\n', 'We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  ', 'First, the position where attention is marginalized is changed from the input to the output.', 'Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output.', 'Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.']","[0, 0, 0, 0, 1, 0]","[0.23076923191547394, 0.10810810327529907, 0.1463414579629898, 0.1599999964237213, 0.3571428656578064, 0.2857142686843872]",BkltNhC9FX,"['Computing attention based on posterior distribution leads to more meaningful attention and better performance', 'This paper proposes a sequence to sequence model where attention is treated as a latent variable, and derives novel inference procedures for this model, obtaining improvements in machine translation and morphological inflection generation tasks.', 'This paper presents a novel posterior attention model for seq2seq problems']","['modern neural architecture critically rely attention mapping structured input sequence ', 'paper show prevalent attention architecture adequately model dependence among attention output token across predicted sequence ', 'present alternative architecture called posterior attention model principled factorization full joint distribution attention output variable  proposes two major change ', 'first  position attention marginalized changed input output ', 'second  attention propagated next decoding stage posterior attention distribution conditioned output ', 'empirically five translation two morphological inflection task proposed posterior attention model yield better bleu score alignment accuracy existing attention model ']","Modern neural architectures critically rely on attention for mapping structured inputs to sequences., In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.
, We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  , First, the position where attention is marginalized is changed from the input to the output., Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output., Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.",9,6.056,13.88888888888889
185,"['The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms.', 'Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation.', 'This is achieved by either eliminating components from the model, or penalizing complexity during training.', 'While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions.', 'In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.', 'In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it.', 'We will show that using this technique, we can achieve competitive results.']","[0, 0, 1, 0, 0, 0, 0]","[0.0, 0.07407406717538834, 0.1538461446762085, 0.13333332538604736, 0.13333332538604736, 0.0, 0.0]",By0ANxbRW,"['Compressing trained DNN models by minimizing their complexity while constraining their loss.', 'This paper proposes a method for deep neural network compression under accuracy constraints.', 'This paper presents a loss value constrained k-means encoding method for network compression and develops an iterative algorithm for model optimization.']","['growing interest implement deep neural network  dnns  resourcebound hardware motivated innovation compression algorithm ', 'using algorithm  dnn model size substantially reduced  little accuracy degradation ', 'achieved either eliminating component model  penalizing complexity training ', 'approach demonstrate considerable compression  former often ignores loss function compression later produce unpredictable compression ', 'paper  propose technique directly minimizes model complexity change loss function ', 'technique  formulate compression constrained optimization problem  present solution ', 'show using technique  achieve competitive result ']","The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms., Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation., This is achieved by either eliminating components from the model, or penalizing complexity during training., While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions., In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function., In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it., We will show that using this technique, we can achieve competitive results.",15,6.008264462809917,8.066666666666666
186,"['Deep neural networks are able to solve tasks across a variety of domains and modalities of data.', 'Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes.', ""In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations."", 'Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs.', 'The computed ""attention masks"" support improved interpretability by highlighting which input attributes are critical in determining output.', 'We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing.', ""The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.""]","[1, 0, 0, 0, 0, 0, 0]","[0.27586206793785095, 0.10810810327529907, 0.24242423474788666, 0.1428571343421936, 0.06666666269302368, 0.2222222238779068, 0.05405404791235924]",SJ60SbW0b,"['We develop a technique to visualize attention mechanisms in arbitrary neural networks. ', 'Proposes to learn a Latent Attention Network that can help to visualize the inner structure of a deep neural network.', 'The authors of this paper propose a data-driven black-box visualization scheme. ']","['deep neural network able solve task across variety domain modality data ', 'despite many empirical success  lack ability clearly understand interpret learned mechanism contribute effective behavior critically  failure mode ', 'work  present general method visualizing arbitrary neural network inner mechanism power limitation ', 'datasetcentric method produce visualization trained network attends component input ', 'computed  attention mask  support improved interpretability highlighting input attribute critical determining output ', 'demonstrate effectiveness framework variety deep neural network architecture domain computer vision natural language processing ', 'primary contribution approach interpretable visualization attention provides unique insight network underlying decisionmaking process irrespective data modality ']","Deep neural networks are able to solve tasks across a variety of domains and modalities of data., Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes., In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations., Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs., The computed ""attention masks"" support improved interpretability by highlighting which input attributes are critical in determining output., We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing., The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.",10,6.046979865771812,14.9
187,"['The design of small molecules with bespoke properties is of central importance to drug discovery.  ', 'However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  ', 'This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  ', 'The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  ', 'Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.']","[0, 0, 0, 1, 0]","[0.04651162400841713, 0.13114753365516663, 0.19672130048274994, 0.2857142686843872, 0.13114753365516663]",HkcTe-bR-,"['We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.', 'Considers model evaluation for molecule generation by proposing 19 benchmarks, expanding small data sets to a large, standardized dataset, and exploring how to apply RL techniques for molecular design.', 'This paper shows that the most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO as the exception, when modeling and synthesizing molecules.']","['design small molecule bespoke property central importance drug discovery ', 'however significant challenge yet remain computational method  despite recent advance deep recurrent network reinforcement learning strategy sequence generation  difficult compare result across different work ', 'work proposes 19 benchmark selected subject expert  expands smaller datasets previously used approximately 11 million training molecule  explores apply new reinforcement learning technique effectively molecular design ', 'benchmark  built openai gym environment  opensourced encourage innovation molecular design algorithm enable usage without background chemistry ', 'finally  work explores recent development reinforcementlearning method excellent sample complexity  a2c ppo algorithm  investigates behavior molecular generation  demonstrating significant performance gain compared standard reinforcement learning technique ']","The design of small molecules with bespoke properties is of central importance to drug discovery.  , However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  , This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  , The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  , Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.",13,6.465753424657534,11.23076923076923
188,"['Analogical reasoning has been a principal focus of various waves of AI research.', 'Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience.', 'Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data.', 'We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model.', 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features.', 'Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.']","[0, 0, 0, 0, 1, 0]","[0.05882352590560913, 0.21276594698429108, 0.27272728085517883, 0.15094339847564697, 0.7241379022598267, 0.09302324801683426]",SylLYsCcFm,"['The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains.', 'The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems', 'This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']","['analogical reasoning principal focus various wave ai research ', 'analogy particularly challenging machine requires relational structure represented flexibly applied across diverse domain experience ', ' study analogical reasoning induced neural network learn perceive reason raw visual data ', 'find critical factor inducing capacity elaborate architecture  rather  careful attention choice data manner presented model ', 'robust capacity analogical reasoning induced network learn analogy contrasting abstract relational structure input domain  training method us input data force model learn important abstract feature ', 'using technique demonstrate capacity complex  visual symbolic analogy making generalisation even simplest neural network architecture ']","Analogical reasoning has been a principal focus of various waves of AI research., Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience., Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data., We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model., The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features., Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.",11,5.471698113207547,14.454545454545455
189,"['Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding.', 'Much progress has been made to build conversation models using techniques such as sequence2sequence modeling.', 'One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals.', 'Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome.', 'However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model.', 'In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents.', 'Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged.', 'We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.0, 0.0, 0.07407406717538834, 0.0, 0.054054051637649536, 0.0, 0.0]",HJXyS7bRb,"['A Goal-oriented Neural Conversation Model by Self-Play', 'A self-play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model.', 'This paper describes a method for improving a goal oriented dialogue system using selfplay. ']","['building chatbots accomplish goal booking flight ticket unsolved problem natural language understanding ', 'much progress made build conversation model using technique sequence2sequence modeling ', 'one challenge applying technique building goaloriented conversation model maximum likelihoodbased model optimized toward accomplishing goal ', 'recently  many method proposed address issue optimizing reward contains task status outcome ', 'however  adding reward optimization fly usually provides little guidance language construction conversation model soon becomes decoupled language model ', 'paper  propose new setting goaloriented dialogue system tighten gap two aspect enforcing model level information isolation individual model two agent ', 'language construction becomes important part reward optimization since way information exchanged ', 'experimented model using selfplay result showed method beat baseline sequence2sequence model reward also generate humanreadable meaningful conversation comparable quality ']","Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding., Much progress has been made to build conversation models using techniques such as sequence2sequence modeling., One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals., Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome., However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model., In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents., Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged., We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.",11,5.913978494623656,16.90909090909091
190,"['Search engine users nowadays heavily depend on query completion and correction to shape their queries.', ' Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database', '. In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix', '.  We show how to address two main challenges that renders this method practical for large-scale deployment', ': 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and', '2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms).', 'Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.\n']","[1, 0, 0, 0, 0, 0, 0]","[0.1666666567325592, 0.0714285671710968, 0.1428571343421936, 0.0, 0.1621621549129486, 0.0, 0.0]",By3VrbbAb,"['realtime search query completion using character-level LSTM language models', 'This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU.', 'The authors propose an algorithm for solving the query completion problem with error correction, and adopt character-level RNN-based modeling and optimize the inference part to achieve targets in real time.']","['search engine user nowadays heavily depend query completion correction shape query ', 'typically  completion done database lookup understand context generalize prefix database', ' paper  propose use unsupervised deep language model complete correct query given arbitrary prefix', ' show address two main challenge render method practical largescale deployment', ' 1  propose method integrating error correction language model completion via editdistance potential variant beam search exploit potential function ', '2  show efficiently perform cpubased computation complete query  error correction  real time  generating top 10 completion within 16 m  ', 'experiment show method substantially increase hit rate standard approach  capable handling tail query ']","Search engine users nowadays heavily depend on query completion and correction to shape their queries.,  Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database, . In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix, .  We show how to address two main challenges that renders this method practical for large-scale deployment, : 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and, 2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms)., Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.
",12,5.363636363636363,11.0
191,"['RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear.', 'Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants.', 'In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways.', 'First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.\n\n', ""Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10."", 'Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \\beta_1.', 'We show that at very high values of the momentum parameter (\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses.', ""On the other hand, NAG can sometimes do better when ADAM's \\beta_1 is set to the most commonly used value: \\beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.\n\n"", 'We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.25, 0.038461532443761826, 0.3181818127632141, 0.3396226465702057, 0.2857142686843872, 0.2790697515010834, 0.13333332538604736, 0.1355932205915451, 0.25]",rkgd0iA9FQ,"['In this paper we prove convergence to criticality of (stochastic and deterministic) RMSProp and deterministic ADAM for smooth non-convex objectives and we demonstrate an interesting beta_1 sensitivity for ADAM on autoencoders. ', 'This paper presents a convergence analysis of RMSProp and ADAM in the case of smooth non-convex functions']","['rmsprop adam continue extremely popular algorithm training neural net theoretical convergence property remained unclear ', ' recent work seemed suggest algorithm worse generalization property compared carefully tuned stochastic gradient descent momentum variant ', 'work  make progress towards deeper understanding adam rmsprop two way ', 'first  provide proof adaptive gradient algorithm guaranteed reach criticality smooth nonconvex objective  give bound running time ', 'next design experiment empirically study convergence generalization property rmsprop adam nesterov accelerated gradient method variety common autoencoder setup vgg9 cifar10 ', 'experiment demonstrate interesting sensitivity adam momentum parameter beta1 ', 'show high value momentum parameter  beta1  099  adam outperforms carefully tuned nag experiment  term getting lower training test loss ', 'hand  nag sometimes better adam beta1 set commonly used value  beta1  09  indicating importance tuning hyperparameters adam get better generalization performance ', 'also report experiment different autoencoders demonstrate nag better ability term reducing gradient norm  also produce iterates exhibit increasing trend minimum eigenvalue hessian loss function iterates ']","RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear., Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants., In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways., First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.

, Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10., Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \beta_1., We show that at very high values of the momentum parameter (\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses., On the other hand, NAG can sometimes do better when ADAM's \beta_1 is set to the most commonly used value: \beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.

, We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.",17,5.509803921568627,15.0
192,"['Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems.', 'We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model.', 'The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners.', 'We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network.', 'To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel.', 'Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer.', 'As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed.', 'We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm.', 'Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.']","[0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.19512194395065308, 0.24390242993831635, 0.1875, 0.24390242993831635, 0.14999999105930328, 0.11764705181121826, 0.19999998807907104, 0.1538461446762085, 0.25641024112701416]",HyI6s40a-,"['Devising unsupervised defense mechanisms against adversarial attacks is crucial to ensure the generalizability of the defense. ', 'This paper presents a method for detecting adversarial examples in a deep learning classification setting', 'This paper presents an unsupervised method for detecting adversarial examples of neural networks.']","['recent advance adversarial deep learning  dl  opened new largely unexplored surface malicious attack jeopardizing integrity autonomous dl system ', 'introduce novel automated countermeasure called parallel checkpointing learner  pcl  thwart potential adversarial attack significantly improve reliability  safety  victim dl model ', 'proposed pcl methodology unsupervised  meaning adversarial sample leveraged buildtrain parallel checkpointing learner ', 'formalize goal preventing adversarial attack optimization problem minimize rarely observed region latent feature space spanned dl network ', 'solve aforementioned minimization problem  set complementary disjoint checkpointing module trained leveraged validate victim model execution parallel ', 'checkpointing learner explicitly characterizes geometry input data corresponding highlevel data abstraction within particular dl layer ', ' adversary required simultaneously deceive defender module order succeed ', 'extensively evaluate performance pcl methodology stateoftheart attack scenario  including fastgradientsign  fgs   jacobian saliency map attack  jsma   deepfool  carlini  wagnerl2 algorithm ', 'extensive proofofconcept evaluation analyzing various data collection including mnist  cifar10  imagenet corroborate effectiveness proposed defense mechanism adversarial sample ']","Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems., We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model., The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners., We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network., To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel., Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer., As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed., We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm., Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.",18,6.376744186046512,11.944444444444445
193,"['Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures.', 'However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet).', 'Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size).', 'As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs.', 'These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task.', 'In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms.', 'We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set.', 'Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization.', 'On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6 fewer parameters.', 'On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2 faster with measured GPU latency.', 'We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.21052631735801697, 0.3265306055545807, 0.15094339847564697, 0.21276594698429108, 0.31578946113586426, 0.2857142686843872, 0.30188679695129395, 0.1764705777168274, 0.12244897335767746, 0.045454539358615875, 0.21739129722118378]",HylVB3AqYm,"['Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.', 'This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size.']","['neural architecture search  na  great impact automatically designing effective neural network architecture ', 'however  prohibitive computational demand conventional na algorithm  eg  10 4 gpu hour  make difficult directly search architecture largescale task  eg  imagenet  ', 'differentiable na reduce cost gpu hour via continuous representation network architecture suffers high gpu memory consumption issue  grow linearly wrt  candidate set size  ', 'result  need utilize proxy task  training smaller dataset  learning block  training epoch ', 'architecture optimized proxy task guaranteed optimal target task ', 'paper  present proxylessnas directly learn architecture largescale target task target hardware platform ', 'address high memory consumption issue differentiable na reduce computational cost  gpu hour gpu memory  level regular training still allowing large candidate set ', 'experiment cifar10 imagenet demonstrate effectiveness directness specialization ', 'cifar10  model achieves 208  test error 57m parameter  better previous stateoftheart architecture amoebanetb  using 6 fewer parameter ', 'imagenet  model achieves 31  better top1 accuracy mobilenetv2  12 faster measured gpu latency ', 'also apply proxylessnas specialize neural architecture hardware direct hardware metric  eg  latency  provide insight efficient cnn architecture design ']","Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures., However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet)., Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size)., As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs., These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task., In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms., We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set., Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization., On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6 fewer parameters., On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2 faster with measured GPU latency., We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",22,5.738095238095238,9.692307692307692
194,"['With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.', 'However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.', 'In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\\cite{hendrycks2016baseline}.', 'Our method first assumes there exists a underlying higher-order distribution $\\mathcal{P}(z)$', ', which generated label-wise distribution $\\mathcal{P}(y)$', 'over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However', ', we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore', ', we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Through', 'comprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.0, 0.054054051637649536, 0.1621621549129486, 0.0, 0.0, 0.13636364042758942, 0.07692307233810425, 0.0, 0.1111111044883728]",ByxmXnA9FQ,"['A new framework based variational inference for out-of-distribution detection', 'Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection.', 'A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets.', 'An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.']","['recently rapid development deep learning  deep neural network widely adopted many reallife application ', 'however  deep neural network also known little control uncertainty test example  potentially cause harmful annoying consequence practical scenario ', 'paper  particularly interested designing higherorder uncertainty metric deep neural network investigate performance outofdistribution detection task proposed bycite  hendrycks2016baseline  ', 'method first assumes exists underlying higherorder distribution  mathcal  p   z  ', ' generated labelwise distribution  mathcal  p    ', 'class kdimension simplex  approximate higherorder distribution via parameterized posterior function  p  theta   zx   variational inference framework  finally use entropy learned posterior distribution  p  theta   zx   uncertainty measure detect outofdistribution example  however', ' identify overwhelming overconcentration issue framework  greatly hinders detection performance  therefore', ' design logsmoothing function alleviate issue greatly increase robustness proposed entropybased uncertainty measure ', 'comprehensive experiment various datasets architecture  proposed variational dirichlet framework entropybased uncertainty measure consistently observed yield significant improvement many baseline system ']","With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications., However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios., In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}., Our method first assumes there exists a underlying higher-order distribution $\mathcal{P}(z)$, , which generated label-wise distribution $\mathcal{P}(y)$, over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However, , we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore, , we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Through, comprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.",20,7.01015228426396,9.85
195,"['Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.', 'Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.', 'In this work, we address the problem of learning an agents action space purely from visual observation.', ""We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content."", 'We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.', 'We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).', 'We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.', 'When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.', 'Project website: https://daniilidis-group.github.io/learned_action_spaces']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.15789473056793213, 0.1395348757505417, 0.307692289352417, 0.2790697515010834, 0.2666666507720947, 0.1621621549129486, 0.1904761791229248, 0.10526315122842789, 0.0]",SylPMnR9Ym,"[""We learn a representation of an agent's action space from pure visual observations. We use a recurrent latent variable approach with a novel composability loss."", 'Proposes a compositional latent-variable model to learn models that predict what will happen next in scenarios where action-labels are not available in abundance.', 'A variational IB based approach to learn action representations directly from videos of actions being taken, achieving better efficiency of subsequent learning methods while requiring lesser amount of action label videos.', 'This paper proposes an approach to video prediction which autonomously finds an action space encoding differences between subsequent frames']","['intelligent agent learn represent action space agent simply observing act ', 'representation help agent quickly learn predict effect action environment plan complex action sequence ', 'work  address problem learning agent  action space purely visual observation ', 'use stochastic video prediction learn latent variable capture scene dynamic minimally sensitive scene static content ', 'introduce loss term encourages network capture composability visual sequence show lead representation disentangle structure action ', 'call full model composable action representation composable learned action space predictor  clasp  ', 'show applicability method synthetic setting potential capture action space complex  realistic visual setting ', 'used semisupervised setting  learned representation perform comparably existing fully supervised method task actionconditioned video prediction planning learned action space  requiring order magnitude fewer action label ', 'project website  http  daniilidisgroupgithubiolearnedactionspaces']","Intelligent agents can learn to represent the action spaces of other agents simply by observing them act., Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences., In this work, we address the problem of learning an agents action space purely from visual observation., We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content., We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions., We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP)., We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings., When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels., Project website: https://daniilidis-group.github.io/learned_action_spaces",13,5.962765957446808,14.461538461538462
196,"['When autonomous agents interact in the same environment, they must often cooperate to achieve their goals.', 'One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it.', 'However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement.', 'Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols.', 'More general methods usually require human input or domain-specific data, and so do not scale.', 'To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning.', 'Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven.', 'We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory.', 'Additionally, we investigate how the physical location of agents influences negotiation outcomes.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.12903225421905518, 0.11428570747375488, 0.3125, 0.13793103396892548, 0.0, 0.23529411852359772, 0.0624999962747097, 0.08888888359069824, 0.14814814925193787]",HJG0ojCcFm,"['Reinforcement learning can be used to train agents to negotiate team formation across many negotiation protocols', 'This paper studies deep multi-agent RL in settings where all of the agents must cooperate to accomplish a task (e.g., search and rescue, multi-player video games), and uses simple cooperative weighted voting games to study the efficacy of deep RL and to compare solutions found by deep RL to a fair solution.', 'A reinforcement learning approach for negotiating coalitions in cooperative game theory settings that can be used in cases where unlimited training simulations are available.']","['autonomous agent interact environment  must often cooperate achieve goal ', 'one way agent cooperate effectively form team  make binding agreement joint plan  execute ', 'however  agent selfinterested  gain team formation must allocated appropriately incentivize agreement ', 'various approach multiagent negotiation proposed  typically work particular negotiation protocol ', 'general method usually require human input domainspecific data  scale ', 'address  propose framework training agent negotiate form team using deep reinforcement learning ', 'importantly  method make assumption specific negotiation protocol  instead completely experience driven ', 'evaluate approach nonspatial spatially extended teamformation negotiation environment  demonstrating agent beat handcrafted bot reach negotiation outcome consistent fair solution predicted cooperative game theory ', 'additionally  investigate physical location agent influence negotiation outcome ']","When autonomous agents interact in the same environment, they must often cooperate to achieve their goals., One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it., However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement., Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols., More general methods usually require human input or domain-specific data, and so do not scale., To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning., Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven., We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory., Additionally, we investigate how the physical location of agents influences negotiation outcomes.",21,6.0476190476190474,8.0
197,"['Neural machine translation (NMT) models learn representations containing substantial linguistic information.', 'However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons.', 'We develop unsupervised methods for discovering important neurons in NMT models.', 'Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision.', 'We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena.', 'Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.']","[0, 0, 1, 0, 0, 0]","[0.0, 0.06666666269302368, 0.5454545617103577, 0.12903225421905518, 0.06451612710952759, 0.2142857164144516]",H1z-PsR5KX,"['Unsupervised methods for finding, analyzing, and controlling important neurons in NMT', 'This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons.', 'Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.']","['neural machine translation  nmt  model learn representation containing substantial linguistic information ', 'however  clear information fully distributed attributed individual neuron ', 'develop unsupervised method discovering important neuron nmt model ', 'method rely intuition different model learn similar property  require costly external supervision ', 'show experimentally translation quality depends discovered neuron  find many capture common linguistic phenomenon ', 'finally  show control nmt translation predictable way  modifying activation individual neuron ']","Neural machine translation (NMT) models learn representations containing substantial linguistic information., However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons., We develop unsupervised methods for discovering important neurons in NMT models., Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision., We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena., Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.",11,5.9411764705882355,9.272727272727273
198,"['Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task.', 'Hence, both environment and task specific knowledge are entangled into one framework.', 'However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes.', 'Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units.', 'The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task.', 'The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.']","[0, 0, 0, 1, 0, 0]","[0.1428571343421936, 0.5, 0.12903225421905518, 0.5405405163764954, 0.23529411852359772, 0.20000000298023224]",B1mvVm-C-,"['We propose a DRL framework that disentangles task and environment specific knowledge.', 'The authors propose to decompose reinforcement learning into a PATH function and a GOAL function', 'A modular architecture with the aim of separating environment specific knowledge and task-specific knowledge into different modules, on par with standard A3C across a wide range of tasks.']","['recent stateoftheart reinforcement learning algorithm trained goal excelling one specific task ', 'hence  environment task specific knowledge entangled one framework ', 'however  often scenario environment  eg  physical world  fixed target task change ', 'hence  borrowing idea hierarchical reinforcement learning  propose framework disentangles task environment specific knowledge separating two unit ', 'environmentspecific unit handle move one state target state  taskspecific unit plan next target state given specific task ', 'extensive result simulator indicate method efficiently separate learn two independent unit  also adapt new task efficiently stateoftheart method ']","Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task., Hence, both environment and task specific knowledge are entangled into one framework., However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes., Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units., The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task., The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.",11,5.7251908396946565,10.916666666666666
199,"['Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics.', 'We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.', 'Our method learns the depth and orientation of scene points visible in images.', 'Our model can then predict the structure of a scene from various, previously unseen view points.', 'It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry.', 'We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images.', 'We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet.', 'Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.20512819290161133, 0.5882353186607361, 0.2666666507720947, 0.3030303120613098, 0.307692289352417, 0.22857142984867096, 0.24242423474788666, 0.3125]",BJeem3C9F7,"['pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images', 'Explores explaining scenes with surfels in a neural recognition model, and demonstrate results on image reconstruction, synthesis, and mental shape rotation.', 'Authors introduce a method to create a 3D scene model given a 2D image and a camera pose using a self-superfised model']","['modelling 3d scene 2d image longstanding problem computer vision implication  eg  simulation robotics ', 'propose pix2scene  deep generativebased approach implicitly model geometric property scene image ', 'method learns depth orientation scene point visible image ', 'model predict structure scene various  previously unseen view point ', 'relies bidirectional adversarial learning mechanism generate scene representation latent code  inferring 3d representation underlying scene geometry ', 'showcase novel differentiable renderer train 3d model endtoend fashion  using image ', 'demonstrate generative ability model qualitatively custom dataset shapenet ', 'finally  evaluate effectiveness learned 3d scene representation supporting 3d spatial reasoning ']","Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics., We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images., Our method learns the depth and orientation of scene points visible in images., Our model can then predict the structure of a scene from various, previously unseen view points., It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry., We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images., We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet., Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.",15,5.5625,9.6
200,"['Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning.', 'Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any.', 'Despite this, how to accurately learn generic relation representations from word representations remains unclear.', 'We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.', 'We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction.', 'Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.']","[0, 0, 0, 1, 0, 0]","[0.48275861144065857, 0.1538461446762085, 0.22727271914482117, 0.7843137383460999, 0.30434781312942505, 0.28070175647735596]",r1e3WW5aTX,"['Identifying the relations that connect words is important for various NLP tasks. We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.', 'This paper presents a novel method for representing lexical relations as vectors using just pre-trained word embeddings and a novel loss function operating over pairs of word pairs.', 'A novel solution to the relation compositon problem when you already have pre trained word/entity embeddings and are interested only in learning to compose relation representations.']","['identifying relation connect word important step towards understanding human language useful various nlp task knowledge base completion analogical reasoning ', 'simple unsupervised operator vector offset twoword embeddings shown recover specific relationship word  ', 'despite  accurately learn generic relation representation word representation remains unclear ', 'model relation representation supervised learning problem learn parametrised operator map pretrained word embeddings relation representation ', 'propose method learning relation representation using feedforward neural network performs relation prediction ', 'evaluation two benchmark datasets reveal penultimate layer trained neural networkbased relational predictor act good representation relation word ']","Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning., Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any., Despite this, how to accurately learn generic relation representations from word representations remains unclear., We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations., We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction., Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.",8,6.33587786259542,16.375
201,"['Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction.', 'However, optimizing RNNs is known to be harder compared to feed-forward neural networks.', 'A number of techniques have been proposed in literature to address this problem.', 'In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal.', 'Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.', 'In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust.', 'We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout.', 'We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2.', 'We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.09090908616781235, 0.10256409645080566, 0.09999999403953552, 0.17777776718139648, 0.8679245114326477, 0.17777776718139648, 0.2857142686843872, 0.08510638028383255, 0.07999999821186066]",SJyVzQ-C-,"['We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.', 'Presents Fraternal dropout as an improvement over Expectation-linear dropout in terms of convergence, and demonstrates the utility of Fraternal dropout on a number of tasks and datasets.']","['recurrent neural network  rnns  important class architecture among neural network useful language modeling sequential prediction ', 'however  optimizing rnns known harder compared feedforward neural network ', 'number technique proposed literature address problem ', 'paper propose simple technique called fraternal dropout take advantage dropout achieve goal ', 'specifically  propose train two identical copy rnn  share parameter  different dropout mask minimizing difference  presoftmax  prediction ', 'way regularization encourages representation rnns invariant dropout mask  thus robust ', 'show regularization term upper bounded expectationlinear dropout objective shown address gap due difference train inference phase dropout ', 'evaluate model achieve stateoftheart result sequence modeling task two benchmark datasets  penn treebank wikitext2 ', 'also show approach lead performance improvement significant margin image captioning  microsoft coco  semisupervised  cifar10  task ']","Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction., However, optimizing RNNs is known to be harder compared to feed-forward neural networks., A number of techniques have been proposed in literature to address this problem., In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal., Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions., In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust., We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout., We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2., We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.",12,5.75,15.666666666666666
202,"['We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.', 'Our method specifically targets the space-time representation of physical surfaces from liquid simulations.', 'Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.', 'Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.', 'Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.', 'To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.', 'Our representation makes it possible to rapidly generate the desired implicit surfaces.', 'We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.23529411852359772, 0.2222222238779068, 0.1428571343421936, 0.13636362552642822, 0.15789473056793213, 0.0624999962747097, 0.0, 0.060606054961681366]",HyeGBj09Fm,"['Learning weighting and deformations of space-time data sets for highly efficient approximations of liquid behavior.', 'A neural-network based model is used to interpolate simulations for novel scene conditions from densely registered 4D implicit surfaces for a structured scene.', 'This paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications.', 'This paper introduces a deep learning approach for physical simulation that combines two networks for synthesizing 4D data that represents 3D physical simulations']","['propose novel approach deformationaware neural network learn weighting synthesis dense volumetric deformation field ', 'method specifically target spacetime representation physical surface liquid simulation ', 'liquid exhibit highly complex  nonlinear behavior changing simulation condition different initial condition ', 'algorithm capture complex phenomenon two stage  first neural network computes weighting function set precomputed deformation  second network directly generates deformation field refining surface ', 'key successful training run setting suitable loss function encodes effect deformation  robust calculation corresponding gradient ', 'demonstrate effectiveness approach  showcase method several complex example flowing liquid topology change ', 'representation make possible rapidly generate desired implicit surface ', 'implemented mobile application demonstrate realtime interaction complex liquid effect possible approach ']","We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields., Our method specifically targets the space-time representation of physical surfaces from liquid simulations., Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions., Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface., Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients., To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes., Our representation makes it possible to rapidly generate the desired implicit surfaces., We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.",12,6.121212121212121,13.75
203,"['This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.', 'The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.', 'Thus the network is aware not of the specific color object, but its colorfulness.', 'The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.', 'An additional annotation was done which labeled whether the car shown was red or non-red.  ', 'The networks were evaluated by their performance on the classification task.', 'With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.', 'We further split the test data in red and non-red cars and did a similar evaluation.', 'It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.', 'The limits of these networks are also discussed.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.4848484694957733, 0.06896550953388214, 0.07407406717538834, 0.12121211737394333, 0.0, 0.07999999821186066, 0.24242423474788666, 0.27586206793785095, 0.04999999701976776, 0.0]",BkoCeqgR-,"['We construct and evaluate color invariant neural nets on a novel realistic data set', 'Proposes a method to make neural networks for image recognition color invariant and evaluates it on the cifar 10 dataset.', 'The authors investigate a modified input layer that results in color invariant networks, and show that certain color invariant input layers can improve accuracy for test-images from a different color distribution than the training images.', 'The authors test a CNN on images with color channels modified to be invariant to permutations, with performance not degraded by too much. ']","['empirical paper construct color invariant network evaluates performance realistic data set ', 'paper study simplest possible case color invariance  invariance pixelwise permutation color channel ', 'thus network aware specific color object  colorfulness ', 'data set introduced paper consists image showing crashed car ten class extracted ', 'additional annotation done labeled whether car shown red nonred ', 'network evaluated performance classification task ', 'color annotation altered color ratio training data analyzed generalization capability network unaltered test data ', 'split test data red nonred car similar evaluation ', 'shown paper pixelwise ordering rgbvalues image performs better least similarly small deviation true color ratio ', 'limit network also discussed ']","This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set., The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels., Thus the network is aware not of the specific color object, but its colorfulness., The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted., An additional annotation was done which labeled whether the car shown was red or non-red.  , The networks were evaluated by their performance on the classification task., With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data., We further split the test data in red and non-red cars and did a similar evaluation., It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios., The limits of these networks are also discussed.",11,5.193181818181818,16.0
204,"['Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.', 'For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.', 'In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).\n', ""To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well."", 'Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.', 'Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.']","[0, 0, 1, 0, 0, 0]","[0.11764705181121826, 0.16326530277729034, 0.22641508281230927, 0.12244897335767746, 0.1860465109348297, 0.13333332538604736]",HkNGsseC-,"['We analyze how the degree of overlaps between the receptive fields of a convolutional network affects its expressive power.', 'The paper studies the expressive power provided by ""overlap"" in convolution layers of DNNs by considering linear activations with product pooling.', 'This paper analyzes the expressivity of convolutional arithmetic circuits and shows that an exponentialy large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs.']","['expressive efficiency refers relation two architecture b  whereby function realized b could replicated  exists function realized  replicated b unless size grows significantly larger ', 'example  known deep network exponentially efficient respect shallow network  sense shallow network must grow exponentially large order approximate function represented deep network polynomial size ', 'work  extend study expressive efficiency attribute network connectivity particular effect  overlap  convolutional process  ie  stride convolution smaller filter size  receptive field  ', 'theoretically analyze aspect network design  focus wellestablished surrogate convnets called convolutional arithmetic circuit  convacs   demonstrate empirically result hold standard convnets well ', 'specifically  analysis show overlapping local receptive field  broadly denser connectivity  result exponential increase expressive capacity neural network ', 'moreover  denser connectivity increase expressive capacity  show common type modern architecture already exhibit exponential increase expressivity  without relying fullyconnected layer ']","Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger., For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size., In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).
, To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well., Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks., Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.",22,5.691244239631336,9.863636363636363
205,"['We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.', 'Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.', 'The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.', 'By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.', 'For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.', 'In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.', 'In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.', 'Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.6829268336296082, 0.21052631735801697, 0.1904761791229248, 0.08163265138864517, 0.052631575614213943, 0.09999999403953552, 0.04347825422883034, 0.04651162400841713]",HylTXn0qYX,"['A theoretical algorithm for testing local optimality and extracting descent directions at nondifferentiable points of empirical risks of one-hidden-layer ReLU networks.', 'Proposes an algorithm to check whether a given point is a generalized second-order stationary point.', 'A theoretical algorithm, involving solving convex and non-convex quadratic programs, for checking local optimality and escaping saddles when training two-layer ReLU networks.', 'Author proposes a method to check if a point is a stationary point or not and then classify stationary points as either local min or second-order stationary']","['provide theoretical algorithm checking local optimality escaping saddle nondifferentiable point empirical risk twolayer relu network ', 'algorithm receives parameter value return  local minimum  secondorder stationary point  strict descent direction ', 'presence data point nondifferentiability relu divide parameter space 2m region  make analysis difficult ', 'exploiting polyhedral geometry  reduce total computation one convex quadratic program  qp  hidden node      equality test  one   nonconvex qp ', 'last qp  show specific problem solved efficiently  spite nonconvexity ', 'benign case  solve one equality constrained qp  prove projected gradient descent solves exponentially fast ', 'bad case  solve inequality constrained qps  prove time complexity exponential number inequality constraint ', 'experiment show either benign case bad case inequality constraint occurs  implying algorithm efficient case ']","We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks., Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction., The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult., By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP., For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity., In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast., In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints., Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.",21,5.3121693121693125,9.0
206,"['We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  ', 'Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  ', 'That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  ', 'We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  ', 'On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).']","[0, 0, 0, 0, 1]","[0.14814814925193787, 0.1904761791229248, 0.1875, 0.05714285373687744, 0.2222222238779068]",BkTQ8UckG,"['A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval.', 'Learning joint embedding of sentences and images using triplet loss that is applied to hardest negatives instead of averaging over all triplets']","['present new technique learning visualsemantic embeddings crossmodal retrieval ', 'inspired use hard negative structured prediction  ranking loss function used retrieval  introduce simple change common loss function used learn multimodal embeddings ', ' combined finetuning use augmented data  yield significant gain retrieval performance ', 'showcase approach  dubbed vse  mscoco flickr30k datasets  using ablation study comparison existing method ', 'mscoco approach outperforms stateoftheart method 88  caption retrieval  113  image retrieval  based r  1  ']","We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  , Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  , That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  , We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  , On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).",13,5.858585858585859,7.615384615384615
207,"['We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.', 'DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks.', 'It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem.', 'We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively.', 'DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations.', 'In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.']","[1, 0, 0, 0, 0, 0]","[0.3636363446712494, 0.1818181723356247, 0.12903225421905518, 0.10256409645080566, 0.0714285671710968, 0.11764705181121826]",B1D6ty-A-,"['We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders.', 'Alternating minimization framework for training autoencoder and encoder-decoder networks', 'The authors explore an alternating optimization approach for training Auto Encoders, treating each layer as a generalized linear model, and suggest using the stochastic normalized GD as the minimization algorithm in each phase.']","['present dante  novel method training neural network  particular autoencoders  using alternating minimization principle ', 'dante provides distinct perspective lieu traditional gradientbased backpropagation technique commonly used train deep network ', 'utilizes adaptation quasiconvex optimization technique cast autoencoder training biquasiconvex optimization problem ', 'show autoencoder configuration differentiable  eg  sigmoid  nondifferentiable  eg  relu  activation function  perform alternation effectively ', 'dante effortlessly extends network multiple hidden layer varying network configuration ', 'experiment standard datasets  autoencoders trained using proposed method found promising compared trained using traditional backpropagation technique  term training speed  well feature extraction reconstruction performance ']","We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle., DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks., It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem., We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively., DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations., In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.",13,6.623076923076923,8.666666666666666
208,"['We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.', 'By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.', 'We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.', 'Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.']","[1, 0, 0, 0]","[0.3636363744735718, 0.06451612710952759, 0.0624999962747097, 0.07407406717538834]",ByzoVi0cFQ,"['Transfer learning for estimating causal effects using neural networks.', 'Develops algorithms to estimate conditional average treatment effect by auxiliary dataset in different environments, both with and without base learner.', 'The authors propose methods to address a novel task of transfer learning for estimating the CATE function, and evaluate them using a synthetic setting and a real-world experimental dataset.', 'Using neural network regression and comparing transfer learning frameworks to estimate a conditional average treatment effect under string ignorability assumptions']","['develop new algorithm estimating heterogeneous treatment effect  combining recent development transfer learning neural network insight causal inference literature ', 'taking advantage transfer learning  able efficiently use different data source related underlying causal mechanism ', 'compare algorithm extant literature using extensive simulation study based largescale voter persuasion experiment mnist database ', 'method perform order magnitude better existing benchmark using fraction data ']","We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature., By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms., We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database., Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.",6,5.9021739130434785,15.333333333333334
209,"['Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or ""motifs"", are thought to be building blocks of neural representations and information processing.', 'We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology.', 'Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction.', 'Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used.', 'An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance.', 'In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.']","[0, 1, 0, 0, 0, 0]","[0.0, 0.25, 0.12903225421905518, 0.0, 0.0714285671710968, 0.0]",SkloDjAqYm,"['We present LeMoNADe, an end-to-end learned motif detection method directly operating on calcium imaging videos.', 'This paper proposes a VAE-style model for identifying motifs from calcium imaging videos, relying on Bernouli variables and requires Gumbel-softmax trick for inference.']","['neuronal assembly  loosely defined subset neuron reoccurring spatiotemporally coordinated activation pattern   motif   thought building block neural representation information processing ', 'propose lemonade  new exploratory data analysis method facilitates hunting motif calcium imaging video  dominant microscopic functional imaging modality neurophysiology ', 'nonparametric method extract motif directly video  bypassing difficult intermediate step spike extraction ', 'technique augments variational autoencoders discrete stochastic node  show detail differentiable reparametrization relaxation used ', 'evaluation simulated data  available ground truth  reveals excellent quantitative performance ', 'real video data acquired brain slice  ground truth available  lemonade uncovers nontrivial candidate motif help generate hypothesis focused biological investigation ']","Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or ""motifs"", are thought to be building blocks of neural representations and information processing., We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology., Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction., Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used., An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance., In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.",17,6.522058823529412,8.0
210,"['A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning.', 'However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.\n', 'In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set.', 'Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks.', 'For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime.', 'Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set.', 'The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills.', 'The videos related to our experiments are available at: https://sites.google.com/view/deepdj']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.3499999940395355, 0.24242423474788666, 0.20512819290161133, 0.10810810327529907, 0.1702127605676651, 0.1428571343421936, 0.11428570747375488, 0.06451612710952759]",rkxkHnA5tX,"['We propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor.', 'Contributes a MAML based algorithm to imitation learning which automatically determines if provided demonstrations are ""suitable"".', 'A method for doing imitation learning from a set of demonstrations that includes useless behavior, which selects the useful demonstrations by their provided performance gains at the meta-training time.']","['noisy diverse demonstration set may hinder performance agent aiming acquire certain skill via imitation learning ', 'however  stateoftheart imitation learning algorithm often assume optimality given demonstration set ', 'paper  address optimal assumption learning suitable demonstration given set ', 'suitability demonstration estimated whether imitating produce desirable outcome achieving goal task ', 'efficient demonstration suitability assessment  learning agent capable imitating demonstration quick possible  share similar spirit fast adaptation metalearning regime ', 'framework  thus built top modelagnostic metalearning  evaluates desirable imitated outcome  adaptation demonstration set ', 'resulting assessment hence enable u select suitable demonstration subset acquiring better imitated skill ', 'video related experiment available  http  sitesgooglecomviewdeepdj']","A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning., However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.
, In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set., Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks., For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime., Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set., The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills., The videos related to our experiments are available at: https://sites.google.com/view/deepdj",15,6.076923076923077,10.4
211,"['We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions.', 'We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph.', 'We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young.', 'We preserve the dependency structure between the labels with a given causal graph.', 'We devise a two-stage procedure for learning a CiGM over the labels and the image.', 'First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels.', 'Later, we combine this with a conditional GAN to generate images conditioned on the binary labels.', 'We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN.', 'We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels.', 'The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image.', 'We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.40909090638160706, 0.1702127605676651, 0.1860465109348297, 0.1111111044883728, 0.1621621549129486, 0.08695651590824127, 0.09999999403953552, 0.3529411852359772, 0.1428571343421936, 0.1463414579629898, 0.31372547149658203]",BJE-4xW0W,"['We introduce causal implicit generative models, which can sample from conditional and interventional distributions and also propose two new conditional GANs which we use for training them.', 'A method of combining a casual graph, describing the dependency structure of labels with two conditional GAN architechtures that generate images conditioning on the binary label', 'The authors address the issue of learning a causal model between image variables and the image itself from observational data, when given a causal structure between image labels.']","['introduce causal implicit generative model  cigms   model allow sampling true observational also true interventional distribution ', 'show adversarial training used learn cigm  generator architecture structured based given causal graph ', 'consider application conditional interventional sampling face image binary feature label  mustache  young ', 'preserve dependency structure label given causal graph ', 'devise twostage procedure learning cigm label image ', 'first train cigm binary label using wasserstein gan generator neural network consistent causal graph label ', 'later  combine conditional gan generate image conditioned binary label ', 'propose two new conditional gan architecture  causalgan causalbegan ', 'show optimal generator causalgan  given label  sample image distribution conditioned label ', 'conditional gan combined trained cigm label cigm label generated image ', 'show proposed architecture used sample observational interventional image distribution  even intervention naturally occur dataset ']","We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions., We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph., We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young., We preserve the dependency structure between the labels with a given causal graph., We devise a two-stage procedure for learning a CiGM over the labels and the image., First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels., Later, we combine this with a conditional GAN to generate images conditioned on the binary labels., We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN., We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels., The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image., We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.",18,5.328767123287672,12.166666666666666
212,"['Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.', 'This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive.', 'In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention.', 'Several\n', 'recent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why.', 'In this study, we provide a theoretical justification to this property by viewing\n', 'NCE as a low-rank matrix approximation.', 'Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models.', 'It also uncovers a surprising negative correlation between self-normalization and\n', 'perplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.1111111044883728, 0.0, 0.06451612710952759, 0.0, 0.11764705181121826, 0.07999999821186066, 0.0952380895614624, 0.06666666269302368]",H1-IBSgMz,"['We prove that NCE is self-normalized and demonstrate it on datasets', 'Presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.', 'This paper considers the problem of self-normalizing models and explains the self-normalizing mechanism by interpreting NCE in terms of matrix factorization.']","['selfnormalizing discriminative model approximate normalized probability class without compute partition function ', 'property useful computationallyintensive neural network classifier  cost computing partition function grows linearly number class may become prohibitive ', 'particular  since neural language model may deal million class  selfnormalization property received notable attention ', 'several', 'recent study empirically found language model  trained using noise contrastive estimation  nce   exhibit selfnormalization  could explain ', 'study  provide theoretical justification property viewing', 'nce lowrank matrix approximation ', 'empirical investigation compare nce alternative explicit approach selfnormalizing language model ', 'also uncovers surprising negative correlation selfnormalization', 'perplexity  well regularity observed error may potentially used improving selfnormalization algorithm future ']","Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function., This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive., In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention., Several
, recent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why., In this study, we provide a theoretical justification to this property by viewing
, NCE as a low-rank matrix approximation., Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models., It also uncovers a surprising negative correlation between self-normalization and
, perplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.",18,6.397350993377484,8.38888888888889
213,"['Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.', 'Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.', 'Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?', ""In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach."", ""We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus."", 'Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.', 'We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.']","[0, 0, 0, 0, 0, 1, 0]","[0.11428570747375488, 0.17142856121063232, 0.1428571343421936, 0.0833333283662796, 0.06896550953388214, 0.2666666507720947, 0.05714285373687744]",By5SY2gA-,"['Enriching word embeddings with affect information improves their performance on sentiment prediction tasks.', 'Proposes to use affect lexica to improve word embeddings to outperform the standard Word2vec and Glove.', 'This paper proposes integrating information from a semantic resource quantifying the affect of words into a text-based word embedding algorithm to make language models more reflective of semantic and pragmatic phenomena.', 'This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings.']","['learning word representation large available corpus relies distributional hypothesis word present similar context tend similar meaning ', 'recent work shown word representation learnt manner lack sentiment information  fortunately  leveraged using external knowledge ', 'work address question  affect lexica improve word representation learnt corpus ', 'work  propose technique incorporate affect lexica  capture finegrained information word psycholinguistic emotional orientation  training process word2vec skipgram  word2vec cbow glove method using joint learning approach ', 'use affect score warriner affect lexicon regularize vector representation learnt unlabelled corpus ', 'proposed method outperforms previously proposed method standard task word similarity detection  outlier detection sentiment detection ', 'also demonstrate usefulness approach new task related prediction formality  frustration politeness corporate communication ']","Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings., Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge., Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?, In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach., We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus., Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection., We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.",15,6.15,10.666666666666666
214,"['Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks.', 'Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset.', ""However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained."", 'To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph.', 'Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs.']","[0, 0, 0, 1, 0]","[0.043478257954120636, 0.10810810327529907, 0.1304347813129425, 0.1904761791229248, 0.1702127605676651]",SygxYoC5FX,"['For unsupervised and inductive network embedding, we propose a novel approach to explore most relevant neighbors and preserve previously learnt knowledge of nodes by utilizing bi-attention architecture and introducing global bias, respectively', 'This proposes an extension to GraphSAGE using a global embedding bias matrix in the local aggregating functions and a method to sample interesting nodes.']","['different kind representation learning technique graph shown significant effect downstream machine learning task ', 'recently  order inductively learn representation graph structure unobservable training  general framework sampling aggregating  graphsage  proposed hamilton ying proved efficient transductive method fileds like transfer learning evolving dataset ', 'however  graphsage uncapable selective neighbor sampling lack memory known node trained ', 'address problem  present unsupervised method sample neighborhood information attended cooccurring structure optimizes trainable global bias representation expectation node given graph ', 'experiment show approach outperforms stateoftheart inductive unsupervised method representation learning graph ']","Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks., Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset., However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained., To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph., Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs.",9,6.21969696969697,14.666666666666666
215,"['Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.', 'Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors.', 'We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors.', 'Both theoretical and empirical evidence are provided to support this argument:', '(a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;', '(b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients.', 'We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.']","[0, 0, 1, 0, 0, 0, 0]","[0.10526315122842789, 0.24390242993831635, 0.8181818127632141, 0.06451612710952759, 0.29999998211860657, 0.5106382966041565, 0.2631579041481018]",B1e9csRcFm,"['We argue that the generalization of linear graph embedding is not due to the dimensionality constraint but rather the small norm of embedding vectors.', 'The authors show that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors rather than dimensionality constraints', 'The authors propose a theoretical bound on the generalization performance of learning graph embeddings and argue that the norm of the coordinates determines the success of the learnt representation.']","['learning distributed representation node graph crucial primitive network analysis wide spectrum application ', 'linear graph embedding method learn representation optimizing likelihood positive negative edge constraining dimension embedding vector ', 'argue generalization performance method due dimensionality constraint commonly believed  rather small norm embedding vector ', 'theoretical empirical evidence provided support argument ', '  prove generalization error method bounded limiting norm vector  regardless embedding dimension ', ' b  show generalization performance linear graph embedding method correlated norm embedding vector  small due early stopping sgd vanishing gradient ', 'performed extensive experiment validate analysis showcased importance proper norm regularization practice ']","Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications., Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors., We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors., Both theoretical and empirical evidence are provided to support this argument:, (a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;, (b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients., We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.",10,5.666666666666667,15.9
216,"['Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning.', 'We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step.', 'We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover.', 'Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE.', 'We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers.', 'Code is immediately available.']","[0, 1, 0, 0, 0, 0]","[0.0, 0.23529411852359772, 0.1111111044883728, 0.04444444179534912, 0.10810810327529907, 0.0]",S1fUpoR5FQ,"['Mix plain SGD and momentum (or do something similar with Adam) for great profit.', 'The paper proposes simple modifications to SGD and Adam, called QH-variants, that can recover the parent method and a host of other optimization tricks.', 'A variant of classical momentum which takes a weighted average of momentum and gradient update, and an evaluation of its relationships between other momentum based optimization schemes.']","['momentumbased acceleration stochastic gradient descent  sgd  widely used deep learning ', 'propose quasihyperbolic momentum algorithm  qhm  extremely simple alteration momentum sgd  averaging plain sgd step momentum step ', 'describe numerous connection identity algorithm  characterize set twostate optimization algorithm qhm recover ', 'finally  propose qh variant adam called qhadam  empirically demonstrate algorithm lead significantly improved training variety setting  including new stateoftheart result wmt16 ende ', 'hope empirical result  combined conceptual practical simplicity qhm qhadam  spur interest practitioner researcher ', 'code immediately available ']","Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning., We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step., We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover., Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE., We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers., Code is immediately available.",13,5.774193548387097,9.538461538461538
217,"['Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks.', ""A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function."", 'lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads.', 'Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice.', 'Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.', 'In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets.', 'Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns.', 'We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.0, 0.1702127605676651, 0.307692289352417, 0.2380952388048172, 0.2857142686843872, 0.5199999809265137, 0.20408162474632263, 0.05405404791235924]",HkpRBFxRb,"['A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns.', 'Extends the A3C algorithm with lambda returns, and proposes an approach for learning the weights of the returns.', 'The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state.']","['reinforcement learning  rl  model complex behavior policy goaldirected sequential decision making task ', 'hallmark rl algorithm temporal difference  td  learning  value function current state moved towards bootstrapped target estimated using next state value function ', 'lambdareturns define target rl agent weighted combination reward estimated using multiple manystep lookaheads ', 'although mathematically tractable  use exponentially decaying weighting nstep return based target lambdareturns rather adhoc design choice ', 'major contribution propose generalization lambdareturns called confidencebased autodidactic return  car   wherein rl agent learns weighting nstep return endtoend manner ', 'contrast lambdareturns wherein rl agent restricted use exponentially decaying weighting scheme  car allows agent learn decide much want weigh nstep return based target ', 'experiment  addition showing efficacy car  also empirically demonstrate using sophisticated weighted mixture multistep return  like car lambdareturns  considerably outperforms use nstep return ', 'perform experiment asynchronous advantage actor critic  a3c  algorithm atari 2600 domain ']","Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks., A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function., lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads., Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice., Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner., In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets., Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns., We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.",13,5.8669950738916254,15.615384615384615
218,"['Current end-to-end deep learning driving models have two problems: (1) Poor\n', 'generalization ability of unobserved driving environment when diversity of train-\n', 'ing driving dataset is limited (2) Lack of accident explanation ability when driving\n', 'models dont work as expected.', 'To tackle these two problems, rooted on the be-\n', 'lieve that knowledge of associated easy task is benificial for addressing difficult\n', 'task, we proposed a new driving model which is composed of perception module\n', 'for see and think and driving module for behave, and trained it with multi-task\n', 'perception-related basic knowledge and driving knowledge stepwisely.', ' Specifi-\n', 'cally segmentation map and depth map (pixel level understanding of images) were\n', 'considered as what & where and how far knowledge for tackling easier driving-\n', 'related perception problems before generating final control commands for difficult\n', 'driving task.', 'The results of experiments demonstrated the effectiveness of multi-\n', 'task perception knowledge for better generalization and accident explanation abil-\n', 'ity.', 'With our method the average sucess rate of finishing most difficult navigation\n', 'tasks in untrained city of CoRL test surpassed current benchmark method for 15\n', 'percent in trained weather and 20 percent in untrained weathers.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0555555522441864, 0.23529411852359772, 0.3243243098258972, 0.0, 0.0, 0.1621621549129486, 0.6315789222717285, 0.3333333432674408, 0.12903225421905518, 0.1111111044883728, 0.10526315122842789, 0.11428570747375488, 0.060606058686971664, 0.4000000059604645, 0.05405404791235924, 0.10526315122842789, 0.060606058686971664]",B14rPj0qY7,"['we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization  and accident explanation ability.', 'Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module.', 'A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model.', 'The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines']","['current endtoend deep learning driving model two problem   1  poor', 'generalization ability unobserved driving environment diversity train', 'ing driving dataset limited  2  lack accident explanation ability driving', 'model  work expected ', 'tackle two problem  rooted', 'lieve knowledge associated easy task benificial addressing difficult', 'task  proposed new driving model composed perception module', 'see think driving module behave  trained multitask', 'perceptionrelated basic knowledge driving knowledge stepwisely ', 'specifi', 'cally segmentation map depth map  pixel level understanding image ', 'considered  far knowledge tackling easier driving', 'related perception problem generating final control command difficult', 'driving task ', 'result experiment demonstrated effectiveness multi', 'task perception knowledge better generalization accident explanation abil', 'ity ', 'method average sucess rate finishing difficult navigation', 'task untrained city corl test surpassed current benchmark method 15', 'percent trained weather 20 percent untrained weather ']","Current end-to-end deep learning driving models have two problems: (1) Poor
, generalization ability of unobserved driving environment when diversity of train-
, ing driving dataset is limited (2) Lack of accident explanation ability when driving
, models dont work as expected., To tackle these two problems, rooted on the be-
, lieve that knowledge of associated easy task is benificial for addressing difficult
, task, we proposed a new driving model which is composed of perception module
, for see and think and driving module for behave, and trained it with multi-task
, perception-related basic knowledge and driving knowledge stepwisely.,  Specifi-
, cally segmentation map and depth map (pixel level understanding of images) were
, considered as what & where and how far knowledge for tackling easier driving-
, related perception problems before generating final control commands for difficult
, driving task., The results of experiments demonstrated the effectiveness of multi-
, task perception knowledge for better generalization and accident explanation abil-
, ity., With our method the average sucess rate of finishing most difficult navigation
, tasks in untrained city of CoRL test surpassed current benchmark method for 15
, percent in trained weather and 20 percent in untrained weathers.",23,5.802139037433155,8.130434782608695
219,"['Recently there has been a surge of interest in designing graph embedding methods.', 'Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements.', 'In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework  a generic methodology allowing contemporary graph embedding methods to scale to large graphs.', 'MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph.', 'It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns.', 'The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them.', 'We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs.', 'Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification.', 'MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.1666666567325592, 0.19354838132858276, 0.4324324429035187, 0.13793103396892548, 0.22857142984867096, 0.3636363446712494, 0.38461539149284363, 0.09999999403953552, 0.20000000298023224]",HJeKCi0qYX,"['A generic framework to scale existing graph embedding techniques to large graphs.', 'This paper proposes a multi-level embedding framework to be applied on top of existing network embedding methods in order to scale to large scale networks with faster speed.', 'The authors propose a three-stage framework for large-scale graph embedding with improved embedding quality.']","['recently surge interest designing graph embedding method ', '  scale largesized graph million node due computational complexity memory requirement ', 'paper  relax limitation introducing multilevel embedding  mile  framework  generic methodology allowing contemporary graph embedding method scale large graph ', 'mile repeatedly coarsens graph smaller one using hybrid matching technique maintain backbone structure graph ', 'applies existing embedding method coarsest graph refines embeddings original graph novel graph convolution neural network learns ', 'proposed mile framework agnostic underlying graph embedding technique applied many existing graph embedding method without modifying ', 'employ framework several popular graph embedding technique conduct embedding realworld graph ', 'experimental result five largescale datasets demonstrate mile significantly boost speed  order magnitude  graph embedding also often generating embeddings better quality task node classification ', 'mile comfortably scale graph 9 million node 40 million edge  existing method run memory take long compute modern workstation ']","Recently there has been a surge of interest in designing graph embedding methods., Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements., In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework  a generic methodology allowing contemporary graph embedding methods to scale to large graphs., MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph., It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns., The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them., We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs., Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification., MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.",13,5.5,16.76923076923077
220,"['Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries.', 'Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data.', 'With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure.', 'To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.', 'The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner.', 'We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space.', 'The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)']","[1, 0, 0, 0, 0, 0, 0]","[0.23255813121795654, 0.18518517911434174, 0.08695651590824127, 0.1428571343421936, 0.1395348757505417, 0.16129031777381897, 0.1818181723356247]",BJgd7m0xRZ,"['A novel method to increase the resistance of OCSVMs against targeted, integrity attacks by selective nonlinear transformations of data to lower dimensions.', 'The authors propose a defense against attacks on the security of one-class SVM based anomaly detectors', 'This paper explores how random projections can be used to make OCSVM robust to adversarially perturbed training data.']","['anomaly detection discovers regular pattern unlabeled data identifies nonconforming data point  case result malicious attack adversary ', 'learner oneclass support vector machine  ocsvms  successfully anomaly detection  yet performance may degrade significantly presence sophisticated adversary  target algorithm compromising integrity training data ', 'rise use machine learning mission critical daytoday activity error may significant consequence  imperative machine learning system made secure ', 'address  propose defense mechanism based contraction data  test effectiveness using ocsvms ', 'proposed approach introduces layer uncertainty top ocsvm learner  making infeasible adversary guess specific configuration learner ', 'theoretically analyze effect adversarial perturbation separating margin ocsvms provide empirical evidence several benchmark datasets  show carefully contracting data low dimensional space  successfully identify adversarial sample would identifiable original dimensional space ', 'numerical result show proposed method improves ocsvms performance significantly  27  ']","Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries., Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data., With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure., To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs., The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner., We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space., The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)",16,5.777251184834123,13.1875
221,"['In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.', 'In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively.', ""We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation."", 'Previously, the Information Bottleneck (IB) framework (\\cite{Tishby99}) extracts relevant information for a target variable.', 'Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance.', 'We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  ']","[0, 0, 0, 1, 0, 0]","[0.14814814925193787, 0.0, 0.20689654350280762, 0.25, 0.23529411852359772, 0.1395348757505417]",ByED-X-0W,"[""Learning a better neural networks' representation with Information Bottleneck principle"", 'Proposes a learning method based on the information bottleneck framework, where hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y.', 'This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck.']","['paper  present layerwise learning stochastic neural network  snns  informationtheoretic perspective ', 'layer snn  compression relevance defined quantify amount information layer contains input space target space  respectively ', 'jointly optimize compression relevance parameter snn better exploit neural network representation ', 'previously  information bottleneck  ib  framework  cite  tishby99   extract relevant information target variable ', ' propose parametric information bottleneck  pib  neural network utilizing   model parameter explicitly approximate compression relevance ', 'show  compared maximum likelihood estimate  mle  principle  pib    improve generalization neural network classification task   ii  push representation neural network closer optimal informationtheoretical representation faster manner ']","In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective., In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively., We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation., Previously, the Information Bottleneck (IB) framework (\cite{Tishby99}) extracts relevant information for a target variable., Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance., We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  ",14,5.933333333333334,10.714285714285714
222,"['The maximum mean discrepancy (MMD) between two probability measures P\n', 'and Q is a metric that is zero if and only if all moments of the two measures\n', 'are equal, making it an appealing statistic for two-sample tests.', 'Given i.i.d. samples\n', 'from P and Q, Gretton et al. (2012) show that we can construct an unbiased\n', 'estimator for the square of the MMD between the two distributions.', 'If P is a\n', 'distribution of interest and Q is the distribution implied by a generative neural\n', 'network with stochastic inputs, we can use this estimator to train our neural network.\n', 'However, in practice we do not always have i.i.d. samples from our target\n', 'of interest.', 'Data sets often exhibit biasesfor example, under-representation of\n', 'certain demographicsand if we ignore this fact our machine learning algorithms\n', 'will propagate these biases.', 'Alternatively, it may be useful to assume our data has\n', 'been gathered via a biased sample selection mechanism in order to manipulate\n', 'properties of the estimating distribution Q.\n', 'In this paper, we construct an estimator for the MMD between P and Q when we\n', 'only have access to P via some biased sample selection mechanism, and suggest\n', 'methods for estimating this sample selection mechanism when it is not already\n', 'known.', 'We show that this estimator can be used to train generative neural networks\n', 'on a biased data sample, to give a simulator that reverses the effect of that\n', 'bias.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.08695651590824127, 0.23529411852359772, 0.1304347813129425, 0.0, 0.19607843458652496, 0.13333332538604736, 0.10000000149011612, 0.25, 0.20000000298023224, 0.07999999821186066, 0.0, 0.04255318641662598, 0.0, 0.1304347813129425, 0.2916666567325592, 0.09302325546741486, 0.2745097875595093, 0.2857142686843872, 0.2916666567325592, 0.40816324949264526, 0.20408162474632263]",SyuWNMZ0W,"['We propose an estimator for the maximum mean discrepancy, appropriate when a target distribution is only accessible via a biased sample selection procedure, and show that it can be used in a generative network to correct for this bias.', 'Proposes an importance-weighted estimator of the MMD to estimate the MMD between distributions based on samples biased according to a known or estimated unknown scheme.', 'The authors address the problem of sample selection bias in MMD-GANs and propose an estimate of the MMD between two distributions using weighted maximum mean discrepancy.', 'This paper presents a modification of the objective used to train generative networks with an MMD adversary ']","['maximum mean discrepancy  mmd  two probability measure p', 'q metric zero moment two measure', 'equal  making appealing statistic twosample test ', 'given iid  sample', 'p q  gretton et al   2012  show construct unbiased', 'estimator square mmd two distribution ', 'p', 'distribution interest q distribution implied generative neural', 'network stochastic input  use estimator train neural network ', 'however  practice always iid  sample target', 'interest ', 'data set often exhibit biasesfor example  underrepresentation', 'certain demographicsand ignore fact machine learning algorithm', 'propagate bias ', 'alternatively  may useful assume data', 'gathered via biased sample selection mechanism order manipulate', 'property estimating distribution q ', 'paper  construct estimator mmd p q', 'access p via biased sample selection mechanism  suggest', 'method estimating sample selection mechanism already', 'known ', 'show estimator used train generative neural network', 'biased data sample  give simulator revers effect', 'bias ']","The maximum mean discrepancy (MMD) between two probability measures P
, and Q is a metric that is zero if and only if all moments of the two measures
, are equal, making it an appealing statistic for two-sample tests., Given i.i.d. samples
, from P and Q, Gretton et al. (2012) show that we can construct an unbiased
, estimator for the square of the MMD between the two distributions., If P is a
, distribution of interest and Q is the distribution implied by a generative neural
, network with stochastic inputs, we can use this estimator to train our neural network.
, However, in practice we do not always have i.i.d. samples from our target
, of interest., Data sets often exhibit biasesfor example, under-representation of
, certain demographicsand if we ignore this fact our machine learning algorithms
, will propagate these biases., Alternatively, it may be useful to assume our data has
, been gathered via a biased sample selection mechanism in order to manipulate
, properties of the estimating distribution Q.
, In this paper, we construct an estimator for the MMD between P and Q when we
, only have access to P via some biased sample selection mechanism, and suggest
, methods for estimating this sample selection mechanism when it is not already
, known., We show that this estimator can be used to train generative neural networks
, on a biased data sample, to give a simulator that reverses the effect of that
, bias.",33,4.906382978723404,6.527777777777778
223,"['We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm.', 'Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive.', 'We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution.', 'We apply our method to a wide range of Atari Arcade Learning Environments.', 'Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.']","[0, 1, 0, 0, 0]","[0.1621621549129486, 0.21052631735801697, 0.17241378128528595, 0.11428570747375488, 0.1395348757505417]",Bk6qQGWRb,"['Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation', 'The authors propose a new algorithm for exploration in Deep RL where they apply Bayesian linear regression with features from the last layer of a DQN network to estimate the Q function for each action.', 'The authors describe how to use Bayesian neural networks with Thompson sampling for efficient exploration in q-learning and propose an approach that outperforms epsilon-greedy exploration approaches.']","['propose bayesian deep qnetwork  bdqn   practical thompson sampling based reinforcement learning  rl  algorithm ', 'thompson sampling allows targeted exploration high dimension posterior sampling usually computationally expensive ', 'address limitation introducing uncertainty output layer network bayesian linear regression  blr  model  trained fast closedform update sample drawn efficiently gaussian distribution ', 'apply method wide range atari arcade learning environment ', 'since bdqn carry efficient exploration  able reach higher reward substantially faster key baseline  ddqn ']","We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm., Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive., We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution., We apply our method to a wide range of Atari Arcade Learning Environments., Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.",9,5.841121495327103,11.88888888888889
224,"['In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN.', 'The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out.', 'Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer.', 'Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning.', 'While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers.', 'By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency.', 'We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.']","[0, 1, 0, 0, 0, 0, 0]","[0.22727271914482117, 0.37288135290145874, 0.21739129722118378, 0.3396226465702057, 0.19999998807907104, 0.10256409645080566, 0.13333332538604736]",B1GHb2RqYX,"['PolyCNN only needs to learn one seed convolutional filter at each layer. This is an efficient variant of traditional CNN, with on-par performance.', 'Attempts at reducing the number of CNN model parameters by using the polynomial transformation of filters to create blow-up the filter responses.', 'The authors propose a weight sharing architecture for reducing the number of convolutional neural network parameters with seed filters']","['work  propose polynomial convolutional neural network  polycnn   new design weightlearning efficient variant traditional cnn ', 'biggest advantage polycnn convolutional layer  one convolutional filter needed learning weight  call seed filter  convolutional filter polynomial transformation seed filter  termed early fanout ', 'alternatively  also perform late fanout seed filter response create number response map needed input next layer ', 'early late fanout allow polycnn learn one convolutional filter layer  dramatically reduce model complexity saving 10x 50x parameter learning ', 'efficient training testing  polycnn suffer performance due nonlinear polynomial expansion translates richer representational power within convolutional layer ', 'allowing direct control model complexity  polycnn provides flexible tradeoff performance efficiency ', 'verified onpar performance proposed polycnn standard cnn several visual datasets  mnist  cifar10  svhn  imagenet ']","In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN., The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out., Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer., Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning., While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers., By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency., We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.",21,5.458128078817734,9.666666666666666
225,"['Detecting the emergence of abrupt property changes in time series is a challenging problem.', 'Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches.', 'However, selecting kernels is non-trivial in practice.', 'Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms.', 'In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model.', 'With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications.', 'The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.']","[0, 0, 0, 0, 1, 0, 0]","[0.2083333283662796, 0.14814814925193787, 0.0, 0.17543859779834747, 0.8852459192276001, 0.18518517911434174, 0.038461532443761826]",r1GbfhRqF7,"['In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. ', 'Describes a novel approach to optimising the choice of kernel towards increased testing power and shown to offer improvements over alternatives.']","['detecting emergence abrupt property change time series challenging problem ', 'kernel twosample test studied task make fewer assumption distribution traditional parametric approach ', 'however  selecting kernel nontrivial practice ', 'although kernel selection twosample test studied  insufficient sample change point detection problem hinder success developed kernel selection algorithm ', 'paper  propose klcpd  novel kernel learning framework time series cpd optimizes lower bound test power via auxiliary generative model ', 'deep kernel parameterization  klcpd endows kernel twosample test datadriven kernel detect different type changepoints realworld application ', 'proposed approach significantly outperformed stateoftheart method comparative evaluation benchmark datasets simulation study ']","Detecting the emergence of abrupt property changes in time series is a challenging problem., Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches., However, selecting kernels is non-trivial in practice., Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms., In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model., With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications., The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.",12,6.161764705882353,11.333333333333334
226,"['Theories in cognitive psychology postulate that humans use similarity as a basis\n', 'for object categorization.', 'However, work in image classification generally as-\n', 'sumes disjoint and equally dissimilar classes to achieve super-human levels of\n', 'performance on certain datasets.', 'In our work, we adapt notions of similarity using\n', 'weak labels over multiple hierarchical levels to boost classification performance.\n', 'Instead of pitting clustering directly against classification, we use a warm-start\n', 'based evaluation to explicitly provide value to a clustering representation by its\n', 'ability to aid classification.', 'We evaluate on CIFAR10 and a fine-grained classifi-\n', 'cation dataset to show improvements in performance with the procedural addition\n', 'of intermediate losses and weak labels based on multiple hierarchy levels.', 'Further-\n', 'more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\n', 'tion with intermediate losses outperforms a classification baseline by over 17% on\n', 'a subset of Birdsnap dataset.', 'Finally, we show improvement over AlexNet trained\n', 'using ImageNet pre-trained weights as initializations which further supports our \n', 'claim of the importance of similarity.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.11764705181121826, 0.0952380895614624, 0.0, 0.10526315122842789, 0.380952388048172, 0.0, 0.0952380895614624, 0.2857142686843872, 0.0, 0.0952380895614624, 0.1904761791229248, 0.1818181723356247, 0.09090908616781235, 0.0, 0.0, 0.09999999403953552, 0.0]",r1-4BLaQz,"['Cluster before you classify; using weak labels to improve classification ', 'Proposes using a clustering based loss function at multiple levels of a deepnet as well as using hierarchical structure of the label space to train better representations.', 'This paper uses hierarchical label information to impose additional losses on intermediate representations in neural network training.']","['theory cognitive psychology postulate human use similarity basis', 'object categorization ', 'however  work image classification generally', 'sumes disjoint equally dissimilar class achieve superhuman level', 'performance certain datasets ', 'work  adapt notion similarity using', 'weak label multiple hierarchical level boost classification performance ', 'instead pitting clustering directly classification  use warmstart', 'based evaluation explicitly provide value clustering representation', 'ability aid classification ', 'evaluate cifar10 finegrained classifi', 'cation dataset show improvement performance procedural addition', 'intermediate loss weak label based multiple hierarchy level ', '', ' show pretraining alexnet hierarchical weak label conjunc', 'tion intermediate loss outperforms classification baseline 17 ', 'subset birdsnap dataset ', 'finally  show improvement alexnet trained', 'using imagenet pretrained weight initialization support', 'claim importance similarity ']","Theories in cognitive psychology postulate that humans use similarity as a basis
, for object categorization., However, work in image classification generally as-
, sumes disjoint and equally dissimilar classes to achieve super-human levels of
, performance on certain datasets., In our work, we adapt notions of similarity using
, weak labels over multiple hierarchical levels to boost classification performance.
, Instead of pitting clustering directly against classification, we use a warm-start
, based evaluation to explicitly provide value to a clustering representation by its
, ability to aid classification., We evaluate on CIFAR10 and a fine-grained classifi-
, cation dataset to show improvements in performance with the procedural addition
, of intermediate losses and weak labels based on multiple hierarchy levels., Further-
, more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-
, tion with intermediate losses outperforms a classification baseline by over 17% on
, a subset of Birdsnap dataset., Finally, we show improvement over AlexNet trained
, using ImageNet pre-trained weights as initializations which further supports our 
, claim of the importance of similarity.",25,6.078313253012048,6.64
227,"['Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels.', 'However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks.', 'In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state.', 'We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malm) first-person navigation benchmarks.']","[0, 0, 1, 0]","[0.25, 0.11320754140615463, 0.4482758641242981, 0.3606557250022888]",BkCV_W-AZ,"['Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.', 'This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL and an algorithm called ARM which can deal with partial observability better.', 'The paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization and claims that the approach can deal with the partial observable domain better than standard methods.']","['deep reinforcement learning algorithm estimate state stateaction value function shown effective variety challenging domain  including learning control strategy raw image pixel ', 'however  algorithm estimate state stateaction value function typically assume fully observed state must compensate partial nonmarkovian observation using finitelength framehistory observation recurrent network ', 'work  propose new deep reinforcement learning algorithm based counterfactual regret minimization iteratively update approximation cumulative clipped advantage function robust partially observed state ', 'demonstrate several partially observed reinforcement learning task  new class algorithm substantially outperform strong baseline method  pong singleframe observation  challenging doom  vizdoom  minecraft  malm  firstperson navigation benchmark ']","Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels., However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks., In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state., We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malm) first-person navigation benchmarks.",9,6.5,14.88888888888889
228,"['Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks.', 'Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed.', 'To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL).', 'TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers.', 'Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers.', 'Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single tasks performance, particularly favourable in scenarios where some of the tasks have insufficient data.']","[0, 0, 1, 0, 0, 0]","[0.1818181723356247, 0.1249999925494194, 0.3125, 0.0952380895614624, 0.0, 0.10810810327529907]",BJxmXhRcK7,"['a deep multi-task learning model adapting tensor ring representation', 'A variant of tensor ring formulation for multi-task learning by sharing some of the TT cores for learning ""common task"" while learning individual TT cores for each separate task']","['recent deep multitask learning  mtl  witnessed success alleviating data scarcity task utilizing domainspecific knowledge related task ', 'nonetheless  several major issue deep mtl  including effectiveness sharing mechanism  efficiency model complexity flexibility network architecture  still remain largely unaddressed ', 'end  propose novel generalized latentsubspace based knowledge sharing mechanism linking taskspecific model  namely tensor ring multitask learning  trmtl  ', 'trmtl highly compact representation  effective transferring taskinvariant knowledge super flexible learning taskspecific feature  successfully mitigating dilemma negativetransfer lower layer undertransfer higher layer ', 'trmtl  feasible task heterogenous input data dimensionality distinct feature size different hidden layer ', 'experiment variety datasets demonstrate model capable significantly improving single task  performance  particularly favourable scenario task insufficient data ']","Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks., Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed., To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL)., TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers., Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers., Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single tasks performance, particularly favourable in scenarios where some of the tasks have insufficient data.",16,6.210843373493976,10.375
229,"['Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.', 'Each function models the distribution of the output given an input, conditioned on the context.', 'NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.', 'Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.', 'We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.', 'We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.']","[1, 0, 0, 0, 0, 0]","[0.2857142686843872, 0.06451612710952759, 0.15686273574829102, 0.14999999105930328, 0.25641024112701416, 0.09999999403953552]",SkE6PjC9KX,"['A model for regression that learns conditional distributions of a stochastic process, by incorporating attention into Neural Processes.', 'Proposes to resolve the issue of underfitting in the neural process method by adding an attention mechanism to the deterministic path.', 'An extension to the framework of Neural Processes that adds an attention-based conditioning mechanism, allowing the model to better capture dependencies in the conditioning set.', 'The authors extend neural processes by incorporating self-attention for enriching the features of the context points and cross-attention for producing a query-specific representation. They resolve the underfitting problem of NPs and show ANPs to converge better and faster than NPs.']","['neural process  np   garnelo et al  2018  approach regression learning map context set observed inputoutput pair distribution regression function ', 'function model distribution output given input  conditioned context ', 'np benefit fitting observed data efficiently linear complexity number context inputoutput pair  learn wide family conditional distribution  learn predictive distribution conditioned context set arbitrary size ', 'nonetheless  show np suffer fundamental drawback underfitting  giving inaccurate prediction input observed data condition ', 'address issue incorporating attention np  allowing input location attend relevant context point prediction ', 'show greatly improves accuracy prediction  result noticeably faster training  expands range function modelled ']","Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions., Each function models the distribution of the output given an input, conditioned on the context., NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size., Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on., We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction., We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.",14,5.578947368421052,10.857142857142858
230,"['Deconvolutional layers have been widely used in a variety of deep\n', 'models for up-sampling, including encoder-decoder networks for\n', 'semantic segmentation and deep generative models for unsupervised\n', 'learning.', 'One of the key limitations of deconvolutional operations\n', 'is that they result in the so-called checkerboard problem.', 'This is\n', 'caused by the fact that no direct relationship exists among adjacent\n', 'pixels on the output feature map.', 'To address this problem, we\n', 'propose the pixel deconvolutional layer (PixelDCL) to establish\n', 'direct relationships among adjacent pixels on the up-sampled feature\n', 'map.', 'Our method is based on a fresh interpretation of the regular\n', 'deconvolution operation.', 'The resulting PixelDCL can be used to\n', 'replace any deconvolutional layer in a plug-and-play manner without\n', 'compromising the fully trainable capabilities of original models.\n', 'The proposed PixelDCL may result in slight decrease in efficiency,\n', 'but this can be overcome by an implementation trick.', 'Experimental\n', 'results on semantic segmentation demonstrate that PixelDCL can\n', 'consider spatial features such as edges and shapes and yields more\n', 'accurate segmentation outputs than deconvolutional layers.', 'When used\n', 'in image generation tasks, our PixelDCL can largely overcome the\n', 'checkerboard problem suffered by regular deconvolution operations.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.1818181723356247, 0.0, 0.0, 0.0, 0.29999998211860657, 0.09090908616781235, 0.11764705181121826, 0.0, 0.10526315122842789, 0.09999999403953552, 0.0, 0.0, 0.19999998807907104, 0.0, 0.09999999403953552, 0.09999999403953552, 0.0, 0.0, 0.0, 0.0952380895614624, 0.3333333432674408]",B1spAqUp-,"['Solve checkerboard problem in Deconvolutional layer by building dependencies between pixels', 'This work proposes pixel deconvolutional layers for convolutional neural networks as a way to alleviate the checkerboard effect.', 'A novel technique to generalize deconvolution operations used in standard CNN architectures, which proposes doing sequential prediction of adjacent pixel features, resulting in more spatially smooth outputs for deconvolution layers.']","['deconvolutional layer widely used variety deep', 'model upsampling  including encoderdecoder network', 'semantic segmentation deep generative model unsupervised', 'learning ', 'one key limitation deconvolutional operation', 'result socalled checkerboard problem ', '', 'caused fact direct relationship exists among adjacent', 'pixel output feature map ', 'address problem ', 'propose pixel deconvolutional layer  pixeldcl  establish', 'direct relationship among adjacent pixel upsampled feature', 'map ', 'method based fresh interpretation regular', 'deconvolution operation ', 'resulting pixeldcl used', 'replace deconvolutional layer plugandplay manner without', 'compromising fully trainable capability original model ', 'proposed pixeldcl may result slight decrease efficiency ', 'overcome implementation trick ', 'experimental', 'result semantic segmentation demonstrate pixeldcl', 'consider spatial feature edge shape yield', 'accurate segmentation output deconvolutional layer ', 'used', 'image generation task  pixeldcl largely overcome', 'checkerboard problem suffered regular deconvolution operation ']","Deconvolutional layers have been widely used in a variety of deep
, models for up-sampling, including encoder-decoder networks for
, semantic segmentation and deep generative models for unsupervised
, learning., One of the key limitations of deconvolutional operations
, is that they result in the so-called checkerboard problem., This is
, caused by the fact that no direct relationship exists among adjacent
, pixels on the output feature map., To address this problem, we
, propose the pixel deconvolutional layer (PixelDCL) to establish
, direct relationships among adjacent pixels on the up-sampled feature
, map., Our method is based on a fresh interpretation of the regular
, deconvolution operation., The resulting PixelDCL can be used to
, replace any deconvolutional layer in a plug-and-play manner without
, compromising the fully trainable capabilities of original models.
, The proposed PixelDCL may result in slight decrease in efficiency,
, but this can be overcome by an implementation trick., Experimental
, results on semantic segmentation demonstrate that PixelDCL can
, consider spatial features such as edges and shapes and yields more
, accurate segmentation outputs than deconvolutional layers., When used
, in image generation tasks, our PixelDCL can largely overcome the
, checkerboard problem suffered by regular deconvolution operations.",30,6.042780748663102,6.233333333333333
231,"['In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem.', 'To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived.', 'After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0).', 'The method does not require fine-tuning after quantization.', 'Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).']","[1, 0, 0, 0, 0]","[0.3243243098258972, 0.19512194395065308, 0.0952380895614624, 0.0, 0.06896550953388214]",ry-TW-WAb,"['We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.', 'Proposes to use a mixture of continuous spike propto 1/abs as prior for a Bayesian neural network and demonstrates the good performance with relatively sparsified convnets for minist and cifar-10.', 'This paper presents a variational Bayesian approach for quantising neural network weights to ternary values post-training in a principled way.']","['paper  preparation neural network pruning fewbit quantization formulated variational inference problem ', 'end  quantizing prior lead multimodal  sparse posterior distribution weight  introduced differentiable kullbackleibler divergence approximation prior derived ', 'training variational network quantization  weight replaced deterministic quantization value small negligible loss task accuracy  including pruning setting weight 0  ', 'method require finetuning quantization ', 'result shown ternary quantization lenet5  mnist  densenet  cifar10  ']","In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem., To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived., After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0)., The method does not require fine-tuning after quantization., Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).",10,6.03030303030303,9.9
232,"['Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.', 'Extensive research work have been conducted on DNN model compression or pruning.', 'However, most of the previous work took heuristic approaches.', 'This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.', 'Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.', 'Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.', 'It achieves up to 34 pruning rate for ImageNet dataset and 167 pruning rate for MNIST dataset, significantly higher than those reached by the literature work.', 'Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.', 'The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.05405404791235924, 0.1666666567325592, 0.0952380895614624, 0.20512820780277252, 0.19999998807907104, 0.13793103396892548, 0.17142856121063232, 0.2142857164144516, 0.1538461446762085]",rygo9iR9F7,"['We implement a DNN weight pruning approach that achieves the highest pruning rates.', 'This paper focuses on weight pruning for neural network compression, achiving 30x compression rate for AlexNet and VGG for ImageNet.', 'A progressive pruning technique which imposes structural sparsity constraint on the weight parameter and rewrites the optimization as an ADMM framework, achieving higher accurancy than projected gradient descent.']","['deep neural network  dnns  although achieving humanlevel performance many domain  large model size hinders broader application edge computing device ', 'extensive research work conducted dnn model compression pruning ', 'however  previous work took heuristic approach ', 'work proposes progressive weight pruning approach based admm  alternating direction method multiplier   powerful technique deal nonconvex optimization problem potentially combinatorial constraint ', 'motivated dynamic programming  proposed method reach extremely high pruning rate using partial pruning moderate pruning rate ', 'therefore  resolve accuracy degradation long convergence time problem pursuing extremely high pruning ratio ', 'achieves 34 pruning rate imagenet dataset 167 pruning rate mnist dataset  significantly higher reached literature work ', 'number epoch  proposed method also achieves faster convergence higher compression rate ', 'code pruned dnn model released anonymous link bitly2zxdlss ']","Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices., Extensive research work have been conducted on DNN model compression or pruning., However, most of the previous work took heuristic approaches., This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints., Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates., Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios., It achieves up to 34 pruning rate for ImageNet dataset and 167 pruning rate for MNIST dataset, significantly higher than those reached by the literature work., Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates., The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.",16,6.023809523809524,10.5
233,"['In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.', 'The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network.', 'The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network.', 'This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate.', 'We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.\n']","[1, 0, 0, 0, 0]","[0.8260869383811951, 0.21052631735801697, 0.3529411852359772, 0.35555556416511536, 0.21276594698429108]",r1efr3C9Ym,"['This paper presents a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.', 'Proposes a framework for making predictions on sparse, irregularly sampled time-series data using an interpolation module that models the missing values in using smooth interpolation, non-smooth interpolation, and intensity. ', 'Solves the problem of supervised learning with sparse and irregularly sampled multivariate time series using a semi-parametric interpolation network followed by a prediction network.']","['paper  present new deep learning architecture addressing problem supervised learning sparse irregularly sampled multivariate time series ', 'architecture based use semiparametric interpolation network followed application prediction network ', 'interpolation network allows information shared across multiple dimension multivariate time series interpolation stage  standard deep learning model used prediction network ', 'work motivated analysis physiological time series data electronic health record  sparse  irregularly sampled  multivariate ', 'investigate performance architecture classification regression task  showing approach outperforms range baseline recently proposed model ']","In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series., The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network., The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network., This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate., We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.
",11,5.8203125,11.636363636363637
234,"['We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.', 'We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.']","[0, 1]","[0.21052631735801697, 0.2222222238779068]",rJlpUiAcYX,"['Permutation-invariant loss function for point set prediction.', 'Proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. ', 'This paper introduces a novel distance function between point sets, applies two other permutation distances in an end-to-end object detection task, and shows that in two dimensions all local minima of the holographic loss are global minima.', 'Proposes permutation invariant loss functions which depend on the distance of sets.']","['introduce analytic distance function moderately sized point set known cardinality shown desirable property  loss function well regularizer machine learning application ', 'compare novel construction point set distance function show proof concept experiment training neural network endtoend point set prediction task object detection ']","We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications., We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.",3,5.477611940298507,22.333333333333332
235,"['We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.', 'Our method learns to assign graph operations to groups and to allocate those groups to available devices.', 'The grouping and device allocations are learned jointly.', 'The proposed method is trained with policy gradient and requires no human intervention.', 'Experiments with widely-used\n', 'computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.', 'In addition, our approach outperforms placements by human\n', 'experts as well as a previous state-of-the-art placement method based on deep reinforcement learning.', 'Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.6499999761581421, 0.0714285671710968, 0.0, 0.0, 0.0, 0.15789473056793213, 0.0, 0.1428571343421936, 0.0555555522441864]",Hkc-TeZ0W,"['We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices.', 'Proposes to jointly learn groups of operators to colocate and to place learned groups on devices to distribute operations for deep learning via reinforcement learning.', ""The authors purpose a fully connect network to replace the co-location step in an auto-placement method proposed to accelerate a TensorFlow model's runtime."", 'Proposes a device placement algorithm to place operations of tensorflow on devices.']","['introduce hierarchical model efficient placement computational graph onto hardware device  especially heterogeneous environment mixture cpu  gpus  computational device ', 'method learns assign graph operation group allocate group available device ', 'grouping device allocation learned jointly ', 'proposed method trained policy gradient requires human intervention ', 'experiment widelyused', 'computer vision natural language model show algorithm find optimized  nontrivial placement tensorflow computational graph 80000 operation ', 'addition  approach outperforms placement human', 'expert well previous stateoftheart placement method based deep reinforcement learning ', 'method achieves runtime reduction 606  per training step applied model neural machine translation ']","We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices., Our method learns to assign graph operations to groups and to allocate those groups to available devices., The grouping and device allocations are learned jointly., The proposed method is trained with policy gradient and requires no human intervention., Experiments with widely-used
, computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations., In addition, our approach outperforms placements by human
, experts as well as a previous state-of-the-art placement method based on deep reinforcement learning., Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.",14,6.044444444444444,9.642857142857142
236,"['Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem.', 'We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion.', 'While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself.', 'We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels.', 'Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types.', 'In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry.', 'Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.']","[0, 1, 0, 0, 0, 0, 0]","[0.21739129722118378, 0.5238094925880432, 0.35555556416511536, 0.47999998927116394, 0.19999998807907104, 0.307692289352417, 0.35555556416511536]",SJLlmG-AZ,"['We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion.', 'Proposes to learn the rigid motion group from a latent representation of image sequences without the need for explicit labels and experimentally demonstrates method on sequences of MINST digits and the KITTI dataset.', 'This paper proposes an approach for learning video motion features in an unsupervised manner, using constraints to optimize the neural network to produce features that can be used to regress odometry.']","['motion important signal agent dynamic environment  learning represent motion unlabeled video difficult underconstrained problem ', 'propose model motion based elementary group property transformation use train representation image motion ', 'method estimating motion based pixellevel constraint  use group property constrain abstract representation motion ', 'demonstrate deep neural network trained using method capture motion synthetic 2d sequence realworld sequence vehicle motion  without requiring label ', 'network trained respect constraint implicitly identify image characteristic motion different sequence type ', 'context vehicle motion  method extract information useful localization  tracking  odometry ', 'result demonstrate representation useful learning motion general setting explicit label difficult obtain ']","Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem., We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion., While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself., We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels., Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types., In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry., Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.",13,5.811688311688312,11.846153846153847
237,"['This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.', 'Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases.', 'We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS.', 'Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters.', 'This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity.', 'Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.']","[0, 1, 0, 0, 0, 0]","[0.25, 0.260869562625885, 0.17142856121063232, 0.09302324801683426, 0.1621621549129486, 0.16326530277729034]",BJjBnN9a-,"['This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space.', 'Projecting examples into an RK Hilbert space and performing convolution and filtering into that space.', 'This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases']","['paper introduces concept continuous convolution neural network deep learning application general ', 'rather directly using discretized information  input data first projected highdimensional reproducing kernel hilbert space  rkhs   modeled continuous function using series kernel base ', 'proceed derive closedform solution continuous convolution operation two arbitrary function operating different rkhs ', 'within framework  convolutional filter also take form continuous function  training procedure involves learning rkhs filter projected  alongside weight parameter ', 'result much expressive filter  require spatial discretization benefit property adaptive support nonstationarity ', 'experiment image classification performed  using classical datasets  result indicating proposed continuous convolutional neural network able achieve competitive accuracy rate far fewer parameter faster convergence rate ']","This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general., Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases., We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS., Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters., This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity., Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.",14,6.04320987654321,11.571428571428571
238,"['Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes.', 'Some recent studies suggest a more important role of image textures.', 'We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict.', 'We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies.', ""We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet."", 'This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.']","[0, 0, 0, 0, 0, 1]","[0.16326530277729034, 0.0952380895614624, 0.15094339847564697, 0.2666666507720947, 0.06896550953388214, 0.29999998211860657]",Bygh9j09KX,"['ImageNet-trained CNNs are biased towards object texture (instead of shape like humans). Overcoming this major difference between human and machine vision yields improved detection performance and previously unseen robustness to image distortions.', 'Using image stylizaton to augment training data for ImageNet-trained CNNs to make resulting networks appear more aligned with human judgements', 'This paper studies CNNs like AlexNet, VGG, GoogleNet, and ResNet50, shows these models are biased towards texture when trained on ImageNet, and proposes a new ImageNet dataset.']","['convolutional neural network  cnns  commonly thought recognise object learning increasingly complex representation object shape ', 'recent study suggest important role image texture ', 'put conflicting hypothesis quantitative test evaluating cnns human observer image textureshape cue conflict ', 'show imagenettrained cnns strongly biased towards recognising texture rather shape  stark contrast human behavioural evidence reveals fundamentally different classification strategy ', 'demonstrate standard architecture  resnet50  learns texturebased representation imagenet able learn shapebased representation instead trained stylizedimagenet   stylized version imagenet ', 'provides much better fit human behavioural performance wellcontrolled psychophysical lab setting  nine experiment totalling 48560 psychophysical trial across 97 observer  come number unexpected emergent benefit improved object detection performance previously unseen robustness towards wide range image distortion  highlighting advantage shapebased representation ']","Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes., Some recent studies suggest a more important role of image textures., We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict., We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies., We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet., This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",9,6.362573099415204,19.0
239,"['In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities.', 'Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances.', 'In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs.', 'The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.']","[0, 0, 1, 0]","[0.10526315122842789, 0.31111109256744385, 0.380952388048172, 0.19230768084526062]",ryeHw1vjiQ,"['We evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency.', 'Propose an autoencoder model to learn a representation for speaker verification using short-duration analysis windows.', 'A modified version of the variational autoencoder model that tackles the speaker recognition problem in the context of short-duration segments']","['work  exploited different strategy provide prior knowledge commonly used generative modeling approach aiming obtain speakerdependent low dimensional representation shortduration segment speech data  making use available information speaker identity ', 'namely  convolutional variational autoencoders employed  statistic learned posterior distribution used low dimensional representation fixed length shortduration utterance ', 'order enforce speaker dependency latent layer  introduced variation commonly used prior within variational autoencoders framework  ie  model simultaneously trained reconstruction input along discriminative task performed top latent layer output ', 'effectiveness triplet loss minimization speaker recognition evaluated implicit prior challenging crosslanguage nist sre 2016 setting compared fully supervised unsupervised baseline ']","In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities., Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances., In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs., The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.",10,6.381294964028777,12.636363636363637
240,"['The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models.', 'Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions.', 'In this work, we provide yet another perspective on the IWAE bounds.', 'We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K).', 'In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions.', 'Based on this analysis we develop jackknife variational inference (JVI),\n', 'a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency.', 'Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders.', 'We also report first results on applying JVI to learning variational autoencoders.\n\n', 'Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705181121826, 0.0, 0.0, 0.0, 0.1428571343421936]",HyZoi-WRb,"[""Variational inference is biased, let's debias it."", 'Introduces jackknife variational inference, a method for debiasing Monte Carlo objectives such as the importance weighted auto-encoder.', 'The authors analyze the bias and variance of the IWAE bound and derive a jacknife approach to estimate moments as a way to debias IWAE for finite importance weighted samples.']","['importanceweighted autoencoder  iwae  approach burda et al  defines sequence increasingly tighter bound marginal likelihood latent variable model ', 'recently  cremer et al  reinterpreted iwae bound ordinary variational evidence lower bound  elbo  applied increasingly accurate variational distribution ', 'work  provide yet another perspective iwae bound ', 'interpret iwae bound biased estimator true marginal likelihood bound defined  k  sample show bias order  1k  ', 'theoretical analysis iwae objective derive asymptotic bias variance expression ', 'based analysis develop jackknife variational inference  jvi  ', 'family biasreduced estimator reducing bias   k    m1     given  k retaining computational efficiency ', 'finally  demonstrate jvi lead improved evidence estimate variational autoencoders ', 'also report first result applying jvi learning variational autoencoders ', 'implementation available http  githubcommicrosoftjackknifevariationalinference']","The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models., Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions., In this work, we provide yet another perspective on the IWAE bounds., We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K)., In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions., Based on this analysis we develop jackknife variational inference (JVI),
, a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency., Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders., We also report first results on applying JVI to learning variational autoencoders.

, Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference",13,6.067073170731708,10.933333333333334
241,"['In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.', 'We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.', 'Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.', 'We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.', 'We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.']","[0, 0, 1, 0, 0]","[0.20408162474632263, 0.2647058665752411, 0.5, 0.1904761791229248, 0.11764705181121826]",B1G6uM0WG,"['A framework that provides a policy for autonomous lane changing by learning to make high-level tactical decisions with deep reinforcement learning, and maintaining a tight integration with a low-level controller to take low-level actions.', 'Considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting, proposes a new learning strategy Q-masking - coupling a defined low level controller with a high level tactical decision making policy.', 'This paper proposes a deep Q-learning approach to the problem of lane change using ""Q-masking,"" which reduces the action space according to contraints or prior knowledge.', 'Authors propose a method which uses a Q-learning-based high-level policy that is combined with a contextual mask derived from safety-contraints and low-level controllers, which disable certain actions from being selectable at certain states. ']","['paper  consider problem autonomous lane changing self driving vehicle multilane  multiagent setting ', 'present framework demonstrates structured data efficient alternative endtoend complete policy learning problem highlevel policy hard formulate using traditional optimization rule based method well designed lowlevel controller available ', 'framework us deep reinforcement learning solely obtain highlevel policy tactical decision making  still maintaining tight integration lowlevel controller  thus getting best world ', 'accomplish qmasking  technique able incorporate prior knowledge  constraint  information lowlevel controller  directly learning process thereby simplifying reward function making learning faster data efficient ', 'provide preliminary result simulator show approach efficient greedy baseline  successful safer human driving ']","In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting., We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available., Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds., We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient., We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.",14,5.705521472392638,11.642857142857142
242,"['Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering.', 'Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates.', 'To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space.', 'We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.', 'Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs.', 'In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation.', 'We show that NGE significantly outperforms previous methods by an order of magnitude.', 'As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs.', 'Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2\n', 'machine.\n']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.1666666567325592, 0.1764705777168274, 0.2142857164144516, 0.0, 0.21621620655059814, 0.13333332538604736, 0.0, 0.1395348757505417, 0.0]",BkgWHnR5tm,"['Automatic robotic design search with graph neural networks', 'Proposes an approach for automatic robot design based on Neural graph evolution. The experiments demonstrate that optimizing both controller and hardware is better than optimizing just the controller.', 'The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers to optimize robot structures, combined with their controllers.  ']","['despite recent success robotic locomotion control  design robot relies heavily human engineering ', 'automatic robot design long studied subject  recent progress slowed due large combinatorial search space difficulty evaluating found candidate ', 'address two challenge  formulate automatic robot design graph search problem perform evolution search graph space ', 'propose neural graph evolution  nge   performs selection current candidate evolves new one iteratively ', 'different previous approach  nge us graph neural network parameterize control policy  reduces evaluation cost new candidate help skill transfer previously evaluated design ', 'addition  nge applies graph mutation uncertainty  gmuc  incorporating model uncertainty  reduces search space balancing exploration exploitation ', 'show nge significantly outperforms previous method order magnitude ', 'shown experiment  nge first algorithm automatically discover kinematically preferred robotic graph structure  fish two symmetrical flat sidefins tail  cheetah athletic front back leg ', 'instead using thousand core week  nge efficiently solves searching problem within day single 64 cpucore amazon ec2', 'machine ']","Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering., Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates., To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space., We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively., Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs., In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation., We show that NGE significantly outperforms previous methods by an order of magnitude., As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs., Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2
, machine.
",22,5.594470046082949,9.863636363636363
243,"['Deep learning on graphs has become a popular research topic with many applications.', 'However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text.', 'Is it possible to transfer this progress to the domain of graphs?', 'We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once.', 'Our method is formulated as a variational autoencoder.', 'We evaluate on the challenging task of conditional molecule generation.']","[0, 0, 0, 0, 1, 0]","[0.10526315122842789, 0.06896551698446274, 0.0, 0.0624999962747097, 0.1428571343421936, 0.1249999925494194]",SJlhPMWAW,"['We demonstate an autoencoder for graphs.', 'Learning to generate graphs using deep learning methods in ""one shot"", directly outputting node and edge existence probabilities, and node attribute vectors.', 'A variational auto encoder to generate graphs']","['deep learning graph become popular research topic many application ', 'however  past work concentrated learning graph embedding task  contrast advance generative model image text ', 'possible transfer progress domain graph ', 'propose sidestep hurdle associated linearization discrete structure decoder output probabilistic fullyconnected graph predefined maximum size directly ', 'method formulated variational autoencoder ', 'evaluate challenging task conditional molecule generation ']","Deep learning on graphs has become a popular research topic with many applications., However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text., Is it possible to transfer this progress to the domain of graphs?, We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once., Our method is formulated as a variational autoencoder., We evaluate on the challenging task of conditional molecule generation.",8,5.552083333333333,12.0
244,"['Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling.', 'Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit.', 'In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.', 'By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability.', 'However, learning towards discrete values of the gates is generally difficult.', 'To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation.', 'Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks.', 'Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0, 0.0357142798602581, 0.4390243887901306, 0.16393442451953888, 0.19354838132858276, 0.04651162400841713, 0.072727270424366, 0.0]",rJiaRbk0-,"['We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.', 'Propose a new ""gate"" function for LSTM to enable the values of the gates towards 0 or 1. ', 'The paper aims to push LSTM gates to be binary by employing the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution.']","['long shortterm memory  lstm  one widely used recurrent structure sequence modeling ', 'goal use gate control information flow  eg  whether skip informationtransformation  recurrent computation  although practical implementation based soft gate partially achieves goal easy overfit ', 'paper  propose new way lstm training  push value gate towards 0 1 ', '  1  better control information flow  gate mostly open closed  instead middle state   2  avoid overfitting certain extent  gate operate flat region  shown correspond better generalization ability ', 'however  learning towards discrete value gate generally difficult ', 'tackle challenge  leverage recently developed gumbelsoftmax trick field variational method  make model trainable standard backpropagation ', 'experimental result language modeling machine translation show  1  value gate generated method reasonable intuitively interpretable   2  proposed method generalizes better achieves better accuracy test set task ', 'moreover  learnt model sensitive lowprecision approximation lowrank approximation gate parameter due flat loss surface ']","Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling., Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit., In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1., By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability., However, learning towards discrete values of the gates is generally difficult., To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation., Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks., Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.",20,5.328947368421052,11.4
245,"['We present a personalized recommender system using neural network for recommending\n', 'products, such as eBooks, audio-books, Mobile Apps, Video and Music.\n', 'It produces recommendations based on customers implicit feedback history such\n', 'as purchases, listens or watches.', 'Our key contribution is to formulate recommendation\n', 'problem as a model that encodes historical behavior to predict the future\n', 'behavior using soft data split, combining predictor and auto-encoder models.', 'We\n', 'introduce convolutional layer for learning the importance (time decay) of the purchases\n', 'depending on their purchase date and demonstrate that the shape of the time\n', 'decay function can be well approximated by a parametrical function.', 'We present\n', 'offline experimental results showing that neural networks with two hidden layers\n', 'can capture seasonality changes, and at the same time outperform other modeling\n', 'techniques, including our recommender in production.', 'Most importantly, we\n', 'demonstrate that our model can be scaled to all digital categories, and we observe\n', 'significant improvements in an online A/B test.', 'We also discuss key enhancements\n', 'to the neural network model and describe our production pipeline.', 'Finally\n', 'we open-sourced our deep learning library which supports multi-gpu model parallel\n', 'training.', 'This is an important feature in building neural network based recommenders\n', 'with large dimensionality of input and output data.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2142857164144516, 0.0, 0.14814814925193787, 0.0, 0.0, 0.06896550953388214, 0.07407406717538834, 0.0, 0.13793103396892548, 0.07692307233810425, 0.2142857164144516, 0.13793103396892548, 0.08695651590824127, 0.0, 0.0, 0.0833333283662796, 0.0, 0.07407406717538834, 0.0, 0.1428571343421936, 0.07999999821186066]",B1lMMx1CW,"['Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website', 'The paper proposes a new neural network based method for recommendation.', 'The authors describe a procedure of building their production recommender system from scratch and integrate time decay of purchases into the learning framework.']","['present personalized recommender system using neural network recommending', 'product  ebooks  audiobooks  mobile apps  video music ', 'produce recommendation based customer  implicit feedback history', 'purchase  listens watch ', 'key contribution formulate recommendation', 'problem model encodes historical behavior predict future', 'behavior using soft data split  combining predictor autoencoder model ', '', 'introduce convolutional layer learning importance  time decay  purchase', 'depending purchase date demonstrate shape time', 'decay function well approximated parametrical function ', 'present', 'offline experimental result showing neural network two hidden layer', 'capture seasonality change  time outperform modeling', 'technique  including recommender production ', 'importantly ', 'demonstrate model scaled digital category  observe', 'significant improvement online ab test ', 'also discus key enhancement', 'neural network model describe production pipeline ', 'finally', 'opensourced deep learning library support multigpu model parallel', 'training ', 'important feature building neural network based recommenders', 'large dimensionality input output data ']","We present a personalized recommender system using neural network for recommending
, products, such as eBooks, audio-books, Mobile Apps, Video and Music.
, It produces recommendations based on customers implicit feedback history such
, as purchases, listens or watches., Our key contribution is to formulate recommendation
, problem as a model that encodes historical behavior to predict the future
, behavior using soft data split, combining predictor and auto-encoder models., We
, introduce convolutional layer for learning the importance (time decay) of the purchases
, depending on their purchase date and demonstrate that the shape of the time
, decay function can be well approximated by a parametrical function., We present
, offline experimental results showing that neural networks with two hidden layers
, can capture seasonality changes, and at the same time outperform other modeling
, techniques, including our recommender in production., Most importantly, we
, demonstrate that our model can be scaled to all digital categories, and we observe
, significant improvements in an online A/B test., We also discuss key enhancements
, to the neural network model and describe our production pipeline., Finally
, we open-sourced our deep learning library which supports multi-gpu model parallel
, training., This is an important feature in building neural network based recommenders
, with large dimensionality of input and output data.",35,5.876847290640394,5.8
246,"['Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration.', 'Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.', 'How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?', 'How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?\n', 'Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.', 'With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.', 'Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.', 'Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.\n', 'Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1538461446762085, 0.1395348757505417, 0.27586206793785095, 0.14999999105930328, 0.6111111044883728, 0.1428571343421936, 0.380952388048172, 0.17142856121063232, 0.1621621549129486]",Hylnis0qKX,"['Couple the GAN based image restoration framework with another task-specific network to generate realistic image while preserving task-specific features.', 'A novel method of Task-GAN of image coupling that couples GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse.', 'The authors propose to augment GAN-based image restoration with another task-specific branch, such as classification tasks, for further improvement.']","['deep learning  dl  algorithm based generative adversarial network  gan  demonstrated great potential computer vision task image restoration ', 'despite rapid development image restoration algorithm using dl gans  image restoration specific scenario  medical image enhancement superresolved identity recognition  still facing challenge ', 'ensure visually realistic restoration avoiding hallucination mode collapse ', 'make sure visually plausible result contain hallucinated feature jeopardizing downstream task pathology identification subject identification ', 'propose resolve challenge coupling gan based image restoration framework another taskspecific network ', 'medical imaging restoration example  proposed model conduct additional pathology recognitionclassification task ensure preservation detailed structure important task ', 'validated multiple medical datasets  demonstrate proposed method lead improved deep learning based image restoration preserving detailed structure diagnostic feature ', 'additionally  trained task network show potential achieve superhuman level performance identifying pathology diagnosis ', 'validation superresolved identity recognition task also show proposed method generalized diverse image restoration task ']","Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration., Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges., How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?, How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?
, Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network., With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task., Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features., Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.
, Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.",15,6.505050505050505,13.2
247,"['Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core.', 'Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space.', 'In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.', 'Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM).', 'Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model.', 'The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.', 'Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.']","[0, 0, 0, 1, 0, 0, 0]","[0.2181818187236786, 0.22580644488334656, 0.2857142686843872, 0.30188679695129395, 0.29999998211860657, 0.14035087823867798, 0.15094339847564697]",BJJLHbb0-,"['An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.', 'The paper presents a joint deep learning framework for dimension reduction-clustering, leads to competitive anomaly detection.', 'A new technique for anomaly detection where the dimension reduction and density estimation steps are jointly optimized.']","['unsupervised anomaly detection multi highdimensional data great importance fundamental machine learning research industrial application  density estimation lie core ', 'although previous approach based dimensionality reduction followed density estimation made fruitful progress  mainly suffer decoupled model learning inconsistent optimization goal incapability preserving essential information lowdimensional space ', 'paper  present deep autoencoding gaussian mixture model  dagmm  unsupervised anomaly detection ', 'model utilizes deep autoencoder generate lowdimensional representation reconstruction error input data point  fed gaussian mixture model  gmm  ', 'instead using decoupled twostage training standard expectationmaximization  em  algorithm  dagmm jointly optimizes parameter deep autoencoder mixture model simultaneously endtoend fashion  leveraging separate estimation network facilitate parameter learning mixture model ', 'joint optimization  well balance autoencoding reconstruction  density estimation latent representation  regularization  help autoencoder escape le attractive local optimum reduce reconstruction error  avoiding need pretraining ', 'experimental result several public benchmark datasets show  dagmm significantly outperforms stateoftheart anomaly detection technique  achieves 14  improvement based standard f1 score ']","Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core., Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space., In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection., Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM)., Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model., The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training., Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.",20,6.49537037037037,10.8
248,"['Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning.', 'In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.', 'In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace.', 'We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification.', 'Moreover, our PSN approach has the ability of end-to-end learning.', 'In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.']","[0, 0, 0, 1, 0, 0]","[0.10256409645080566, 0.25806450843811035, 0.0, 0.27586206793785095, 0.09999999403953552, 0.05882352590560913]",rkzfuiA9F7,"['We proposed Projective Subspace Networks for few-shot and semi-supervised few-shot learning', 'This paper proposes a new embedding-based approach for the problem of few-shot learning and an extension to this model to the semi-supervised few-shot learning setting.', 'New method for fully and semi-supervised few-shot classification based on learning a general embedding and then learning a subspace of it for each class']","['generalization limited example  usually studied umbrella metalearning  equips learning technique ability adapt quickly dynamical environment prof essential aspect lifelong learning ', 'paper  introduce projective subspace network  psn   deep learning paradigm learns nonlinear embeddings limited supervision ', 'contrast previous study  embedding psn deems sample given class form affine subspace ', 'show modeling lead robust solution  yielding competitive result supervised semisupervised fewshot classification ', 'moreover  psn approach ability endtoend learning ', 'contrast previous work  projective subspace thought richer representation capturing higherorder information datapoints modeling new concept ']","Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning., In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision., In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace., We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification., Moreover, our PSN approach has the ability of end-to-end learning., In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.",14,5.94488188976378,9.071428571428571
249,"['This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.', 'To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE).', 'In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games.', 'The ADM is trained in a self-supervised fashion to predict the actions taken by the agent.', 'The learned contingency information is used as a part of the state representation for exploration purposes.', 'We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.', ""For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data."", 'Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.3333333134651184, 0.11764705181121826, 0.09090908616781235, 0.0624999962747097, 0.060606054961681366, 0.13636362552642822, 0.2666666507720947, 0.1860465109348297]",HyxGB2AcY7,"[""We investigate contingency-awareness and controllable aspects in exploration and achieve state-of-the-art performance on Montezuma's Revenge without expert demonstrations."", 'This paper investigates the problem of extracting a meaningful state representation to help with exploration when confronted with a sparse reward task by identifying controllable (learned) features of the state', 'This paper proposes the novel idea of using contingency awareness to aid exploration in sparse-reward reinforcement learning tasks, obtaining state of the art results.']","['paper investigates whether learning contingencyawareness controllable aspect environment lead better exploration reinforcement learning ', 'investigate question  consider instantiation hypothesis evaluated arcade learning element  ale  ', 'study  develop attentive dynamic model  adm  discovers controllable element observation  often associated location character atari game ', 'adm trained selfsupervised fashion predict action taken agent ', 'learned contingency information used part state representation exploration purpose ', 'demonstrate combining actorcritic algorithm countbased exploration using representation achieves impressive result set notoriously challenging atari game due sparse reward ', 'example  report stateoftheart score  11000 point montezuma revenge without using expert demonstration  explicit highlevel information  eg  ram state   supervisory data ', 'experiment confirm contingencyawareness indeed extremely powerful concept tackling exploration problem reinforcement learning open interesting research question investigation ']","This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning., To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE)., In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games., The ADM is trained in a self-supervised fashion to predict the actions taken by the agent., The learned contingency information is used as a part of the state representation for exploration purposes., We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards., For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data., Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.",15,6.2555555555555555,12.0
250,"['Disentangling factors of variation has always been a challenging problem in representation learning.', 'Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc.', 'In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.', 'The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation.', 'By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images.', 'In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training.', 'Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.']","[0, 0, 1, 0, 0, 0, 0]","[0.1599999964237213, 0.12121211737394333, 0.5517241358757019, 0.13793103396892548, 0.1666666567325592, 0.13793103396892548, 0.06451612710952759]",Syr8Qc1CW,"['We proposed a supervised algorithm, DNA-GAN, to disentangle multiple attributes of images.', 'This paper investigates the problem of attribute-conditioned image generation using generative adversarial networks, and proposes to generate images from attribute and latent code as high-level representation.', 'This paper proposed a new method to disentangle different attributes of images using a novel DNA structure GAN']","['disentangling factor variation always challenging problem representation learning ', 'existing algorithm suffer many limitation  unpredictable disentangling factor  bad quality generated image encoding  lack identity information  etc ', 'paper  proposed supervised algorithm called dnagan trying disentangle different attribute image ', 'latent representation image dnalike  individual piece represents independent factor variation ', 'annihilating recessive piece swapping certain piece two latent representation  obtain another two different representation could decoded image ', 'order obtain realistic image also disentangled representation  introduced discriminator adversarial training ', 'experiment multipie celeba datasets demonstrate effectiveness method advantage overcoming limitation existing method ']","Disentangling factors of variation has always been a challenging problem in representation learning., Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc., In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images., The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation., By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images., In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training., Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.",15,6.294117647058823,9.066666666666666
251,"['Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode.', 'We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations).', 'This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data).', 'We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.']","[0, 1, 0, 0]","[0.09090908616781235, 0.21875, 0.145454540848732, 0.1666666567325592]",B1e4wo09K7,"['This paper presents a novel latent-variable generative modelling technique that enables the representation of global information into one latent variable and local information into another latent variable.', 'The paper presents a VAE that uses labels to separate the learned representation into an invariant and a covariant part.']","['representation learnt deep neural network tend highly informative  opaque term information learn encode ', 'introduce approach probabilistic modelling learns represent data two separate deep representation  invariant representation encodes information class data belongs  equivariant representation encodes symmetry transformation defining particular data point within class manifold  equivariant sense representation varies naturally symmetry transformation  ', 'approach representation learning conceptually transparent  easy implement  inprinciple generally applicable data comprised discrete class continuous distribution  eg  object image  topic language  individual behavioural data  ', 'demonstrate qualitatively compelling representation learning competitive quantitative performance  supervised semisupervised setting  versus comparable modelling approach literature little fine tuning ']","Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode., We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations)., This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data)., We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.",12,6.523809523809524,11.307692307692308
252,"['Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.', 'However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.', 'One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).\n\n', 'Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.', 'Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.', 'We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.', 'As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.', 'Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.\n']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.2181818187236786, 0.2666666507720947, 0.2222222238779068, 0.44897958636283875, 0.33898305892944336, 0.1860465109348297, 0.260869562625885, 0.21739129722118378]",H1aIuk-RW,"['We approach to the problem of active learning as a core-set selection problem and show that this approach is especially useful in the batch active learning setting which is crucial when training CNNs.', 'The authors provide an algorithm-agnostic active learning algorithm for multi-class classification', 'The paper proposes a batch mode active learning algorithm for CNN as a core-set problem which outperforms random sampling and uncertainty sampling.', 'Studies active learning for convolutional neural networks and formulates the active learning problem as core-set selection and presents a novel strategy']","['convolutional neural network  cnns  successfully applied many recognition learning task using universal recipe  training deep model large dataset supervised example ', 'however  approach rather restrictive practice since collecting large set labeled image expensive ', 'one way ease problem coming smart way choosing image labelled large collection  ie  active learning  ', 'empirical study suggests many active learning heuristic literature effective applied cnns applied batch setting ', 'inspired limitation  define problem active learning coreset selection  ie  choosing set point model learned selected subset competitive remaining data point ', 'present theoretical result characterizing performance selected subset using geometry datapoints ', 'active learning algorithm  choose subset expected yield best result according characterization ', 'experiment show proposed method significantly outperforms existing approach image classification experiment large margin ']","Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples., However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive., One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).

, Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting., Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points., We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints., As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization., Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.
",12,5.474226804123711,13.857142857142858
253,"['Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP).', 'This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps.', 'We introduce a simple stochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem.', 'Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed.', 'Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them.', 'Our algorithm\\footnote{Our code is available at https://github.com/bhargav104/h-detach.', '} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better.', 'We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0714285671710968, 0.16326530277729034, 0.3636363446712494, 0.1111111044883728, 0.1538461446762085, 0.0, 0.1818181723356247, 0.23255813121795654]",ryf7ioRqFX,"['A simple algorithm to improve optimization and handling of long term dependencies in LSTM', 'The paper introduces a simple stochastic algorithm called h-detach that is specific to LSTM optimization and targeted towards addressing this problem.', 'Proposes a simple modification to the training process of the LSTM to facilitate gradient propogation along cell states, or the ""linear temporal path""']","['recurrent neural network known notorious exploding vanishing gradient problem  evgp  ', 'problem becomes evident task information needed correctly solve exist long time scale  evgp prevents important gradient component backpropagated adequately large number step ', 'introduce simple stochastic algorithm  textit  h  detach  specific lstm optimization targeted towards addressing problem ', 'specifically  show lstm weight large  gradient component linear path  cell state  lstm computational graph get suppressed ', 'based hypothesis component carry information long term dependency  show empirically   suppression prevent lstms capturing ', 'algorithmfootnote  code available http  githubcombhargav104hdetach ', ' prevents gradient flowing path getting suppressed  thus allowing lstm capture dependency better ', 'show significant improvement vanilla lstm gradient based training term convergence speed  robustness seed learning rate  generalization using modification lstm gradient various benchmark datasets ']","Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP)., This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps., We introduce a simple stochastic algorithm (\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem., Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed., Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them., Our algorithm\footnote{Our code is available at https://github.com/bhargav104/h-detach., } prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better., We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.",15,6.217877094972067,11.933333333333334
254,"['Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision.', 'However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes.', 'Further, they are also vulnerable to adversarial examples.', 'We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set.', 'We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization.', 'As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples.', ""To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness."", 'We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.', 'Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.23529411852359772, 0.1599999964237213, 0.14999999105930328, 0.10256409645080566, 0.22727271914482117, 0.10810810327529907, 0.26923075318336487, 0.1249999925494194]",Skgge3R9FQ,"['Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.', 'This paper proposes adding an additional label for detecting OOD samples and adversarial examples in CNN models.', 'The paper proposes an additional class that incorporates natural out-distribution images and interpolated images for adversarial and out-distribution samples in CNNs']","['convolutional neural network  cnns  significantly improve stateoftheart many application  especially computer vision ', 'however  cnns still suffer tendency confidently classify outdistribution sample unknown class predefined known class ', ' also vulnerable adversarial example ', 'relating two issue tendency cnns overgeneralize area input space covered well training set ', 'show cnn augmented extra output class act simple yet effective endtoend model controlling overgeneralization ', 'appropriate training set extra class  introduce two resource computationally efficient obtain  representative natural outdistribution set interpolated indistribution sample ', 'help select representative natural outdistribution set among available one  propose simple measurement ass outdistribution set fitness ', 'also demonstrate training augmented cnn representative outdistribution natural datasets interpolated sample allows better handle wide range unseen outdistribution sample blackbox adversarial example without training adversary ', 'finally  show generation whitebox adversarial attack using proposed augmented cnn become harder  attack algorithm get around rejection region generating actual adversary ']","Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision., However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes., Further, they are also vulnerable to adversarial examples., We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set., We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization., As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples., To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness., We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries., Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.",16,6.070422535211268,13.3125
255,"['Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization.', 'Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit.', 'Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity.', 'Although these techniques have been proven successful in terms of results, they seem to waste capacity.', 'In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity.', 'In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.']","[0, 0, 0, 0, 1, 0]","[0.1428571343421936, 0.1249999925494194, 0.1071428507566452, 0.04444443807005882, 0.2448979616165161, 0.20895521342754364]",ByJWeR1AW,"['In a deep convolutional neural network trained with sufficient level of data augmentation, optimized by SGD, explicit regularizers (weight decay and dropout) might not provide any additional generalization improvement.', 'This paper proposes data augmentation as an alternative to commonly used regularisation techniques, and shows that for a few reference models/tasks that the same generalization performance can be achived using only data augmentation.', 'This paper presents a systematic study of data augmentation in image classification with deep neural networks, suggesting that data augmentation can replicit some common regularizers like weight decay and dropout.']","['modern deep artificial neural network achieved impressive result model large capacity  compared number training example  control overfitting help different form regularization ', 'regularization implicit  case stochastic gradient descent parameter sharing convolutional layer  explicit ', 'common explicit regularization technique  dropout weight decay  reduce effective capacity model typically require use deeper wider architecture compensate reduced capacity ', 'although technique proven successful term result  seem waste capacity ', 'contrast  data augmentation technique reduce generalization error increasing number training example without reducing effective capacity ', 'paper systematically analyze effect data augmentation popular architecture conclude data augmentation alone  without explicit regularization technique  achieve performance higher regularized model  especially training fewer example ']","Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization., Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit., Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity., Although these techniques have been proven successful in terms of results, they seem to waste capacity., In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity., In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.",13,6.182926829268292,12.615384615384615
256,"['Text editing on mobile devices can be a tedious process.', 'To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing.', 'In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.', 'Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo.', 'Variations of our gestures exist for one and two hands.', 'We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions.', 'Our results showed that Gedits gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.']","[0, 0, 1, 0, 0, 0, 0]","[0.23076923191547394, 0.17777776718139648, 1.0, 0.1621621549129486, 0.23076923191547394, 0.06451612710952759, 0.09302324801683426]",CZ938F7zVF,"['In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.', 'Reports the design and evaluation of the Gedit interaction techniques.', 'Presents a new set of touch gestures to perform seamless transition between text entry and text editing in mobile devices']","['text editing mobile device tedious process ', 'perform various editing operation  user must repeatedly move finger text input area keyboard  making multiple round trip breaking flow typing ', 'work  present gedit  system onkeyboard gesture convenient mobile text editing ', 'design includes ring gesture flick cursor control  bezel gesture mode switching  four gesture shortcut copy  paste  cut  undo ', 'variation gesture exist one two hand ', 'conducted experiment compare gedit de facto touchwidget based editing interaction ', 'result showed gedit  gesture easy learn  24  17  faster de facto interaction one twohanded use  respectively  preferred participant ']","Text editing on mobile devices can be a tedious process., To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing., In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing., Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo., Variations of our gestures exist for one and two hands., We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions., Our results showed that Gedits gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.",19,5.108695652173913,7.2631578947368425
257,"['Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.', 'Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored.', 'This paper attempts to provide an alternative understanding from the perspective of maximum entropy.', 'We first derive two feature conditions that softmax regression strictly apply maximum entropy principle.', 'DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle.', 'The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.']","[0, 0, 0, 0, 1, 0]","[0.0, 0.0, 0.2857142686843872, 0.3571428656578064, 0.4864864945411682, 0.15789473056793213]",r1kj4ACp-,"['We prove that DNN is a recursively approximated solution to the maximum entropy principle.', 'Presents a derivation which links a DNN to recursive application of maximum entropy model fitting.', 'The paper aims to provide a view of deep learning from the perspective of maximum entropy principle.']","['deep learning achieves remarkable generalization capability overwhelming number model parameter ', 'theoretical understanding deep learning generalization receives recent attention yet remains fully explored ', 'paper attempt provide alternative understanding perspective maximum entropy ', 'first derive two feature condition softmax regression strictly apply maximum entropy principle ', 'dnn regarded approximating feature condition multilayer feature learning  proved recursive solution towards maximum entropy principle ', 'connection dnn maximum entropy well explains typical design shortcut regularization improves model generalization  provides instruction future model development ']","Deep learning achieves remarkable generalization capability with overwhelming number of model parameters., Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored., This paper attempts to provide an alternative understanding from the perspective of maximum entropy., We first derive two feature conditions that softmax regression strictly apply maximum entropy principle., DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle., The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.",8,6.6,13.125
258,"[' As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.', 'We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.', 'Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.', 'We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.']","[0, 1, 0, 0]","[0.25974026322364807, 0.9818181991577148, 0.18867923319339752, 0.15686273574829102]",SyNvti09KQ,"['We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. ', 'Proposes a reinforcement learning framework based on human emotional reaction in the context of autonomous driving.', 'The authors propose to use signals, such as basic autonomic visceral responses that influence decision-making, within the RL framework by augmenting RL reward functions with a model learned directly from human nervous system responses.', 'Proposes to use physiological signals to improve performance of reinforcement learning algorithms and build an intrinsic reward function that is less sparse by measuring heart pulse amplitude']","['people learn navigate world  autonomic nervous system  eg   fight flight  response provide intrinsic feedback potential consequence action choice  eg  becoming nervous close cliff edge driving fast around bend   physiological change correlated biological preparation protect oneself danger ', 'present novel approach reinforcement learning leverage taskindependent intrinsic reward function trained peripheral pulse measurement correlated human autonomic nervous system response ', 'hypothesis reward function circumvent challenge associated sparse skewed reward reinforcement learning setting help improve sample efficiency ', 'test simulated driving environment show increase speed learning reduce number collision learning stage ']"," As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger., We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses., Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency., We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.",7,5.854014598540146,17.125
259,"['Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.', 'However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.', 'Are CNNs robust or fragile to label noise?', 'Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.', 'In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).', 'Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.', 'We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.']","[0, 0, 1, 0, 0, 0, 0]","[0.23999999463558197, 0.06666666269302368, 0.9411764740943909, 0.13333332538604736, 0.10256409645080566, 0.3199999928474426, 0.07692307233810425]",H1xmqiAqFm,"['Are CNNs robust or fragile to label noise? Practically, robust.', 'The authors challenge the CNNs robustness to label noise using ImageNet 1k tree of WordNet.', 'An analysis of convolutional neural network model performance when class dependent and class independent noise is introduced', 'Demonstrates that CNNs are more robust to class-relevant label noise and argues that real-world noise should be class-relevant']","['deep convolutional neural network  cnns  known robust label noise extensive datasets ', 'however  time  cnns capable memorizing label even random  mean memorize corrupted label ', 'cnns robust fragile label noise ', 'much research focusing memorization us classindependent label noise simulate label corruption  setting simple unrealistic ', 'paper  investigate behavior cnns classdependently simulated label noise  generated based conceptual distance class large dataset  ie  imagenet1k  ', 'contrary previous knowledge  reveal cnns robust classdependent label noise classindependent label noise ', 'also demonstrate network classdependent noise situation learn similar representation noise situation  compared classindependent noise situation ']","Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets., However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels., Are CNNs robust or fragile to label noise?, Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic., In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k)., Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise., We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.",16,5.801418439716312,8.8125
260,"['Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.', 'Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.', 'Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.', 'Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.\n']","[0, 0, 1, 0]","[0.14814814925193787, 0.05128204822540283, 0.19354838132858276, 0.0952380895614624]",H1xQVn09FX,"['High-quality audio synthesis with GANs', 'Proposes an approach that uses GAN framework to generate audio through modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. ', 'A strategy to generate audio samples from noise with GANs, with changes to the architecture and representation necessary to generate convincing audio that contains an interpretable latent code.', 'Presents a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied']","['efficient audio synthesis inherently difficult machine learning task  human perception sensitive global structure finescale waveform coherence ', 'autoregressive model  wavenet  model local structure expense global latent structure slow iterative sampling  generative adversarial network  gans   global latent conditioning efficient parallel sampling  struggle generate locallycoherent audio waveform ', 'herein  demonstrate gans fact generate highfidelity locallycoherent audio modeling log magnitude instantaneous frequency sufficient frequency resolution spectral domain ', 'extensive empirical investigation nsynth dataset  demonstrate gans able outperform strong wavenet baseline automated human evaluation metric  efficiently generate audio several order magnitude faster autoregressive counterpart ']","Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence., Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms., Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain., Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.
",13,6.53125,9.846153846153847
261,"['In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.', 'Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain.', 'We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity.', 'Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.']","[0, 1, 0, 0]","[0.06896550953388214, 0.4000000059604645, 0.06896550953388214, 0.060606054961681366]",HklZOfW0W,"['Graph Optimization with signal filtering in the vertex domain.', 'The paper investigates learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights uses a projected sub-gradient descent algorithm.', 'Develops a novel scheme for backpropogating on the adjacency matrix of a neural network graph']","['work propose novel approach learning graph representation data using gradient obtained via backpropagation ', 'next build neural network architecture compatible optimization approach motivated graph filtering vertex domain ', 'demonstrate learned graph richer structure often used nearest neighbor graph constructed based feature similarity ', 'experiment demonstrate improve prediction quality several convolution graph architecture  others appeared insensitive input graph ']","In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation., Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain., We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity., Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.",5,5.872093023255814,17.2
262,"['The use of AR in an industrial context could help for the training of new operators.', 'To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.', 'This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.', 'This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.', 'WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.', ""The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks."", 'In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.']","[0, 0, 0, 1, 0, 0, 0]","[0.25, 0.2926829159259796, 0.24390242993831635, 0.3333333432674408, 0.2222222238779068, 0.10256409645080566, 0.1860465109348297]",1qdNTwXpgE,"['This paper describe a 3D authoring tool for providing AR in assembly lines of industry 4.0', 'The paper addresses how AR authoring tools support training of assembly line systems and proposes an approach', 'An AR guidance system for industrial assembly lines that allows for on-site authoring of AR content.', 'Presents a system that allows factory workers to be trained more efficiently using augmented reality system. ']","['use ar industrial context could help training new operator ', 'able use ar guidance system  need tool quickly create 3d representation assembly line ar annotation ', 'tool easy use operator ar vr specialist  typically manager assembly line ', 'proposed waat  3d authoring tool allowing user quickly create 3d model workstation  also test ar guidance placement ', 'waat make onsite authoring possible  really help accurate 3d representation assembly line ', 'verification ar guidance also useful make sure everything visible nt interfere technical task ', 'addition feature  future work directed deployment waat real boiler assembly line ass usability solution ']","The use of AR in an industrial context could help for the training of new operators., To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations., This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line., This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement., WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line., The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks., In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.",12,4.592814371257485,13.916666666666666
263,"['Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions.', 'In spite of its great success in applications, GAN is known to be notoriously hard to train.', 'The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area.', 'To resolve these issues, we need to first understand how GANs work.', 'Herein, we take a step toward this direction by examining the dynamics of GANs.', 'We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator.', 'By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.', 'The same framework also applies to multi-task learning and distributional robust learning problems.', 'We verify our analysis on numerical examples with both synthetic and real data sets.', 'We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.0833333283662796, 0.09756097197532654, 0.20408162474632263, 0.10810810327529907, 0.1538461446762085, 0.260869562625885, 0.8571428656578064, 0.05405404791235924, 0.10256409645080566, 0.0952380895614624]",rylIy3R9K7,"['We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.', 'This paper uses GANs and multi-task learning to provide a convergence guarantee for primal-dual algorithms on certain min-max problems.', 'Analyses the learning dynamics of GANs by formulating the problem as a primal-dual optimisation problem by assuming a limited class of models']","['generative adversarial network  gan  one best known unsupervised learning technique day due superior ability learn data distribution ', 'spite great success application  gan known notoriously hard train ', 'tremendous amount time take run training algorithm sensitivity hyperparameter tuning haunting researcher area ', 'resolve issue  need first understand gans work ', 'herein  take step toward direction examining dynamic gans ', 'relate large class gans including wasserstein gans maxmin optimization problem coupling term linear discriminator ', 'developing new primaldual optimization tool  show  proper stepsize choice  widely used firstorder iterative algorithm training gans would fact converge stationary solution sublinear rate ', 'framework also applies multitask learning distributional robust learning problem ', 'verify analysis numerical example synthetic real data set ', 'hope analysis shed light future study theoretical property relevant machine learning problem ']","Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions., In spite of its great success in applications, GAN is known to be notoriously hard to train., The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area., To resolve these issues, we need to first understand how GANs work., Herein, we take a step toward this direction by examining the dynamics of GANs., We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator., By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate., The same framework also applies to multi-task learning and distributional robust learning problems., We verify our analysis on numerical examples with both synthetic and real data sets., We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.",16,5.372448979591836,12.25
264,"['Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction.', 'We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest.', 'However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict.', ""We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards)."", 'We call this consequentialist conditional cooperation.', 'We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games.', 'We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.']","[0, 0, 0, 0, 0, 1, 0]","[0.10810810327529907, 0.2702702581882477, 0.05714285373687744, 0.24390242993831635, 0.08695651590824127, 0.5333333015441895, 0.10810810327529907]",BkabRiQpb,"['We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.', 'Learning to play two-player general-sum games with state with imperfect information', ""Specifies a trigger strategy (CCC) and corresponding algorithm, demonstrating convergence to efficient outcomes in social dilemmas without need for agents to observe each other's actions.""]","['social dilemma  mutual cooperation lead high payoff participant face incentive cheat  ubiquitous multiagent interaction ', 'wish construct agent cooperate pure cooperator  avoid exploitation pure defector  incentivize cooperation rest ', 'however  often action taken partner  partially  unobserved consequence individual action hard predict ', 'show large class game good strategy constructed conditioning one behavior solely outcome  ie  one past reward  ', 'call consequentialist conditional cooperation ', 'show construct strategy using deep reinforcement learning technique demonstrate  analytically experimentally  effective social dilemma beyond simple matrix game ', 'also show limitation relying purely consequence discus need understanding consequence intention behind action ']","Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction., We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest., However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict., We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards)., We call this consequentialist conditional cooperation., We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games., We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.",14,5.778523489932886,9.933333333333334
265,"['In distributed training, the communication cost due to the transmission of gradients\n', 'or the parameters of the deep model is a major bottleneck in scaling up the number\n', 'of processing nodes.', 'To address this issue, we propose dithered quantization for\n', 'the transmission of the stochastic gradients and show that training with Dithered\n', 'Quantized Stochastic Gradients (DQSG) is similar to the training with unquantized\n', 'SGs perturbed by an independent bounded uniform noise, in contrast to the other\n', 'quantization methods where the perturbation depends on the gradients and hence,\n', 'complicating the convergence analysis.', 'We study the convergence of training\n', 'algorithms using DQSG and the trade off between the number of quantization\n', 'levels and the training time.', 'Next, we observe that there is a correlation among the\n', 'SGs computed by workers that can be utilized to further reduce the communication\n', 'overhead without any performance loss.', 'Hence, we develop a simple yet effective\n', 'quantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\n', 'communication significantly without requiring the workers communicating extra\n', 'information to each other.', 'We prove that although NDQSG requires significantly\n', 'less bits, it can achieve the same quantization variance bound as DQSG.', 'Our\n', 'simulation results confirm the effectiveness of training using DQSG and NDQSG\n', 'in reducing the communication bits or the convergence time compared to the\n', 'existing methods without sacrificing the accuracy of the trained model.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2380952388048172, 0.13333332538604736, 0.05882352963089943, 0.09999999403953552, 0.1428571343421936, 0.190476194024086, 0.13636362552642822, 0.1463414579629898, 0.05714285373687744, 0.10810810327529907, 0.190476194024086, 0.1111111119389534, 0.04878048226237297, 0.1818181723356247, 0.0, 0.0, 0.1428571343421936, 0.1538461446762085, 0.05714285373687744, 0.0, 0.1395348757505417, 0.1428571343421936, 0.19512194395065308, 0.14999999105930328]",rJxMM2C5K7,"['The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ', 'The authors propose applying dithered quantization to the stochastic gradients computed through the training process, which improves quantization error and achieves superior results compared to baselines, and propose a nested scheme to reduce communication cost.', 'Authors establish a connection between communication reduction in distributed optimization and dithered quantization and develops two new distributed training algorithms where communication overhead is significantly reduced.']","['distributed training  communication cost due transmission gradient', 'parameter deep model major bottleneck scaling number', 'processing node ', 'address issue  propose dithered quantization', 'transmission stochastic gradient show training dithered', 'quantized stochastic gradient  dqsg  similar training unquantized', 'sg perturbed independent bounded uniform noise  contrast', 'quantization method perturbation depends gradient hence ', 'complicating convergence analysis ', 'study convergence training', 'algorithm using dqsg trade number quantization', 'level training time ', 'next  observe correlation among', 'sg computed worker utilized reduce communication', 'overhead without performance loss ', 'hence  develop simple yet effective', 'quantization scheme  nested dithered quantized sg  ndqsg   reduce', 'communication significantly without requiring worker communicating extra', 'information ', 'prove although ndqsg requires significantly', 'le bit  achieve quantization variance bound dqsg ', '', 'simulation result confirm effectiveness training using dqsg ndqsg', 'reducing communication bit convergence time compared', 'existing method without sacrificing accuracy trained model ']","In distributed training, the communication cost due to the transmission of gradients
, or the parameters of the deep model is a major bottleneck in scaling up the number
, of processing nodes., To address this issue, we propose dithered quantization for
, the transmission of the stochastic gradients and show that training with Dithered
, Quantized Stochastic Gradients (DQSG) is similar to the training with unquantized
, SGs perturbed by an independent bounded uniform noise, in contrast to the other
, quantization methods where the perturbation depends on the gradients and hence,
, complicating the convergence analysis., We study the convergence of training
, algorithms using DQSG and the trade off between the number of quantization
, levels and the training time., Next, we observe that there is a correlation among the
, SGs computed by workers that can be utilized to further reduce the communication
, overhead without any performance loss., Hence, we develop a simple yet effective
, quantization scheme, nested dithered quantized SG (NDQSG), that can reduce the
, communication significantly without requiring the workers communicating extra
, information to each other., We prove that although NDQSG requires significantly
, less bits, it can achieve the same quantization variance bound as DQSG., Our
, simulation results confirm the effectiveness of training using DQSG and NDQSG
, in reducing the communication bits or the convergence time compared to the
, existing methods without sacrificing the accuracy of the trained model.",33,5.662222222222222,6.818181818181818
266,"['Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks.', 'However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  ', 'Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  ', 'Including adversarial examples during training is a popular defense mechanism against adversarial attacks.', 'In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.', 'We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game.', 'We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.\n']","[0, 0, 0, 0, 0, 1, 0]","[0.11764705181121826, 0.05405404791235924, 0.25, 0.2222222238779068, 0.13333332538604736, 0.3636363446712494, 0.13636362552642822]",S1lIMn05F7,"['Jointly train an adversarial noise generating network with a classification network to provide better robustness to adversarial attacks.', 'A GAN solution for deep models of classification, faced to white and black box attacks, that produces robust models. ', 'The paper proposes a defensive mechanism against adversarial attacks using GANs with generated perturbations used as adversarial examples and a discriminator used to distinguish between them']","['deep neural network shown perform well many classical machine learning problem  especially image classification task ', 'however  researcher found neural network easily fooled  surprisingly sensitive small perturbation imperceptible human ', 'carefully crafted input image  adversarial example  force welltrained neural network provide arbitrary output ', 'including adversarial example training popular defense mechanism adversarial attack ', 'paper propose new defensive mechanism generative adversarial network  gan  framework ', 'model adversarial noise using generative network  trained jointly classification discriminative network minimax game ', 'show empirically adversarial network approach work well black box attack  performance par stateofart method ensemble adversarial training adversarial training projected gradient descent ']","Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks., However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  , Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  , Including adversarial examples during training is a popular defense mechanism against adversarial attacks., In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework., We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game., We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.
",12,6.302158273381295,11.583333333333334
267,"['Deep learning has become the state of the art approach in many machine learning problems such as classification.', 'It has recently been shown that deep learning is highly vulnerable to adversarial perturbations.', 'Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians.', 'Hence, in order to use deep learning without safety concerns a proper defense strategy is required.', 'We propose to use ensemble methods as a defense strategy against adversarial perturbations.', 'We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task.', 'This makes ensemble methods an attractive defense strategy against adversarial attacks.', 'We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.06896550953388214, 0.29629629850387573, 0.1904761791229248, 0.27586206793785095, 0.692307710647583, 0.12121211737394333, 0.4166666567325592, 0.3255814015865326]",rkA1f3NpZ,"['Using ensemble methods as a defense to adversarial perturbations against deep neural networks.', 'This paper proposes to use ensembling as an adversarial defense mechanism.', 'Empirally investigated the robustness of different deep neural entworks ensembles to the two types of attacks, FGSM and BIM, on two popular datasets, MNIST and CIFAR10']","['deep learning become state art approach many machine learning problem classification ', 'recently shown deep learning highly vulnerable adversarial perturbation ', 'taking camera system selfdriving car example  small adversarial perturbation cause system make error important task  classifying traffic sign detecting pedestrian ', 'hence  order use deep learning without safety concern proper defense strategy required ', 'propose use ensemble method defense strategy adversarial perturbation ', 'find attack leading one model misclassify imply network performing task ', 'make ensemble method attractive defense strategy adversarial attack ', 'empirically show mnist cifar10 data set ensemble method improve accuracy neural network test data also increase robustness adversarial perturbation ']","Deep learning has become the state of the art approach in many machine learning problems such as classification., It has recently been shown that deep learning is highly vulnerable to adversarial perturbations., Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians., Hence, in order to use deep learning without safety concerns a proper defense strategy is required., We propose to use ensemble methods as a defense strategy against adversarial perturbations., We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task., This makes ensemble methods an attractive defense strategy against adversarial attacks., We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.",11,5.563291139240507,14.363636363636363
268,"['In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.', 'In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.', 'However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.', 'Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.', 'A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.', 'Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.']","[0, 0, 0, 1, 0, 0]","[0.21276594698429108, 0.19512194395065308, 0.1538461446762085, 0.29411762952804565, 0.20512819290161133, 0.29411762952804565]",HJ39YKiTb,"['Proposal of the sentence generation method based on fusion between textual information and visual information associated with the textual information', 'This work describes a deep learning model for dialogue systems that takes advantage of visual information.', 'This paper proposes a novel dataset for grounded dialog and makes a computational observation that it could help to reason about vision even when performing text-based dialog.', 'Proposes to augment traditional text-based sentence generation/dialogue approaches by incorporating visual information by collecting a bunch of data consisting of both text and associated images or video']","['paper  propose associative conversation model generates visual information textual information us generating sentence order utilize visual information dialogue system without image input ', 'research neural machine translation  study generate translated sentence using image sentence  study show visual information improves translation performance ', 'however  possible use sentence generation algorithm using image dialogue system since many textbased dialogue system accept text input ', 'approach generates  associate  visual information input text generates response text using context vector fusing associative visual information sentence textual information ', 'comparative experiment proposed model model without association showed proposed model generating useful sentence associating visual information related sentence ', 'furthermore  analysis experiment visual association showed proposed model generates  associate  visual information effective sentence generation ']","In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input., In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance., However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input., Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information., A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences., Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.",11,6.339622641509434,14.454545454545455
269,"['Feedforward convolutional neural network has achieved a great success in many computer vision tasks.', 'While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system.', 'In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.', 'We found that such feedback connections could enable lower layers to ``rethink"" about their representations given the top-down contextual information.', 'We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification.', 'We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.']","[0, 1, 0, 0, 0, 0]","[0.2142857164144516, 0.24390242993831635, 0.19354838132858276, 0.05882352590560913, 0.04651162400841713, 0.05405404791235924]",HkzyX3CcFQ,"['we proposed a novel contextual recurrent convolutional network with robust property of visual learning ', 'This paper introduces feedback connection to enhance feature learning through incorporating context information.', 'The paper proposes to add ""recurrent"" connections inside a convolution network with gating mechanism.']","['feedforward convolutional neural network achieved great success many computer vision task ', 'validly imitates hierarchical structure biological visual system  still lack one essential architectural feature  contextual recurrent connection feedback  widely exists biological visual system ', 'work  designed contextual recurrent convolutional network feature embedded standard cnn structure ', 'found feedback connection could enable lower layer  rethink  representation given topdown contextual information ', 'carefully studied component network  showed robustness superiority feedforward baseline task noise image classification  partially occluded object recognition finegrained image classification ', 'believed work could important step help bridge gap computer vision model real biological visual system ']","Feedforward convolutional neural network has achieved a great success in many computer vision tasks., While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system., In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure., We found that such feedback connections could enable lower layers to ``rethink"" about their representations given the top-down contextual information., We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification., We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.",11,6.1521739130434785,12.545454545454545
270,"['Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains.', 'The techniques driving these advances, however, lack a formal method to account for model uncertainty.', 'While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.', 'In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.', 'We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty.', 'Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure.', 'Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.']","[0, 0, 0, 0, 1, 0, 0]","[0.23076923191547394, 0.1599999964237213, 0.3214285671710968, 0.3050847351551056, 0.970588207244873, 0.3571428656578064, 0.21875]",BJlrSmbAZ,"['We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty in conventional networks.', 'This paper proposes using batch normalisation at test time to get the predictive uncertainty, and shows Monte Carlo prediction at test time using batch norm is better than dropout.', 'Proposes that the regularization procedure called batch normalization can be understood as performing approximate Bayesian inference, which performs similarly to MC dropout in terms of the estimates of uncertainty that it produces.']","['deep neural network led series breakthrough  dramatically improving stateoftheart many domain ', 'technique driving advance  however  lack formal method account model uncertainty ', 'bayesian approach learning provides solid theoretical framework handle uncertainty  inference bayesianinspired deep neural network difficult ', 'paper  provide practical approach bayesian learning relies regularization technique found nearly every modern network  batch normalization ', 'show training deep network using batch normalization equivalent approximate inference bayesian model  demonstrate finding allows u make useful estimate model uncertainty ', 'using approach  possible make meaningful uncertainty estimate using conventional architecture without modifying network training procedure ', 'approach thoroughly validated series empirical experiment different task using various measure  showing outperform baseline majority datasets strong statistical significance ']","Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains., The techniques driving these advances, however, lack a formal method to account for model uncertainty., While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult., In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization., We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty., Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure., Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.",16,5.92814371257485,10.4375
271,"['Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  ', 'However, gradient dropping has been shown to slow convergence.  ', 'We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.', 'We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.', ""We also show that gradient dropping with a local gradient update does not reduce the model's final quality.""]","[0, 0, 0, 0, 1]","[0.2916666567325592, 0.1463414579629898, 0.23076923191547394, 0.2857142686843872, 0.375]",BkeSusCcYm,"['We improve gradient dropping (a technique of only exchanging large gradients on distributed training) by incorporating local gradients while doing a parameter update to reduce quality loss and further improve the training time.', 'This paper proposes a 3 modes for combining local and global gradients to better use more computing nodes', 'Looks at the problem of reducing the communication requirement for implementing the distributed optimiztion techniques, particularly SGD']","['dataparallel neural network training networkintensive  gradient dropping designed exchange large gradient ', 'however  gradient dropping shown slow convergence ', 'propose improve convergence node combine locally computed gradient sparse global gradient exchanged network ', 'empirically confirm machine translation task gradient dropping local gradient approach convergence 48  faster noncompressed multinode training 28  faster compared vanilla gradient dropping ', 'also show gradient dropping local gradient update reduce model final quality ']","Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  , However, gradient dropping has been shown to slow convergence.  , We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network., We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping., We also show that gradient dropping with a local gradient update does not reduce the model's final quality.",7,6.105263157894737,13.571428571428571
272,"['    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.\n    ', 'In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.', 'This approach does not require counting and, therefore, generalizes well to the Deep RL.', 'We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.', ""This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration."", 'We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.', 'We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.', 'Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.', 'New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.260869562625885, 0.13793103396892548, 0.0952380895614624, 0.17391303181648254, 0.0, 0.1599999964237213, 0.0, 0.08695651590824127, 0.0]",S1fNJhRqFX,"['Exploration using Distributional RL and truncagted variance.', 'Presents an RL method to manage exploration-explotation trade-offs via UCB techniques.', 'A method to use the distribution learned by Quantile Regression DQN for exploration, in place of the usual epsilon-greedy strategy.', 'Proposes new algorithsms (QUCB and QUCB+) to handle the exploration tradeoff in Multi-Armed Bendits and more generally in Reinforcement Learning']","['establish relation distributional rl upper confidence bound  ucb  approach exploration ', 'paper show density q function estimated distributional rl successfully used estimation ucb ', 'approach require counting  therefore  generalizes well deep rl ', 'also point asymmetry empirical density estimated distributional rl algorithm like qrdqn ', 'observation lead reexamination variance performance ucb type approach exploration ', 'introduce truncated variance alternative estimator ucb novel algorithm based ', 'empirically show newly introduced algorithm achieves better performance multiarmed bandit setting ', 'finally  extend approach highdimensional setting test atari 2600 game ', 'new approach achieves better performance compared qrdqn 26 game  13 tie 49 game ']","    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.
    , In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB., This approach does not require counting and, therefore, generalizes well to the Deep RL., We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN., This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration., We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it., We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting., Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games., New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.",13,5.333333333333333,12.0
273,"['Good representations facilitate transfer learning and few-shot learning.', 'Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate.', 'Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate.', 'Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations.', 'Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.']","[0, 0, 0, 1, 0]","[0.1428571343421936, 0.30188679695129395, 0.04999999329447746, 0.5652173757553101, 0.1395348757505417]",HkzL4hR9Ym,"['Motivated by theories of language and communication, we introduce community-based autoencoders, in which multiple encoders and decoders collectively learn structured and reusable representations.', 'The authors tackle the problem of representation learning, aim to build reusable and structured represenation, argue co-adaptation between encoder and decoder in traditional AE yields poor representation, and introduce community based auto-encoders.', 'The paper presents a community based autoencoder framework to address co-adaptation of encoders and decoders and aims at constructing better representations.']","['good representation facilitate transfer learning fewshot learning ', 'motivated theory language communication explain community large number speaker  average  simpler language regularity  cast representation learning problem term learning communicate ', 'starting point see traditional autoencoders single encoder fixed decoder partner must learn communicate ', 'generalizing  introduce communitybased autoencoders multiple encoders decoder collectively learn representation randomly paired successive training iteration ', 'experiment show increasing community size reduce idiosyncrasy learned code  resulting invariant representation increased reusability structure ']","Good representations facilitate transfer learning and few-shot learning., Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate., Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate., Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations., Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.",10,6.464285714285714,11.2
274,"['Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.', 'Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.', 'Achieving these abilities in autonomous agents is an open problem.', 'In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.', 'MetaMimic can learn both', '(i) policies for high-fidelity one-shot imitation of diverse novel skills, and', '(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.', 'MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.', 'This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.\n', 'The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1621621549129486, 0.15789473056793213, 0.0624999962747097, 0.11428570747375488, 0.0, 0.3030303120613098, 0.11428570747375488, 0.08510638028383255, 0.25, 0.16326530277729034]",HJMjW3RqtX,"['We present MetaMimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy.', 'The paper looks at the problem of one-shot imitation with high accuracy of imitation, extending DDPGfD to use only state trajectories.', 'This paper proposes an approach for one-shot imitation with high accuracy, and addresses the common problem of exploration in imitation learning.', 'Presents an RL method for learning from video demonstration without access to expert actions']","['human expert highfidelity imitation  closely mimicking demonstration  often one attempt ', 'human use ability quickly solve task instance  bootstrap learning new task ', 'achieving ability autonomous agent open problem ', 'paper  introduce offpolicy rl algorithm  metamimic  narrow gap ', 'metamimic learn', '  policy highfidelity oneshot imitation diverse novel skill ', ' ii  policy enable agent solve task efficiently demonstrator ', 'metamimic relies principle storing experience memory replaying learn massive deep neural network policy offpolicy rl ', 'paper introduces  best knowledge  largest existing neural network deep rl show larger network normalization needed achieve oneshot highfidelity imitation challenging manipulation task ', 'result also show type policy learned vision  spite task reward sparse  without access demonstrator action ']","Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt., Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks., Achieving these abilities in autonomous agents is an open problem., In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap., MetaMimic can learn both, (i) policies for high-fidelity one-shot imitation of diverse novel skills, and, (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators., MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL., This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.
, The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.",18,5.373563218390805,9.666666666666666
275,"['Normalization methods are a central building block in the deep learning toolbox.', 'They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.', 'When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.', 'As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.', 'We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.']","[0, 0, 0, 0, 1]","[0.19354838132858276, 0.0, 0.1111111044883728, 0.16326530277729034, 0.25641024112701416]",HyN-M2Rctm,"['We present a novel normalization method for deep neural networks that is robust to multi-modalities in intermediate feature distributions.', 'Normalization method that learns multi-modal distribution in the feature space', 'Proposes a generalization of Batch Normalization under the assumption that the statistics of the unit activations over the batches and over the spatial dimensions is not unimodal']","['normalization method central building block deep learning toolbox ', 'accelerate stabilize training  decreasing dependence manually tuned learning rate schedule ', 'learning multimodal distribution  effectiveness batch normalization  bn   arguably prominent normalization method  reduced ', 'remedy  propose flexible approach  extending normalization single mean variance  detect mode data onthefly  jointly normalizing sample share common feature ', 'demonstrate method outperforms bn widely used normalization technique several experiment  including single multitask datasets ']","Normalization methods are a central building block in the deep learning toolbox., They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules., When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced., As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features., We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.",13,6.089108910891089,7.769230769230769
276,"['Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving.', 'However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations.', 'In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation.', 'Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation.', 'Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.', 'Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.']","[0, 0, 1, 0, 0, 0]","[0.19512194395065308, 0.2380952388048172, 0.5625, 0.2857142686843872, 0.25806450843811035, 0.0952380895614624]",S1gUsoR9YX,"['We proposed a knowledge distillation based method to boost the accuracy of multilingual neural machine translation.', 'A many-to-one multilingual neural machine translation model that first training separate models for each language pair then performs distillation.', 'The paper aims at training a machine translation model by augmenting the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models.']","['multilingual machine translation  translates multiple language single model  attracted much attention due efficiency offline training online serving ', 'however  traditional multilingual translation usually yield inferior accuracy compared counterpart using individual model language pair  due language diversity model capacity limitation ', 'paper  propose distillationbased approach boost accuracy multilingual machine translation ', 'specifically  individual model first trained regarded teacher  multilingual model trained fit training data match output individual model simultaneously knowledge distillation ', 'experiment iwslt  wmt ted talk translation datasets demonstrate effectiveness method ', 'particularly  show one model enough handle multiple language  44 language experiment   comparable even better accuracy individual model ']","Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving., However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations., In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation., Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation., Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method., Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",16,6.090277777777778,9.0
277,"['What makes humans so good at solving seemingly complex video games?  ', 'Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making.', 'This paper investigates the role of human priors for solving video games.', 'Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors.', 'We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors.', 'We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes.', 'Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.']","[0, 0, 0, 0, 0, 1, 0]","[0.0, 0.2790697515010834, 0.2702702581882477, 0.19512194395065308, 0.20408162474632263, 0.3333333134651184, 0.30434781312942505]",Hk91SGWR-,"['We investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay.', 'The authors study by experiment, what aspects of human priors are the important for reinforcement learning in video games.', 'The authors present a study of priors employed by humans in playing video games and demonstrates the existence of a taxonomy of features that affect the ability to complete tasks in the game to varying degrees.']","['make human good solving seemingly complex video game ', 'unlike computer  human bring great deal prior knowledge world  enabling efficient decision making ', 'paper investigates role human prior solving video game ', 'given sample game  conduct series ablation study quantify importance various prior ', 'modifying video game environment systematically mask different type visual information could used human prior ', 'find removal prior knowledge cause drastic degradation speed human player solve game  eg  2 minute 20 minute ', 'furthermore  result indicate general prior  importance object visual consistency  critical efficient gameplay ']","What makes humans so good at solving seemingly complex video games?  , Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making., This paper investigates the role of human priors for solving video games., Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors., We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors., We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes., Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.",14,5.162962962962963,9.0
278,"['Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \\emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.', 'Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.\n', 'In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.', 'Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  ', 'We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.', 'In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.', 'Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.']","[0, 0, 1, 0, 0, 0, 0]","[0.3529411852359772, 0.05714285373687744, 0.7567567229270935, 0.15789473056793213, 0.2222222238779068, 0.09090908616781235, 0.17777776718139648]",HyBbjW-RW,"['Driven by the need for parallelizable, open-loop hyperparameter optimization methods, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.', 'Proposes using the k-DPP to select candidate points in hyperparameter searches.', 'The authors propose k-DPP as an open loop method for hyperparameter optimization and provide its empirical study and comparison with other methods.', ""Considers non-sequential and uninformed hyperparameter search using determinantal point processes, which are probability distributions over subsets of a ground set with the property that subsets with more 'diverse' elements haev higher probability""]","['driven need parallelizable hyperparameter optimization method  paper study emph  open loop  search method  sequence predetermined generated single configuration evaluated ', 'example include grid search  uniform random search  low discrepancy sequence  sampling distribution ', 'particular  propose use  k  determinantal point process hyperparameter optimization via random search ', 'compared conventional uniform random search hyperparameter setting sampled independently   k  dpp promotes diversity ', 'describe approach transforms hyperparameter search space efficient use  k  dpp ', 'addition  introduce novel metropolishastings algorithm sample  k  dpps defined space mixture discrete continuous dimension ', 'experiment show significant benefit uniform random search realistic scenario limited budget training supervised learner  whether serial parallel ']","Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated., Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.
, In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search., Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  , We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP., In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions., Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.",15,6.4071428571428575,9.333333333333334
279,"['In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\n', 'When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\n', 'However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.\n', 'In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\n', 'We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.']","[1, 0, 0, 0, 0]","[0.9629629850387573, 0.07843136787414551, 0.10810810327529907, 0.060606054961681366, 0.045454543083906174]",rye7IMbAZ,"['In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.', 'Addresses the problem of transfer learning in deep networks and proposes to have a regularization term that penalizes divergence from initialization.', 'Proposes an analysis on different adaptive regularization techniques for deep transfer learning, specifically focusing on the use of an L@-SP condition']","['inductive transfer learning  finetuning pretrained convolutional network substantially outperforms training scratch ', 'using finetuning  underlying assumption pretrained model extract generic feature  least partially relevant solving target task  would difficult extract limited amount data available target task ', 'however  besides initialization pretrained model early stopping  mechanism finetuning retaining feature learned source task ', 'paper  investigate several regularization scheme explicitly promote similarity final solution initial model ', 'eventually recommend simple  l2  penalty using pretrained model reference  show approach behaves much better standard scheme using weight decay partially frozen network ']","In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.
, When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.
, However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.
, In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.
, We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.",13,5.753623188405797,10.615384615384615
280,"['Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem.', 'We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer.', 'These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups.', 'This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers.', 'Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.', 'This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.']","[0, 0, 0, 0, 1, 0]","[0.14999999105930328, 0.2857142686843872, 0.11428570747375488, 0.06451612710952759, 0.29411762952804565, 0.04999999701976776]",HyI5ro0pW,"['We look at neural networks with block diagonal inner product layers for efficiency.', 'This paper proposes making the inner layers in a neural network be block diagonal, and discusses that block diagonal matrices are more efficient than pruning and block diagonal layers lead to more efficient networks.', 'Replacing fully connected layers with block-diagonal fully connected layers']","['artificial neural network opened world possibility data science artificial intelligence  neural network cumbersome tool grow complexity learning problem ', 'make contribution issue considering modified version fully connected layer call block diagonal inner product layer ', 'modified layer weight matrix block diagonal  turning single fully connected layer set densely connected neuron group ', 'idea natural extension group  depthwise separable  convolutional layer applied fully connected layer ', 'block diagonal inner product layer achieved either initializing purely block diagonal weight matrix iteratively pruning diagonal block entry ', 'method condenses network storage speed run time without significant adverse effect testing accuracy  thus offering new approach improve network computation efficiency ']","Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem., We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer., These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups., This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers., Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries., This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.",11,5.532894736842105,13.818181818181818
281,"['One of the challenges in the study of generative adversarial networks is the instability of its training. \n', 'In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\n', 'Our new normalization technique is computationally light and easy to incorporate into existing implementations. \n', 'We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.']","[0, 1, 0, 0]","[0.19999998807907104, 0.800000011920929, 0.19354838132858276, 0.31372547149658203]",B1QRgziT-,"['We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.', 'This paper uses spectral regularization to normalize GAN objectives, and the ensuing GAN, called SN-GAN, essentially ensures the Lipschitz property of the discriminator.', 'This paper proposes""spectral normalization"", moving a nice step forward in improving the training of GANs.']","['one challenge study generative adversarial network instability training ', 'paper  propose novel weight normalization technique called spectral normalization stabilize training discriminator ', 'new normalization technique computationally light easy incorporate existing implementation ', 'tested efficacy spectral normalization cifar10  stl10  ilsvrc2012 dataset  experimentally confirmed spectrally normalized gans  sngans  capable generating image better equal quality relative previous training stabilization technique ']","One of the challenges in the study of generative adversarial networks is the instability of its training. 
, In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.
, Our new normalization technique is computationally light and easy to incorporate into existing implementations. 
, We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",8,6.144444444444445,11.25
282,"['Humans acquire complex skills by exploiting previously learned skills and making transitions between them.', 'To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.', 'To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.', 'The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.', 'We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.', 'The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.', 'We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .']","[0, 0, 0, 0, 1, 0, 0]","[0.29629629850387573, 0.19512194395065308, 0.0555555522441864, 0.04878048226237297, 0.48275861144065857, 0.0555555522441864, 0.06666666269302368]",rygrBhC5tQ,"['Transition policies enable agents to compose complex skills by smoothly connecting previously acquired primitive skills.', 'Proposes a scheme for transitioning to favorable strating states for executing given options in continuous domains. This uses two learning processes carried out simultaneously.', 'Presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks using state proximity estimator to reward for transition policy.', 'Proposes a new training scheme with a learned auxiliary reward function to optimise transition policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option']","['human acquire complex skill exploiting previously learned skill making transition ', 'empower machine ability  propose method learn transition policy effectively connect primitive skill perform sequential task without handcrafted reward ', 'efficiently train transition policy  introduce proximity predictor induce reward gauging proximity suitable initial state next skill ', 'proposed method evaluated set complex continuous control task bipedal locomotion robotic arm manipulation traditional policy gradient method struggle ', 'demonstrate transition policy enable u effectively compose complex skill existing primitive skill ', 'proposed induced reward computed using proximity predictor improve training efficiency providing dense information sparse reward environment ', 'make environment  primitive skill  code public research http  youngwoongithubiotransition ']","Humans acquire complex skills by exploiting previously learned skills and making transitions between them., To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards., To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill., The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at., We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills., The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments., We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .",11,6.340136054421769,13.363636363636363
283,"['Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.', 'Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.', 'As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.', 'In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\n', 'We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.', 'In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.', 'These findings were then experimentally verified in two dimensions by means of time series prediction.']","[0, 0, 0, 1, 0, 0, 0]","[0.07547169178724289, 0.178571417927742, 0.0, 0.37037035822868347, 0.1666666567325592, 0.20338982343673706, 0.3414634168148041]",H1eiZnAqKm,"['We classify the the dynamical features one and two GRU cells can and cannot capture in continuous time, and verify our findings experimentally with k-step time series prediction. ', 'The authors analyse GRUs with hidden sizes of one and two as continuous-time dynamical systems, claiming that the expressive power of the hidden state representation can provide prior knowledge on how well a GRU will perform on a given dataset', 'This paper analyzes GRUs from a dynamical systems perspective, and shows that 2d GRUs can be trained to adopt a variety of fixed points and can approximate line attractors, but cannot mimic a ring attractor.', 'Converts GRU equations into continuous time and uses theory and experiemnts to study 1- and 2-dimensional GRU networks and showcase every variety of dynamical topology available in these systems']","['gated recurrent unit  grus  inspired common gated recurrent unit  long shortterm memory  lstm   mean capturing temporal structure le complex memory unit architecture ', 'despite incredible success task natural artificial language processing  speech  video  polyphonic music  little understood specific dynamic feature representable gru network ', 'result  difficult know priori successful grurnn perform given data set ', 'paper  develop new theoretical framework analyze one two dimensional grus continuous dynamical system  classify dynamical feature obtainable system ', 'found rich repertoire includes stable limit cycle time  nonlinear oscillation   multistable state transition various topology  homoclinic orbit ', 'addition  show finite dimensional gru precisely replicate dynamic ring attractor  generally  continuous attractor  limited finitely many isolated fixed point theory ', 'finding experimentally verified two dimension mean time series prediction ']","Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture., Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network., As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set., In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.
, We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits., In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory., These findings were then experimentally verified in two dimensions by means of time series prediction.",22,5.554347826086956,8.363636363636363
284,"['Stacked hourglass network has become an important model for Human pose estimation.', 'The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location.', 'The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network.', 'In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  ', 'The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   ', 'And a new loss function is proposed for multi-scale stacked hourglass network.  ', 'Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  ', 'Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.']","[0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.1764705777168274, 0.1538461446762085, 0.1463414579629898, 0.17391303181648254, 0.06451612710952759, 0.19512194395065308, 0.10810810327529907]",HkM3vjCcF7,"['Differentiated inputs cause functional differentiation of the network, and the interaction of loss functions between networks can affect the optimization process.', 'A modification to the original hourglass network for single pose estimation that yields improvements over the original baseline.', 'Authors extend a stacked hourglass network with inception-resnet-A modules and propose a multi-scale approach for human pose estimation in still RGB images.']","['stacked hourglass network become important model human pose estimation ', 'estimation human body posture depends global information keypoints type local information keypoints location ', 'consistent processing input constraint make difficult form differentiated determined collaboration mechanism stacked hourglass network ', 'paper  propose multiscale stacked hourglass  mssh  network highlight differentiation capability hourglass network human pose estimation ', 'preprocessing network form feature map different scale  dispatch various location stack hourglass network  smallscale feature reach front stacked hourglass network  largescale feature reach rear stacked hourglass network ', 'new loss function proposed multiscale stacked hourglass network ', 'different keypoints different weight coefficient loss function different scale  keypoints weight coefficient dynamically adjusted toplevel hourglass network bottomlevel hourglass network ', 'experimental result show proposed method competitive respect comparison algorithm mpii lsp datasets ']","Stacked hourglass network has become an important model for Human pose estimation., The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location., The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network., In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  , The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   , And a new loss function is proposed for multi-scale stacked hourglass network.  , Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  , Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.",12,6.087431693989071,15.25
285,"['We present a new unsupervised method for learning general-purpose sentence embeddings.\n', 'Unlike existing methods which rely on local contexts, such as words\n', 'inside the sentence or immediately neighboring sentences, our method selects, for\n', 'each target sentence, influential sentences in the entire document based on a document\n', 'structure.', 'We identify a dependency structure of sentences using metadata\n', 'or text styles.', 'Furthermore, we propose a novel out-of-vocabulary word handling\n', 'technique to model many domain-specific terms, which were mostly discarded by\n', 'existing sentence embedding methods.', 'We validate our model on several tasks\n', 'showing 30% precision improvement in coreference resolution in a technical domain,\n', 'and 7.5% accuracy increase in paraphrase detection compared to baselines.']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.12121211737394333, 0.0624999962747097, 0.1249999925494194, 0.12121211737394333, 0.20000000298023224, 0.0, 0.13793103396892548, 0.0624999962747097, 0.1599999964237213, 0.0714285671710968, 0.12903225421905518, 0.1249999925494194]",H1a37GWCZ,"['To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words.', 'Presents ideas for improving sentence embedding by drawing from more context.', 'Learning sentence representations with sentences dependencies information', 'Extends the idea of forming an unsupervised representation of sentences used in the SkipThough approach by using a broader set of evidence for forming the representation of a sentence']","['present new unsupervised method learning generalpurpose sentence embeddings ', 'unlike existing method rely local context  word', 'inside sentence immediately neighboring sentence  method selects ', 'target sentence  influential sentence entire document based document', 'structure ', 'identify dependency structure sentence using metadata', 'text style ', 'furthermore  propose novel outofvocabulary word handling', 'technique model many domainspecific term  mostly discarded', 'existing sentence embedding method ', 'validate model several task', 'showing 30  precision improvement coreference resolution technical domain ', '75  accuracy increase paraphrase detection compared baseline ']","We present a new unsupervised method for learning general-purpose sentence embeddings.
, Unlike existing methods which rely on local contexts, such as words
, inside the sentence or immediately neighboring sentences, our method selects, for
, each target sentence, influential sentences in the entire document based on a document
, structure., We identify a dependency structure of sentences using metadata
, or text styles., Furthermore, we propose a novel out-of-vocabulary word handling
, technique to model many domain-specific terms, which were mostly discarded by
, existing sentence embedding methods., We validate our model on several tasks
, showing 30% precision improvement in coreference resolution in a technical domain,
, and 7.5% accuracy increase in paraphrase detection compared to baselines.",19,6.118181818181818,5.7894736842105265
286,"['Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions.', 'It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better.', 'However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.\n\n', 'In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.', 'First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions.', 'Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.']","[0, 0, 0, 1, 0, 0]","[0.1764705777168274, 0.08163265138864517, 0.277777761220932, 0.8717948794364929, 0.14999999105930328, 0.3589743673801422]",HkmaTz-0W,"['We explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.', 'This paper proposes a method to visualize the loss function of a NN and provides insights on the trainability and generalization of NNs.', 'Investigates the non-convexity of the loss surface and optimization paths.']","['neural network training relies ability find      good  minimizers highly nonconvex loss function ', 'well known certain network architecture design  eg  skip connection  produce loss function train easier  wellchosen training parameter  batch size  learning rate  optimizer  produce minimizers generalize better ', 'however  reason difference  effect underlying loss landscape  well understood ', 'paper  explore structure neural loss function  effect loss landscape generalization  using range visualization method ', 'first  introduce simple   filter normalization  method help u visualize loss function curvature  make meaningful sidebyside comparison loss function ', ' using variety visualization  explore network architecture effect loss landscape  training parameter affect shape minimizers ']","Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions., It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better., However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.

, In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods., First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions., Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.",21,6.0144927536231885,6.571428571428571
287,"['Deep models are state-of-the-art for many computer vision tasks including image classification and object detection.', 'However, it has been shown that deep models are vulnerable to adversarial examples.', 'We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping.', 'We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust.', 'Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks.', 'We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training.', 'The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.']","[0, 0, 0, 1, 0, 0, 0]","[0.04999999701976776, 0.2631579041481018, 0.13636362552642822, 0.7317073345184326, 0.19999998807907104, 0.07692307233810425, 0.08695651590824127]",B1xOYoA5tQ,"['We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks.', 'This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class.', 'Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines']","['deep model stateoftheart many computer vision task including image classification object detection ', 'however  shown deep model vulnerable adversarial example ', 'highlight onehot encoding directly contributes vulnerability propose breaking away widelyused  highlyvulnerable mapping ', 'demonstrate leveraging different output encoding  multiway encoding  make model robust ', 'approach make difficult adversary find useful gradient generating adversarial attack ', 'present stateoftheart robustness result blackbox  whitebox attack  achieve higher clean accuracy four benchmark datasets  mnist  cifar10  cifar100  svhn combined adversarial training ', 'strength approach also presented form attack model watermarking  raising challenge detecting stolen model ']","Deep models are state-of-the-art for many computer vision tasks including image classification and object detection., However, it has been shown that deep models are vulnerable to adversarial examples., We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping., We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust., Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks., We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training., The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.",17,6.128787878787879,7.764705882352941
288,"['Existing approaches to neural machine translation condition each output word on previously generated outputs.', 'We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.', 'Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.', 'We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 EnglishGerman and two WMT language pairs.', 'By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 EnglishRomanian.']","[0, 1, 0, 0, 0]","[0.0, 0.25641024112701416, 0.04081632196903229, 0.1463414579629898, 0.1621621549129486]",B1l8BtlCb,"['We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.', 'This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words', 'This paper describes an approach to decode non-autoregressively for neural machine translation with the possibility of more parallel decoding which can result in a significant speed-up.', 'Proposes the introduction of a set of latent variables to represent the fertility of each source word to make the target sentence generation non-autoregressive']","['existing approach neural machine translation condition output word previously generated output ', 'introduce model avoids autoregressive property produce output parallel  allowing order magnitude lower latency inference ', 'knowledge distillation  use input token fertility latent variable  policy gradient finetuning  achieve cost little 20 bleu point relative autoregressive transformer network used teacher ', 'demonstrate substantial cumulative improvement associated three aspect training strategy  validate approach iwslt 2016 englishgerman two wmt language pair ', 'sampling fertility parallel inference time  nonautoregressive model achieves nearstateoftheart performance 298 bleu wmt 2016 englishromanian ']","Existing approaches to neural machine translation condition each output word on previously generated outputs., We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference., Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher., We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 EnglishGerman and two WMT language pairs., By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 EnglishRomanian.",11,5.9453125,11.636363636363637
289,"['While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.', 'Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.', 'Can we somehow end this arms race?', 'In this work, we study this problem for neural networks with one hidden layer.', 'We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value.', 'Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.', 'On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n']","[0, 0, 0, 0, 1, 0, 0]","[0.05128204822540283, 0.11764705181121826, 0.0, 0.07692307233810425, 0.24390242993831635, 0.05405404791235924, 0.0952380895614624]",Bys4ob-Rb,"['We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.', 'Proposes a new defense against security attacks on neural networks with the atack model that outputs a security certificate on the algorithm.', 'Derives an upper bound on adversarial perturbation for neural networks with one hidden layer']","['neural network achieved high accuracy standard image classification benchmark  accuracy drop nearly zero presence small adversarial perturbation test input ', 'defense based regularization adversarial training proposed  often followed new  stronger attack defeat defense ', 'somehow end arm race ', 'work  study problem neural network one hidden layer ', 'first propose method based semidefinite relaxation output certificate given network test input  attack force error exceed certain value ', 'second  certificate differentiable  jointly optimize network parameter  providing adaptive regularizer encourages robustness attack ', 'mnist  approach produce network certificate perturbs pixel  epsilon  01  cause  35   test error ']","While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs., Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses., Can we somehow end this arms race?, In this work, we study this problem for neural networks with one hidden layer., We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value., Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks., On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\epsilon = 0.1$ can cause more than $35\%$ test error.
",16,5.282051282051282,9.75
290,"['We formulate an information-based optimization problem for supervised classification.', 'For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  ', 'We propose an objective function and prove that it solves the optimization problem.', 'Our framework allows us to learn latent features in an more interpretable form while improving the classification performance.', 'We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.']","[0, 0, 1, 0, 0]","[0.0952380895614624, 0.1860465109348297, 0.23999999463558197, 0.19999998807907104, 0.14814814925193787]",BJgvg30ctX,"['we propose a regularizer that improves the classification performance of neural networks', 'the authors propose to train a model from a point of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning.', 'Proposes to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer to maximize mutual information I(Y, \\hat{T}) while constraining irrelevant information']","['formulate informationbased optimization problem supervised classification ', 'invertible neural network  control information term passed latent feature parameter matrix last fully connected layer  given mutual information invariant invertible map ', 'propose objective function prove solves optimization problem ', 'framework allows u learn latent feature interpretable form improving classification performance ', 'perform extensive quantitative qualitative experiment comparison existing stateoftheart classification model ']","We formulate an information-based optimization problem for supervised classification., For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  , We propose an objective function and prove that it solves the optimization problem., Our framework allows us to learn latent features in an more interpretable form while improving the classification performance., We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.",7,6.322222222222222,12.857142857142858
291,"['Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.', 'These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.', 'We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).', 'This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.', 'Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.']","[0, 1, 0, 0, 0]","[0.17391303181648254, 0.25531914830207825, 0.11999999731779099, 0.1428571343421936, 0.20689654350280762]",BkMqUiA5KX,"['This paper introduces a novel generative modelling framework that avoids latent-variable collapse and clarifies the use of certain ad-hoc factors in training Variational Autoencoders.', 'The paper proposes to resolve the issue about a variational auto-encoder ignoring the latent variables.', ""This paper proposes adding a stochastic autoencoder to the original VAE model to address the problem that the LSTM decoder of a language model might be too strong to ignore the latent variable's information."", 'This paper presents AutoGen, which combines a generative variational autoencoder with a high-fidelity reconstruction model based on autoencoder to better utiliza latent representation']","['powerful generative model  particularly natural language modelling  commonly trained maximizing variational lower bound data log likelihood ', 'model often suffer poor use latent variable  adhoc annealing factor used encourage retention information latent variable ', 'discus alternative general approach latent variable modelling  based objective encourages perfect reconstruction tying stochastic autoencoder variational autoencoder  vae  ', 'ensures design latent variable capture information observation  whilst retaining ability generate well ', 'interestingly  although model fundamentally different vae  lower bound attained identical standard vae bound addition simple prefactor  thus  providing formal interpretation commonly used  adhoc prefactors training vaes ']","Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood., These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable., We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE)., This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well., Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.",14,5.764285714285714,10.0
292,"['This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.', 'This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.', 'Previous works only calibrate the condent prediction of classiers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).', 'In contrast, this paper proposes a probabilistic way of directly estimating and ne-tuning the decision boundary between seen and unseen classes.', 'In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.', 'Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the rst time, are introduced to uncover and ne-tune the decision boundary of each domain.', 'Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted condently.', 'Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.800000011920929, 0.3199999928474426, 0.09999999403953552, 0.21621620655059814, 0.1904761791229248, 0.14999999105930328, 0.1621621549129486, 0.0624999962747097]",H1GaLiAcY7,"[' This paper studies the problem of domain division by segmenting instances drawn from different probabilistic distributions.  ', 'This paper deals with the problem of novelty recognition in open set learning and generalized zero-shot learning and proposes a possible solution', 'An approach to domain separation based on bootstrapping to identify similarity cutoff thresholds for known classes, followed by a Kolmogorov-Smirnoff test to refine the bootstrapped in-distribution zones.', 'Proposes to introduce a new domain, the uncertain domain, to better handle the division between seen/unseen domains in open-set and generalized zero-shot learning']","['paper study problem domain division aim segment instance drawn different probabilistic distribution ', 'problem exists many previous recognition task  open set learning  osl  generalized zeroshot learning  gzsl   testing instance come either seen unseennovel class different probabilistic distribution ', 'previous work calibrate condent prediction classiers seen class  wsvm scheirer et al   2014   taking unseen class outlier socher et al   2013  ', 'contrast  paper proposes probabilistic way directly estimating netuning decision boundary seen unseen class ', 'particular  propose domain division algorithm split testing instance known  unknown uncertain domain  conduct recognition task domain ', 'two statistical tool  namely  bootstrapping kolmogorovsmirnov  k  test  rst time  introduced uncover netune decision boundary domain ', 'critically  uncertain domain newly introduced framework adopt instance whose domain label predicted condently ', 'extensive experiment demonstrate approach achieved stateoftheart performance osl gzsl benchmark ']","This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions., This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions., Previous works only calibrate the condent prediction of classiers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013)., In contrast, this paper proposes a probabilistic way of directly estimating and ne-tuning the decision boundary between seen and unseen classes., In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain., Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the rst time, are introduced to uncover and ne-tune the decision boundary of each domain., Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted condently., Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.",19,5.98936170212766,8.952380952380953
293,"['Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.', 'We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.', 'This potential is however not the original loss function in general.', 'So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.', 'Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.', 'Instead, they resemble closed loops with deterministic components.', 'We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.', 'We provide extensive empirical validation of these claims, proven in the appendix.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.1538461446762085, 0.1463414579629898, 0.3030303120613098, 0.29999998211860657, 0.3461538553237915, 0.0, 0.22641508281230927, 0.11764705181121826]",HyWrIgW0W,"['SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss', 'This paper provides a variational analysis of SGD as a non-equilibrium process.', 'This paper discusses the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation.', 'Develops a theory to study the impact of stocastic gradient noise for SGD, especially for deep neural network models']","['stochastic gradient descent  sgd  widely believed perform implicit regularization used train deep neural network  precise manner occurs thus far elusive ', 'prove sgd minimizes average potential posterior distribution weight along entropic regularization term ', 'potential however original loss function general ', 'sgd perform variational inference  different loss one used compute gradient ', 'even surprisingly  sgd even converge classical sense  show likely trajectory sgd deep network behave like brownian motion around critical point ', 'instead  resemble closed loop deterministic component ', 'prove outofequilibrium behavior consequence highly nonisotropic gradient noise sgd  covariance matrix minibatch gradient deep network rank small 1  dimension ', 'provide extensive empirical validation claim  proven appendix ']","Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive., We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term., This potential is however not the original loss function in general., So SGD does perform variational inference, but for a different loss than the one used to compute the gradients., Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points., Instead, they resemble closed loops with deterministic components., We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension., We provide extensive empirical validation of these claims, proven in the appendix.",13,5.402366863905326,13.0
294,"[""The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate."", 'We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.', 'In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.', 'The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.', ""Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference."", 'We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.', 'Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.', 'Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.260869562625885, 0.145454540848732, 0.09090908616781235, 0.11999999731779099, 0.1249999925494194, 0.0, 0.07692307233810425, 0.0555555522441864]",BkisuzWRW,"['Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time.', 'This paper proposes and approach for zero-shot visual learning by learning parametric skill functions.', 'A paper about imitation of a task presented just during inference, where learning is performed in a self-supervised manner and during training the agent explores related but different tasks.', 'Proposes a method for sidestepping the issue of expensive expert demonstration by using the random exploration of an agent to learn generalizable skills which can be applied without specific pretraining']","['current dominant paradigm imitation learning relies strong supervision expert action learn   imitate ', 'pursue alternative paradigm wherein agent first explores world without expert supervision distills experience goalconditioned skill policy novel forward consistency loss ', 'framework  role expert communicate goal  ie  imitate  inference ', 'learned policy employed mimic expert  ie  imitate  seeing sequence image demonstrating desired task ', 'method zeroshot  sense agent never access expert action training task demonstration inference ', 'evaluate zeroshot imitator two realworld setting  complex rope manipulation baxter robot navigation previously unseen office environment turtlebot ', 'experiment vizdoom simulation  provide evidence better mechanism exploration lead learning capable policy turn improves end task performance ', 'video  model  detail available http  pathak22githubiozeroshotimitation ']","The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate., We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss., In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference., The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task., Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference., We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot., Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance., Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.",14,5.647058823529412,13.357142857142858
295,"['Distributional Semantics Models(DSM) derive word space from linguistic items\n', 'in context.', 'Meaning is obtained by defining a distance measure between vectors\n', 'corresponding to lexical entities.', 'Such vectors present several problems.', 'This\n', 'work concentrates on quality of word embeddings, improvement of word embedding\n', 'vectors, applicability of a novel similarity metric used on top of the\n', 'word embeddings.', 'In this paper we provide comparison between two methods\n', 'for post process improvements to the baseline DSM vectors.', 'The counter-fitting\n', 'method which enforces antonymy and synonymy constraints into the Paragram\n', 'vector space representations recently showed improvement in the vectors capability\n', 'for judging semantic similarity.', 'The second method is our novel RESM\n', 'method applied to GloVe baseline vectors.', 'By applying the hubness reduction\n', 'method, implementing relational knowledge into the model by retrofitting synonyms\n', 'and providing a new ranking similarity definition RESM that gives maximum\n', 'weight to the top vector component values we equal the results for the ESL\n', 'and TOEFL sets in comparison with our calculations using the Paragram and Paragram\n', '+ Counter-fitting methods.', 'For SIMLEX-999 gold standard since we cannot\n', 'use the RESM the results using GloVe and PPDB are significantly worse compared\n', 'to Paragram.', 'Apparently, counter-fitting corrects hubness.', 'The Paragram\n', 'or our cosine retrofitting method are state-of-the-art results for the SIMLEX-999\n', 'gold standard.', 'They are 0.2 better for SIMLEX-999 than word2vec with sense\n', 'de-conflation (that was announced to be state-of the-art method for less reliable\n', 'gold standards).', 'Apparently relational knowledge and counter-fitting is more important\n', 'for judging semantic similarity than sense determination for words.', 'It is to\n', 'be mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999\n', 'results.', 'The lesson is that many corrections to word embeddings are necessary\n', 'and methods with more parameters and hyperparameters perform better.\n']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.12903225421905518, 0.0624999962747097, 0.07692307233810425, 0.0, 0.12903225421905518, 0.12121211737394333, 0.0, 0.12903225421905518, 0.1249999925494194, 0.1249999925494194, 0.07692307233810425, 0.0, 0.1428571343421936, 0.0, 0.0624999962747097, 0.12121211737394333, 0.1764705777168274, 0.1818181723356247, 0.0, 0.0, 0.11764705181121826, 0.0, 0.060606054961681366, 0.12121211737394333, 0.11764705181121826, 0.06666666269302368, 0.06666666269302368, 0.07999999821186066, 0.1249999925494194, 0.12121211737394333, 0.12903225421905518]",HyHmGyZCZ,"['Paper provides a description of a procedure to enhance word vector space model with an evaluation of Paragram and GloVe models for Similarity Benchmarks.', 'This paper suggests a new algorithm that adjusts GloVe word vectors and then uses a non-Euclidean similarity function between them.', 'The authors present observations on the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors']","['distributional semantics model  dsm  derive word space linguistic item', 'context ', 'meaning obtained defining distance measure vector', 'corresponding lexical entity ', 'vector present several problem ', '', 'work concentrate quality word embeddings  improvement word embedding', 'vector  applicability novel similarity metric used  top ', 'word embeddings ', 'paper provide comparison two method', 'post process improvement baseline dsm vector ', 'counterfitting', 'method enforces antonymy synonymy constraint paragram', 'vector space representation recently showed improvement vector  capability', 'judging semantic similarity ', 'second method novel resm', 'method applied glove baseline vector ', 'applying hubness reduction', 'method  implementing relational knowledge model retrofitting synonym', 'providing new ranking similarity definition resm give maximum', 'weight top vector component value equal result esl', 'toefl set comparison calculation using paragram paragram', ' counterfitting method ', 'simlex999 gold standard since', 'use resm result using glove ppdb significantly worse compared', 'paragram ', 'apparently  counterfitting corrects hubness ', 'paragram', 'cosine retrofitting method stateoftheart result simlex999', 'gold standard ', '02 better simlex999 word2vec sense', 'deconflation  announced stateof theart method le reliable', 'gold standard  ', 'apparently relational knowledge counterfitting important', 'judging semantic similarity sense determination word ', '', 'mentioned  though paragram hyperparameters fitted simlex999', 'result ', 'lesson many correction word embeddings necessary', 'method parameter hyperparameters perform better ']","Distributional Semantics Models(DSM) derive word space from linguistic items
, in context., Meaning is obtained by defining a distance measure between vectors
, corresponding to lexical entities., Such vectors present several problems., This
, work concentrates on quality of word embeddings, improvement of word embedding
, vectors, applicability of a novel similarity metric used on top of the
, word embeddings., In this paper we provide comparison between two methods
, for post process improvements to the baseline DSM vectors., The counter-fitting
, method which enforces antonymy and synonymy constraints into the Paragram
, vector space representations recently showed improvement in the vectors capability
, for judging semantic similarity., The second method is our novel RESM
, method applied to GloVe baseline vectors., By applying the hubness reduction
, method, implementing relational knowledge into the model by retrofitting synonyms
, and providing a new ranking similarity definition RESM that gives maximum
, weight to the top vector component values we equal the results for the ESL
, and TOEFL sets in comparison with our calculations using the Paragram and Paragram
, + Counter-fitting methods., For SIMLEX-999 gold standard since we cannot
, use the RESM the results using GloVe and PPDB are significantly worse compared
, to Paragram., Apparently, counter-fitting corrects hubness., The Paragram
, or our cosine retrofitting method are state-of-the-art results for the SIMLEX-999
, gold standard., They are 0.2 better for SIMLEX-999 than word2vec with sense
, de-conflation (that was announced to be state-of the-art method for less reliable
, gold standards)., Apparently relational knowledge and counter-fitting is more important
, for judging semantic similarity than sense determination for words., It is to
, be mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999
, results., The lesson is that many corrections to word embeddings are necessary
, and methods with more parameters and hyperparameters perform better.
",45,6.017543859649122,6.333333333333333
296,"['Recurrent neural networks have achieved excellent performance in many applications.', 'However, on portable devices with limited resources, the models are often too large to deploy.', 'For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.', 'In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem.', 'Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  ', 'We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.', 'Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy.', 'By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration.', 'Both results beat the exiting quantization works with large margins.  ', 'We extend our alternating quantization to image classification tasks.', 'In both RNNs and feedforward neural networks, the method also achieves  excellent performance.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.0, 0.06451612710952759, 0.052631575614213943, 0.1860465109348297, 0.04878048226237297, 0.19999998807907104, 0.1818181723356247, 0.09302324801683426, 0.07407406717538834, 0.23999999463558197, 0.27586206793785095]",S19dR9x0b,"['We propose a  new  quantization method and apply it to quantize RNNs for both compression and acceleration', 'This paper proposes a multi-bit quantization method for recurrent neural networks.', 'A technique for quantizing neural network weight matrices, and an alternating optimization procedure to estimate the set of k binary vectors and coefficients that best represent the original vector.']","['recurrent neural network achieved excellent performance many application ', 'however  portable device limited resource  model often large deploy ', 'application server large scale concurrent request  latency inference also critical costly computing resource ', 'work  address problem quantizing network  weight activation  multiple binary code  1  1   formulate quantization optimization problem ', 'key observation quantization coefficient fixed binary code derived efficiently binary search tree  alternating minimization applied ', 'test quantization two wellknown rnns  ie  long short term memory  lstm  gated recurrent unit  gru   language model ', 'compared fullprecision counter part  2bit quantization achieve 16x memory saving 6x real inference acceleration cpu  reasonable loss accuracy ', '3bit quantization  achieve almost loss accuracy even surpass original model  105x memory saving 3x real inference acceleration ', 'result beat exiting quantization work large margin ', 'extend alternating quantization image classification task ', 'rnns feedforward neural network  method also achieves excellent performance ']","Recurrent neural networks have achieved excellent performance in many applications., However, on portable devices with limited resources, the models are often too large to deploy., For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources., In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem., Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  , We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models., Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy., By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration., Both results beat the exiting quantization works with large margins.  , We extend our alternating quantization to image classification tasks., In both RNNs and feedforward neural networks, the method also achieves  excellent performance.",26,5.660465116279069,7.962962962962963
297,"['The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).', 'We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.', 'The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.', 'We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n']","[0, 0, 0, 1]","[0.18518517911434174, 0.27272728085517883, 0.0952380895614624, 0.36734694242477417]",rkGZuJb0b,"['We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. ', 'In the paper the authors suggest to use MERA tensorization technique for compressing neural networks.', 'A new parameterization of linear maps for neural network use, using a hierarchical factorization of the linear map that reduces the number of parameters while still allowing for relatively complex interactions to be modelled.', 'Studies compressing feed forward layers using low rank tensor decompositions and explore a tree like decomposition']","['goal paper demonstrate method tensorizing neural network based upon efficient way approximating scale invariant quantum state  multiscale entanglement renormalization ansatz  mera  ', 'employ mera replacement linear layer neural network test implementation cifar10 dataset ', 'proposed method outperforms factorization using tensor train  providing greater compression level accuracy greater accuracy level compression ', 'demonstrate meralayers 3900 time fewer parameter reduction accuracy le 1  compared equivalent fully connected layer ']","The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA)., We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset., The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression., We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.
",6,5.666666666666667,17.0
298,"['Deep learning models have outperformed traditional methods in many fields such\n', 'as natural language processing and computer vision.', 'However, despite their\n', 'tremendous success, the methods of designing optimal Convolutional Neural Networks\n', '(CNNs) are still based on heuristics or grid search.', 'The resulting networks\n', 'obtained using these techniques are often overparametrized with huge computational\n', 'and memory requirements.', 'This paper focuses on a structured, explainable\n', 'approach towards optimal model design that maximizes accuracy while keeping\n', 'computational costs tractable.', 'We propose a single-shot analysis of a trained CNN\n', 'that uses Principal Component Analysis (PCA) to determine the number of filters\n', 'that are doing significant transformations per layer, without the need for retraining.\n', 'It can be interpreted as identifying the dimensionality of the hypothesis space\n', 'under consideration.', 'The proposed technique also helps estimate an optimal number\n', 'of layers by looking at the expansion of dimensions as the model gets deeper.\n', 'This analysis can be used to design an optimal structure of a given network on\n', 'a dataset, or help to adapt a predesigned network on a new dataset.', 'We demonstrate\n', 'these techniques by optimizing VGG and AlexNet networks on CIFAR-10,\n', 'CIFAR-100 and ImageNet datasets.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.0, 0.0833333283662796, 0.0, 0.14814814925193787, 0.0, 0.0, 0.0, 0.09999999403953552, 0.0833333283662796, 0.07407406717538834, 0.0, 0.4000000059604645, 0.13793103396892548, 0.0, 0.0714285671710968, 0.07692307233810425, 0.06666666269302368, 0.4375, 0.2142857164144516, 0.07407406717538834, 0.0952380895614624]",SJgzJh0qtQ,"['We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure', 'This paper proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset', 'This paper presents a framework for optimising neural networks architectures through the identification of redundant filters across layers']","['deep learning model outperformed traditional method many field', 'natural language processing computer vision ', 'however  despite', 'tremendous success  method designing optimal convolutional neural network', ' cnns  still based heuristic grid search ', 'resulting network', 'obtained using technique often overparametrized huge computational', 'memory requirement ', 'paper focus structured  explainable', 'approach towards optimal model design maximizes accuracy keeping', 'computational cost tractable ', 'propose singleshot analysis trained cnn', 'us principal component analysis  pca  determine number filter', 'significant transformation per layer  without need retraining ', 'interpreted identifying dimensionality hypothesis space', 'consideration ', 'proposed technique also help estimate optimal number', 'layer looking expansion dimension model get deeper ', 'analysis used design optimal structure given network', 'dataset  help adapt predesigned network new dataset ', 'demonstrate', 'technique optimizing vgg alexnet network cifar10 ', 'cifar100 imagenet datasets ']","Deep learning models have outperformed traditional methods in many fields such
, as natural language processing and computer vision., However, despite their
, tremendous success, the methods of designing optimal Convolutional Neural Networks
, (CNNs) are still based on heuristics or grid search., The resulting networks
, obtained using these techniques are often overparametrized with huge computational
, and memory requirements., This paper focuses on a structured, explainable
, approach towards optimal model design that maximizes accuracy while keeping
, computational costs tractable., We propose a single-shot analysis of a trained CNN
, that uses Principal Component Analysis (PCA) to determine the number of filters
, that are doing significant transformations per layer, without the need for retraining.
, It can be interpreted as identifying the dimensionality of the hypothesis space
, under consideration., The proposed technique also helps estimate an optimal number
, of layers by looking at the expansion of dimensions as the model gets deeper.
, This analysis can be used to design an optimal structure of a given network on
, a dataset, or help to adapt a predesigned network on a new dataset., We demonstrate
, these techniques by optimizing VGG and AlexNet networks on CIFAR-10,
, CIFAR-100 and ImageNet datasets.",28,5.8052631578947365,6.785714285714286
299,"['Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversarys capability to conduct attacks on black-box networks.', 'This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  ', 'First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  ', 'Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique.', 'Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victims entire network architecture.  ', 'In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation.', 'Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting.', 'From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process.', 'Third, we propose and evaluate new framework-level defense techniques that obfuscate our attackers observations.', 'Our empirical security analysis represents a step toward understanding the DNNs vulnerability to cache side-channel attacks.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.23529411852359772, 0.5714285373687744, 0.11267605423927307, 0.2448979616165161, 0.17391303181648254, 0.0416666604578495, 0.1538461446762085, 0.15789473056793213, 0.05128204822540283, 0.6341463327407837]",rk4Wf30qKQ,"['We conduct the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels, which represents a step toward understanding the DNNs vulnerability to side-channel attacks.', 'This paper considers the problem of fingerprinting neural network architectures using cache side channels, and discusses security-through-obscurity defenses.', 'This paper performs cache side-channel attacks to extract attributes of a victim model and infer its architecture, as well as show they can achieve a nearly perfect classification accuracy.']","['recent work introduced attack extract architecture information deep neural network  dnn   knowledge enhances adversary  capability conduct attack blackbox network ', 'paper present first indepth security analysis dnn fingerprinting attack exploit cache sidechannels ', 'first  define threat model attack  adversary need ability query victim model  instead  run colocated process host machine victim  deep learning  dl  system running passively monitor access target function shared framework ', 'second  introduce deeprecon  attack reconstructs architecture victim network using internal information extracted via flushreload  cache sidechannel technique ', 'attacker observes function invocation map directly architecture attribute victim network  attacker reconstruct victim  entire network architecture ', 'evaluation  demonstrate attacker accurately reconstruct two complex network  vgg19 resnet50  observed one forward propagation ', 'based extracted architecture attribute  also demonstrate attacker build metamodel accurately fingerprint architecture family pretrained model transfer learning setting ', 'metamodel  evaluate importance observed attribute fingerprinting process ', 'third  propose evaluate new frameworklevel defense technique obfuscate attacker  observation ', 'empirical security analysis represents step toward understanding dnns  vulnerability cache sidechannel attack ']","Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversarys capability to conduct attacks on black-box networks., This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  , First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  , Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique., Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victims entire network architecture.  , In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation., Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting., From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process., Third, we propose and evaluate new framework-level defense techniques that obfuscate our attackers observations., Our empirical security analysis represents a step toward understanding the DNNs vulnerability to cache side-channel attacks.",21,6.040485829959514,11.761904761904763
300,"['Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.', 'Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.', 'We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.', 'This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.', 'We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.', 'The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.', 'In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.\n']","[0, 0, 0, 1, 0, 0, 0]","[0.3265306055545807, 0.22641508281230927, 0.2745097875595093, 0.3478260934352875, 0.04999999701976776, 0.19230768084526062, 0.2800000011920929]",HyM7AiA5YX,"['We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.', 'Considers augmenting the cross-entropy objective with ""complement"" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth labels.', 'The authors propose a secondary objective for softmax minimization based on evaluating the information gathered from the incorrect classes, leading to a new training approach.', 'Deals with the training of neural networks for classification or sequence generation tasks using across-entropy loss']","['learning primary objective  softmax cross entropy classification sequence generation  norm training deep neural network year ', 'although widelyadopted approach  using cross entropy primary objective exploit mostly information groundtruth class maximizing data likelihood  largely ignores information complement  incorrect  class ', 'argue  addition primary objective  training also using complement objective leverage information complement class effective improving model performance ', 'motivates u study new training paradigm maximizes likelihood groundtruth class neutralizing probability complement class ', 'conduct extensive experiment multiple task ranging computer vision natural language understanding ', 'experimental result confirm  compared conventional training one primary objective  training also complement objective improves performance stateoftheart model across task ', 'addition accuracy improvement  also show model trained primary complement objective robust singlestep adversarial attack ']","Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years., Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes., We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance., This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes., We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding., The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks., In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.
",16,6.064864864864865,11.5625
301,"['We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output.', 'We extend softmax layer with an additional constant input.', 'The corresponding additional output is able to represent the uncertainty of the network.', 'The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets.', 'We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.']","[0, 0, 0, 1, 0]","[0.2142857164144516, 0.09999999403953552, 0.08695651590824127, 0.23076923191547394, 0.0]",rJxA-h05KQ,"['Uncertainty estimation in a single forward pass without additional learnable parameters.', 'A new method for computing output uncertainty estimates in DNNs for classification problems that matches state-of-the-art methods for uncertainty estimation and outperforms them in out-of-distribution detection tasks.', 'The authors present inhibited softmax, a modification of the softmax through adding a constant activation which provides a measure for uncertainty. ']","['present new method uncertainty estimation outofdistribution detection neural network softmax output ', 'extend softmax layer additional constant input ', 'corresponding additional output able represent uncertainty network ', 'proposed method requires neither additional parameter multiple forward pass input preprocessing outofdistribution datasets ', 'show method performs comparably computationally expensive method outperforms baseline experiment image recognition sentiment analysis domain ']","We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output., We extend softmax layer with an additional constant input., The corresponding additional output is able to represent the uncertainty of the network., The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets., We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.",5,6.45679012345679,16.2
302,"['When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise.', 'These issues are especially evident in the healthcare, finance, law and government industries.', 'Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption.', 'This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.', 'As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST.', 'Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.\n\n']","[0, 0, 0, 1, 0, 0]","[0.13793103396892548, 0.0, 0.20000000298023224, 0.25806450843811035, 0.10256409645080566, 0.060606054961681366]",ByCPHrgCW,"['We made a feature-rich system for deep learning with encrypted inputs, producing encrypted outputs, preserving privacy.', 'A framework for private deep learning model inference using FHE schemes that support fast bootstrapping and thus can reduce computation time.', 'The paper presents a means of evaluating a neural network securely using homomorphic encryption.']","['deep learning applied sensitive data set  many privacyrelated implementation issue arise ', 'issue especially evident healthcare  finance  law government industry ', 'homomorphic encryption could allow server make inference input encrypted client  best knowledge  complete implementation common deep learning operation  arbitrary model depth  using homomorphic encryption ', 'paper demonstrates novel approach  efficiently implementing many deep learning function bootstrapped homomorphic encryption ', 'part implementation  demonstrate single multilayer neural network  wisconsin breast cancer dataset  well convolutional neural network mnist ', 'result give promising direction privacypreserving representation learning  return data control user ']","When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise., These issues are especially evident in the healthcare, finance, law and government industries., Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption., This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption., As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST., Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.

",18,6.128,6.944444444444445
303,"['In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.', 'Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner.', 'Hence, they provide an opportunity to explore theorem proving with human supervision.', 'We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem.', 'We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.']","[1, 0, 0, 0, 0]","[0.8085106611251831, 0.277777761220932, 0.25, 0.3414634168148041, 0.2978723347187042]",r1xwKoR9Y7,"['We introduce a system called GamePad to explore the application of machine learning methods to theorem proving in the Coq proof assistant.', 'This paper describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an SVM on both tasks.', 'Proposes that machine learning techniques be used to help build proof in the theorem prover Coq.']","['paper  introduce system called gamepad used explore application machine learning method theorem proving coq proof assistant ', 'interactive theorem provers coq enable user construct machinecheckable proof stepbystep manner ', 'hence  provide opportunity explore theorem proving human supervision ', 'use gamepad synthesize proof simple algebraic rewrite problem train baseline model formalization feitthompson theorem ', 'address position evaluation  ie  predict number proof step left  tactic prediction  ie  predict next proof step  task  arise naturally tacticbased theorem proving ']","In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant., Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner., Hence, they provide an opportunity to explore theorem proving with human supervision., We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem., We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.",10,5.587155963302752,10.9
304,"['Deep neural networks are usually huge, which significantly limits the deployment on low-end devices.', 'In recent years, many\n', 'weight-quantized models have  been proposed.', 'They have small storage and fast inference, but training can still be time-consuming.', 'This can be improved with distributed learning.', 'To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. \n', 'In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.\n', 'We show  that', '(i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;', '(ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and', '(iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization.', 'Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.0555555522441864, 0.07692307233810425, 0.07407407462596893, 0.11428570747375488, 0.13793103396892548, 0.1304347813129425, 0.44999998807907104, 0.0, 0.1111111044883728, 0.15789473056793213, 0.1538461446762085, 0.1860465109348297]",ryM_IoAqYX,"['In this paper, we studied efficient training of loss-aware weight-quantized  networks with  quantized gradient  in a distributed environment, both theoretically and empirically.', 'This paper studies convergence properties of loss-aware weight quantization with different gradient precisions in the distributed environment, and provides convergence analysis for weight quantization with full-precision, quantized and quantized clipped gradients.', 'The authors proposes an analysis of the effect of simultaneously quantizing the weights and gradients in training a parametrized model in a fully-synchronized distributed environment.']","['deep neural network usually huge  significantly limit deployment lowend device ', 'recent year  many', 'weightquantized model proposed ', 'small storage fast inference  training still timeconsuming ', 'improved distributed learning ', 'reduce high communication cost due workerserver synchronization  recently gradient quantization also proposed train deep network fullprecision weight ', 'paper  theoretically study combination weight gradient quantization affect convergence ', 'show', '  weightquantized model converge error related weight quantization resolution weight dimension ', ' ii  quantizing gradient slows convergence factor related gradient quantization resolution dimension ', ' iii  clipping gradient quantization render factor dimensionfree  thus allowing use fewer bit gradient quantization ', 'empirical experiment confirm theoretical convergence result  demonstrate quantized network speed training comparable performance fullprecision network ']","Deep neural networks are usually huge, which significantly limits the deployment on low-end devices., In recent years, many
, weight-quantized models have  been proposed., They have small storage and fast inference, but training can still be time-consuming., This can be improved with distributed learning., To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. 
, In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.
, We show  that, (i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;, (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and, (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization., Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.",19,6.269938650306749,8.578947368421053
305,"['Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task.', 'In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them.', 'To achieve Selfless Sequential Learning we study different regularization strategies and activation functions.', 'We find that\n', 'imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity.', 'In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition.', 'It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks.', 'As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,\n', 'our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain.', 'We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network.', 'We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.1875, 0.1304347813129425, 0.08695651590824127, 0.0, 0.3333333432674408, 0.07692307233810425, 0.0, 0.0714285671710968, 0.07407406717538834, 0.19354838132858276, 0.06896550953388214]",Bkxbrn0cYX,"['A regularization strategy for improving the performance of sequential learning', 'A novel, regularization based approach to the sequential learning problem using a fixed size model that adds extra terms to the loss, encouraging representation sparsity and combating catastrophic forgetting.', 'This paper deals with the problem of catastrophic forgetting in lifelong learning by proposing regularized learning strategies']","['sequential learning  also called lifelong learning  study problem learning task sequence access restricted data current task ', 'paper look scenario fixed model capacity  postulate learning process selfish  ie  account future task added thus leave enough capacity ', 'achieve selfless sequential learning study different regularization strategy activation function ', 'find', 'imposing sparsity level representation  ie  neuron activation  beneficial sequential learning encouraging parameter sparsity ', 'particular  propose novel regularizer  encourages representation sparsity mean neural inhibition ', 'result active neuron turn leaf free neuron utilized upcoming task ', 'neural inhibition entire layer drastic  especially complex task requiring strong representation ', 'regularizer inhibits neuron local neighbourhood  inspired lateral inhibition process brain ', 'combine novel regularizer stateoftheart lifelong learning method penalize change important previously learned part network ', 'show new regularizer lead increased sparsity translates consistent performance improvement diverse datasets ']","Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task., In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them., To achieve Selfless Sequential Learning we study different regularization strategies and activation functions., We find that
, imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity., In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition., It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks., As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,
, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain., We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network., We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.",19,5.699530516431925,10.142857142857142
306,"['A Synaptic Neural Network (SynaNN) consists of synapses and neurons.', 'Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.', 'Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space.', 'Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution.', 'In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network.', 'Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms.', 'In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.']","[0, 0, 0, 1, 0, 0, 0]","[0.1875, 0.24390242993831635, 0.30434781312942505, 0.4390243887901306, 0.31578946113586426, 0.2666666507720947, 0.2380952388048172]",ryGpEiAcFQ,"['A synaptic neural network with synapse graph and learning that has the feature of topological conjugation and Bose-Einstein distribution in surprisal space.  ', 'The authors propose a hybrid neural nework composed of a synapse graph that can be embedded into a standard neural network', 'Presents a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells']","['synaptic neural network  synann  consists synapsis neuron ', 'inspired synapse research neuroscience  built synapse model nonlinear synapse function excitatory inhibitory channel probability ', 'introduced concept surprisal space constructed commutative diagram  proved inhibitory probability function log  1exp  x   surprisal space topologically conjugate function inhibitory complementary probability 1x probability space ', 'furthermore  found derivative synapse parameter surprisal space equal negative boseeinstein distribution ', 'addition  constructed fully connected synapse graph  tensor  synapse block synaptic neural network ', 'moreover  proved gradient formula crossentropy loss function parameter  synapse learning work gradient descent backpropagation algorithm ', 'proofofconcept experiment  performed mnist training testing mlp model synapse network hidden layer ']","A Synaptic Neural Network (SynaNN) consists of synapses and neurons., Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities., Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space., Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution., In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network., Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms., In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.",14,5.898089171974522,11.214285714285714
307,"['Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.', 'Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.', 'In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.', 'Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.']","[1, 0, 0, 0]","[0.0, 0.0, 0.0, 0.0]",SJd0EAy0b,"['Generalized Graph Embedding Models', 'A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches.\n\n', 'Tackles the task of learning embeddings of multi-relational graphs using a neural network', 'Proposes a new method, GEN, to compute embeddings of multirelationship graphs, particularly that so-called E-Cells and R-Cells can answer queries of the form (h,r,?),(?r,t), and (h,?,t)']","['many type relation physical  biological  social information system modeled homogeneous heterogeneous concept graph ', 'hence  learning graph embeddings drawn great deal research interest recently  ad hoc solution obtained far ', 'paper  conjecture oneshot supervised learning mechanism bottleneck improving performance graph embedding learning algorithm  propose extend introducing multishot unsupervised learning framework ', 'empirical result several realworld data set show proposed model consistently significantly outperforms existing stateoftheart approach knowledge base completion graph based multilabel classification task ']","Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs., Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far., In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework., Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.",10,6.063063063063063,11.1
308,"['We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning.', 'The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages.', 'At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset.', 'MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement.', 'We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.', 'We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance.', 'Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set.', 'Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.21052631735801697, 0.0555555522441864, 0.1599999964237213, 0.2222222238779068, 0.11764705181121826, 0.0952380895614624, 0.05714285373687744, 0.0952380895614624]",BywyFQlAW,"['Minimax Curriculum Learning is a machine teaching method involving increasing desirable hardness and scheduled reducing diversity.', ' A curriculum learning approach using a submodular set function that captures the diversity of examples chosen during training. ', 'The paper introduces MiniMax Curriculum learning as an approach for adaptively training models by providing it different subsets of data. ']","['introduce study minimax curriculum learning  mcl   new method adaptively selecting sequence training subset succession stage machine learning ', 'subset encouraged small diverse early  larger  harder  allowably homogeneous later stage ', 'stage  model weight training set chosen solving joint continuousdiscrete minimax optimization  whose objective composed continuous loss  reflecting training set hardness  discrete submodular promoter diversity chosen subset ', 'mcl repeatedly solves sequence optimization schedule increasing training set size decreasing pressure diversity encouragement ', 'reduce mcl minimization surrogate function handled submodular maximization continuous gradient method ', 'show mcl achieves better performance  clustering trick  us fewer labeled sample shallow deep model achieving performance ', 'method involves repeatedly solving constrained submodular maximization slowly varying function ground set ', 'therefore  develop heuristic method utilizes previous submodular maximization solution warm start current submodular maximization process reduce computation still yielding guarantee ']","We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning., The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages., At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset., MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement., We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods., We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance., Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set., Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.",17,5.713592233009709,12.117647058823529
309,"['Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.', 'However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.', 'In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?', 'How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?', 'To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.', 'For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.', 'For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.', 'In experiments, we scale Bayesian inference on up to a billion genetic measurements.', 'We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1428571343421936, 0.06896551698446274, 0.05882352590560913, 0.07407406717538834, 0.19999998807907104, 0.07692307233810425, 0.0, 0.09999999403953552, 0.0]",SyELrEeAb,"['Implicit models applied to causality and genetics', 'The authors propose to use the implicit model to tackle Genome-Wide Association problem.', 'This paper proposes solutions for the problems in genome-wide association studies of confounding due to population structure and the potential presence of non-linear interactions between different parts of the genome, and bridges statistical genetics and ML.', 'Presents a non-linear generative model for GWAS that models population structure where non-linearities are modeled using neural networks as non-linear function approximators and inference is performed using likelihood-free variational inference']","['progress probabilistic generative model accelerated  developing richer model neural architecture  implicit density  scalable algorithm bayesian inference ', 'however  limited progress model capture causal relationship  example  individual genetic factor cause major human disease ', 'work  focus two challenge particular  build richer causal model  capture highly nonlinear relationship interaction multiple cause ', 'adjust latent confounders  variable influencing cause effect prevent learning causal relationship ', 'address challenge  synthesize idea causality modern probabilistic modeling ', 'first  describe implicit causal model  class causal model leverage neural architecture implicit density ', 'second  describe implicit causal model adjusts confounders sharing strength across example ', 'experiment  scale bayesian inference billion genetic measurement ', 'achieve state art accuracy identifying causal factor  significantly outperform second best result absolute difference 15453  ']","Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference., However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases., In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?, How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?, To address these challenges, we synthesize ideas from causality and modern probabilistic modeling., For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density., For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples., In experiments, we scale Bayesian inference on up to a billion genetic measurements., We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.",23,5.891304347826087,8.0
310,"['\nFew-shot learning trains image classifiers over datasets with few examples per category. \n', 'It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. \n', 'Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories.', 'In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.\n', 'For a new category, even though its images are not seen by the model, some objects may appear in the training images.', 'Hence, object-level relation is useful for inferring the relation of images from unseen categories.', 'Consequently, our model generalizes well for new categories without fine-tuning.\n', 'Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.1666666567325592, 0.13333332538604736, 0.1249999925494194, 0.3125, 0.12903225421905518, 0.25, 0.0, 0.0]",rkzcvoA9YX,"['Few-shot learning by exploiting the object-level relation to learn the image-level relation (similarity)', 'This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples', 'Proposes a few shot learning method that exploits the object-level relation between different images based on neared neighbor search and concatenates feature maps of two input images into one feature map']","['fewshot learning train image classifier datasets example per category ', 'pose challenge optimization algorithm  typically require many example finetune model parameter new category ', 'distancelearningbased approach avoid optimization issue embedding image metric space applying nearest neighbor classifier new category ', 'paper  propose exploit objectlevel relation learn image relation feature  converted distance directly ', 'new category  even though image seen model  object may appear training image ', 'hence  objectlevel relation useful inferring relation image unseen category ', 'consequently  model generalizes well new category without finetuning ', 'experimental result benchmark datasets show approach outperforms stateoftheart method ']","
Few-shot learning trains image classifiers over datasets with few examples per category. 
, It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. 
, Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories., In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.
, For a new category, even though its images are not seen by the model, some objects may appear in the training images., Hence, object-level relation is useful for inferring the relation of images from unseen categories., Consequently, our model generalizes well for new categories without fine-tuning.
, Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.",15,5.970588235294118,9.066666666666666
311,"['Word embeddings are widely used in machine learning based natural language processing systems.', 'It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance.', 'There has been a recent interest in applying natural language processing techniques to programming languages.', 'However, none of this recent work uses pre-trained embeddings on code tokens.', 'Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\\% improvement in test loss, 4\\% improvement in F1 scores, and resistance to over-fitting.', 'We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   ']","[0, 0, 0, 0, 0, 1]","[0.21621620655059814, 0.09090908616781235, 0.25641024112701416, 0.1666666567325592, 0.25, 0.2641509473323822]",H1glKiCqtm,"['Researchers exploring natural language processing techniques applied to source code are not using any form of pre-trained embeddings, we show that they should be.', 'This paper sets to understand whether pretraining word embeddings for programming language code by using NLP-like language models has an impact on extreme code summarization task.', 'This work shows how pre-training word vectors using corpuses of code leads to representations that are more suitable than randomly initialized and trained representations for function/method name prediction']","['word embeddings widely used machine learning based natural language processing system ', 'common use pretrained word embeddings provide benefit reduced training time improved overall performance ', 'recent interest applying natural language processing technique programming language ', 'however  none recent work us pretrained embeddings code token ', 'using extreme summarization downstream task  show using pretrained embeddings code token provides benefit natural language  achieving  19x speedup  5  improvement test loss  4  improvement f1 score  resistance overfitting ', 'also show choice language used embeddings match task achieve benefit even embeddings pretrained human language provide benefit programming language ']","Word embeddings are widely used in machine learning based natural language processing systems., It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance., There has been a recent interest in applying natural language processing techniques to programming languages., However, none of this recent work uses pre-trained embeddings on code tokens., Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\% improvement in test loss, 4\% improvement in F1 scores, and resistance to over-fitting., We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   ",12,5.566433566433567,11.916666666666666
312,"['Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.', 'These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).', 'In these two-player games, a reward is always received at the end of the game.', 'However, the Rubiks Cube has only a single solved state, and episodes are not guaranteed to terminate.', 'This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.', 'We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.', 'Autodidactic Iteration is able to learn how to solve the Rubiks Cube and the 15-puzzle without relying on human data.', 'Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves  less than or equal to solvers that employ human domain knowledge.']","[0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.0, 0.08695651590824127, 0.1538461446762085, 0.06896550953388214, 0.10526315122842789, 0.2222222238779068, 0.052631575614213943]",Hyfn2jCcKm,"[""We solve the Rubik's Cube with pure reinforcement learning"", 'Solution to solving Rubik cube using reinforcement learning (RL) with Monte-Carlo tree search (MCTS) through autodidactic iteration. ', ""This work solves Rubik's Cube using an approximate policy iteration method called Autodidactic iteration, overcoming the problem of sparse rewards by creating its own rewards system."", ""Introduces a deep RL algorithm to solve the Rubik's cube that handles the huge state space and very sparse reward of the Rubik's cube""]","['recently  approximate policy iteration  api  algorithm achieved superhuman proficiency twoplayer zerosum game go  chess  shogi without human data ', 'api algorithm iterate two policy  slow policy  tree search   fast policy  neural network  ', 'twoplayer game  reward always received end game ', 'however  rubik  cube single solved state  episode guaranteed terminate ', 'pose major problem api algorithm since rely reward received end game ', 'introduce autodidactic iteration  api algorithm overcomes problem sparse reward training distribution state allows reward propagate goal state state farther away ', 'autodidactic iteration able learn solve rubik  cube 15puzzle without relying human data ', 'algorithm able solve 100  randomly scrambled cube achieving median solve length 30 move  le equal solver employ human domain knowledge ']","Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data., These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network)., In these two-player games, a reward is always received at the end of the game., However, the Rubiks Cube has only a single solved state, and episodes are not guaranteed to terminate., This poses a major problem for these API algorithms since they rely on the reward received at the end of the game., We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away., Autodidactic Iteration is able to learn how to solve the Rubiks Cube and the 15-puzzle without relying on human data., Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves  less than or equal to solvers that employ human domain knowledge.",15,5.016393442622951,12.2
313,"['Answering compositional questions requiring multi-step reasoning is challenging for current models.', 'We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics.', 'Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning.', 'Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question.', 'For example, to interpret not green, the model will represent green as a set of entities, not as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation.', 'For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent.', 'We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task.', 'The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines.', 'We will release our code.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.04651162400841713, 0.3265306055545807, 0.26923075318336487, 0.3461538553237915, 0.2539682388305664, 0.2711864411830902, 0.3050847351551056, 0.15686273574829102, 0.054054051637649536]",rkaqxm-0b,"['We describe an end-to-end differentiable model for QA that learns to represent spans of text in the question as denotations in knowledge graph, by learning both neural modules for composition and the syntactic structure of the sentence.', 'This paper presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser.', 'Proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words']","['answering compositional question requiring multistep reasoning challenging current model ', 'introduce endtoend differentiable model interpreting question  inspired formal approach semantics ', 'span text represented denotation knowledge graph  together vector capture ungrounded aspect meaning ', 'learned composition module recursively combine constituent  culminating grounding complete sentence answer question ', 'example  interpret  green   model represent  green  set entity    trainable ungrounded vector  use vector parametrize composition function perform complement operation ', 'sentence  build parse chart subsuming possible par  allowing model jointly learn composition operator output structure gradient descent ', 'show model learn represent variety challenging semantic operator  quantifier  negation  disjunction composed relation synthetic question answering task ', 'model also generalizes well longer sentence seen training data  contrast lstm relnet baseline ', 'release code ']","Answering compositional questions requiring multi-step reasoning is challenging for current models., We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics., Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning., Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question., For example, to interpret not green, the model will represent green as a set of entities, not as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation., For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent., We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task., The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines., We will release our code.",22,5.62303664921466,8.681818181818182
314,"['Deep learning software demands reliability and performance.', 'However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.', 'We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.', 'Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.', 'With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.']","[0, 1, 0, 0, 0]","[0.0952380895614624, 0.3684210479259491, 0.23255813121795654, 0.29999998211860657, 0.29999998211860657]",ryG6xZ-RZ,"['We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.', 'Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices.', 'This paper presents a compiler framework that allows definition of domain-specific languages for deep learning systems, and defines compilation stages that can take advantage of standard optimizations and specialized optimizations for neural networks.', 'This paper introduces a DLVM to take advantage of the compiler aspects of a tensor compiler']","['deep learning software demand reliability performance ', 'however  many existing deep learning framework software library act unsafe dsl python computation graph interpreter ', 'present dlvm  design implementation compiler infrastructure linear algebra intermediate representation  algorithmic differentiation adjoint code generation  domain specific optimization code generator targeting gpu via llvm ', 'designed modern compiler infrastructure inspired llvm  dlvm modular generic existing deep learning compiler framework  support tensor dsl high expressivity ', 'prototypical staged dsl embedded swift  argue dlvm system enables form modular  safe performant framework deep learning ']","Deep learning software demands reliability and performance., However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter., We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM., Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity., With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.",13,5.816666666666666,9.23076923076923
315,"['In this work, we focus on the problem of grounding language by training an agent\n', 'to follow a set of natural language instructions and navigate to a target object\n', 'in a 2D grid environment.', 'The agent receives visual information through raw\n', 'pixels and a natural language instruction telling what task needs to be achieved.\n', 'Other than these two sources of information, our model does not have any prior\n', 'information of both the visual and textual modalities and is end-to-end trainable.\n', 'We develop an attention mechanism for multi-modal fusion of visual and textual\n', 'modalities that allows the agent to learn to complete the navigation tasks and also\n', 'achieve language grounding.', 'Our experimental results show that our attention\n', 'mechanism outperforms the existing multi-modal fusion mechanisms proposed in\n', 'order to solve the above mentioned navigation task.', 'We demonstrate through the\n', 'visualization of attention weights that our model learns to correlate attributes of\n', 'the object referred in the instruction with visual representations and also show\n', 'that the learnt textual representations are semantically meaningful as they follow\n', 'vector arithmetic and are also consistent enough to induce translation between instructions\n', 'in different natural languages.', 'We also show that our model generalizes\n', 'effectively to unseen scenarios and exhibit zero-shot generalization capabilities.\n', 'In order to simulate the above described challenges, we introduce a new 2D environment\n', 'for an agent to jointly learn visual and textual modalities']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.12903225421905518, 0.1428571343421936, 0.4761904776096344, 0.0, 0.13333332538604736, 0.0, 0.0, 0.0714285671710968, 0.0, 0.21052631735801697, 0.0, 0.07999999821186066, 0.0, 0.0, 0.0, 0.07407406717538834, 0.0, 0.0, 0.09999999403953552, 0.0, 0.0, 0.2666666507720947, 0.07692307233810425]",HJPSN3gRW,"['Attention based architecture for language grounding via reinforcement learning in a new customizable 2D grid environment  ', 'The paper tackles the problem of navigation given an instruction and proposes an approach to combine textual and visual information via an attention mechanism', 'This paper considers the problem of following natural language instructions given a first-person view of an a priori unknown environment, and proposes a neural architecture method.', 'Studies the problem of navigating to a target object in a 2D grid environment by following given natural language description and receiving visual information as raw pixels.']","['work  focus problem grounding language training agent', 'follow set natural language instruction navigate target object', '2d grid environment ', 'agent receives visual information raw', 'pixel natural language instruction telling task need achieved ', 'two source information  model prior', 'information visual textual modality endtoend trainable ', 'develop attention mechanism multimodal fusion visual textual', 'modality allows agent learn complete navigation task also', 'achieve language grounding ', 'experimental result show attention', 'mechanism outperforms existing multimodal fusion mechanism proposed', 'order solve mentioned navigation task ', 'demonstrate', 'visualization attention weight model learns correlate attribute', 'object referred instruction visual representation also show', 'learnt textual representation semantically meaningful follow', 'vector arithmetic also consistent enough induce translation instruction', 'different natural language ', 'also show model generalizes', 'effectively unseen scenario exhibit zeroshot generalization capability ', 'order simulate described challenge  introduce new 2d environment', 'agent jointly learn visual textual modality']","In this work, we focus on the problem of grounding language by training an agent
, to follow a set of natural language instructions and navigate to a target object
, in a 2D grid environment., The agent receives visual information through raw
, pixels and a natural language instruction telling what task needs to be achieved.
, Other than these two sources of information, our model does not have any prior
, information of both the visual and textual modalities and is end-to-end trainable.
, We develop an attention mechanism for multi-modal fusion of visual and textual
, modalities that allows the agent to learn to complete the navigation tasks and also
, achieve language grounding., Our experimental results show that our attention
, mechanism outperforms the existing multi-modal fusion mechanisms proposed in
, order to solve the above mentioned navigation task., We demonstrate through the
, visualization of attention weights that our model learns to correlate attributes of
, the object referred in the instruction with visual representations and also show
, that the learnt textual representations are semantically meaningful as they follow
, vector arithmetic and are also consistent enough to induce translation between instructions
, in different natural languages., We also show that our model generalizes
, effectively to unseen scenarios and exhibit zero-shot generalization capabilities.
, In order to simulate the above described challenges, we introduce a new 2D environment
, for an agent to jointly learn visual and textual modalities",26,5.6535087719298245,8.76923076923077
316,"[' Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention.', 'Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.', 'We propose a new Q\\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.', 'On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.', 'The speed-up gain allows us to train the model with much more data.', 'We hence combine our model with data generated by backtranslation from a neural machine translation model. \n', 'On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.']","[1, 0, 0, 0, 0, 0, 0]","[0.3243243098258972, 0.21052631735801697, 0.21739129722118378, 0.19512194395065308, 0.12903225421905518, 0.05882352590560913, 0.25531914830207825]",B14TlG-RW,"['A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.', 'A fast high performance paraphrasing based data augmentation method and a non-recurrent reading comprehension model using only convolutions and attention.', 'This paper proposes applying CNNs+self-attention modules instead of LSTMs and enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model in order to improve RC performance.', 'This paper presents a reading comprehension model using convolutions and attention and propose to augment additional training data by paraphrasing based on off-the-shelf neural machine translation']","['current endtoend machine reading question answering  q   model primarily based recurrent neural network  rnns  attention ', 'despite success  model often slow training inference due sequential nature rnns ', 'propose new q  architecture called qanet  require recurrent network  encoder consists exclusively convolution selfattention  convolution model local interaction selfattention model global interaction ', 'squad dataset  model 3x 13x faster training 4x 9x faster inference  achieving equivalent accuracy recurrent model ', 'speedup gain allows u train model much data ', 'hence combine model data generated backtranslation neural machine translation model ', 'squad dataset  single model  trained augmented data  achieves 846 f1 score test set  significantly better best published f1 score 818 ']"," Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention., Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs., We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions., On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models., The speed-up gain allows us to train the model with much more data., We hence combine our model with data generated by backtranslation from a neural machine translation model. 
, On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",16,5.443037974683544,9.875
317,"['Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.', 'However, a number of problems of recent interest have created a demand for models that can analyze spherical images.', 'Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.', 'A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\n', 'In this paper we introduce the building blocks for constructing spherical CNNs.', 'We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.', 'The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.', 'We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.']","[0, 0, 0, 0, 0, 0, 0, 1]","[0.05405404791235924, 0.1621621549129486, 0.21621620655059814, 0.16326530277729034, 0.1875, 0.29411762952804565, 0.19512194395065308, 0.44999998807907104]",Hkbd5xZRb,"['We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression.', 'The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts', 'This paper focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal.', 'The authors develop a novel scheme for representing spherical data from the ground up']","['convolutional neural network  cnns  become method choice learning problem involving 2d planar image ', 'however  number problem recent interest created demand model analyze spherical image ', 'example include omnidirectional vision drone  robot  autonomous car  molecular regression problem  global weather climate modelling ', 'naive application convolutional network planar projection spherical signal destined fail  spacevarying distortion introduced projection make translational weight sharing ineffective ', 'paper introduce building block constructing spherical cnns ', 'propose definition spherical crosscorrelation expressive rotationequivariant ', 'spherical correlation satisfies generalized fourier theorem  allows u compute efficiently using generalized  noncommutative  fast fourier transform  fft  algorithm ', 'demonstrate computational efficiency  numerical accuracy  effectiveness spherical cnns applied 3d model recognition atomization energy regression ']","Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images., However, a number of problems of recent interest have created a demand for models that can analyze spherical images., Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling., A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.

, In this paper we introduce the building blocks for constructing spherical CNNs., We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant., The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm., We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.",17,6.314465408805032,9.352941176470589
318,"['We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.', 'Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness.', 'We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions.', 'We highlight that our method has two distinct benefits over other automated design approaches.', 'First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest.', 'Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods.', 'These two strengths work together to overcome some of the current shortcomings of automated design.']","[1, 0, 0, 0, 0, 0, 0]","[0.5600000023841858, 0.19230768084526062, 0.1538461446762085, 0.20512820780277252, 0.1428571343421936, 0.09302324801683426, 0.1538461446762085]",ByaQIGg0-,"['A method for performing automated design on real world objects such as heat sinks and wing airfoils that makes use of neural networks and gradient descent.', 'Neural network (parameterization and prediction) and gradient descent (back propogation) to automatically design for engineering tasks. ', 'This paper introduces using a deep network to approximate the behavior of a complex physical system, and then design optimal devices by optimizing this network with respect to its inputs.']","['propose novel method make use deep neural network gradient decent perform automated design complex real world engineering task ', 'approach work training neural network mimic fitness function design optimization task  using differential nature neural network  perform gradient decent maximize fitness ', 'demonstrate method effectiveness designing optimized heat sink 2d 3d airfoil maximize lift drag ratio steady state flow condition ', 'highlight method two distinct benefit automated design approach ', 'first  evaluating neural network prediction fitness order magnitude faster simulating system interest ', 'second  using gradient decent allows design space searched much efficiently gradient free method ', 'two strength work together overcome current shortcoming automated design ']","We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks., Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness., We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions., We highlight that our method has two distinct benefits over other automated design approaches., First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest., Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods., These two strengths work together to overcome some of the current shortcomings of automated design.",11,5.410256410256411,14.181818181818182
319,"['Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.', 'However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.', 'We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.', 'Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.', 'We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.', 'In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.']","[0, 0, 0, 1, 0, 0]","[0.1428571343421936, 0.08163265138864517, 0.2857142686843872, 0.3492063581943512, 0.21739129722118378, 0.31111109256744385]",BkA7gfZAb,"[' We propose a dual version of the logistic adversarial distance for feature alignment and show that it yields more stable gradient step iterations than the min-max objective.', 'The paper deals with fixing GANs at the computational level', 'This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss, and turns the standard min max problem of adversarial training into a single minimization problem.', 'Proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression']","['method align distribution minimizing adversarial distance recently achieved impressive result ', 'however  approach difficult optimize gradient descent often converge well without careful hyperparameter tuning proper initialization ', 'investigate whether turning adversarial minmax problem optimization problem replacing maximization part dual improves quality resulting alignment explore connection maximum mean discrepancy ', 'empirical result suggest using dual formulation restricted family linear discriminator result stable convergence desirable solution compared performance primal minmax ganlike objective mmd objective restriction ', 'test hypothesis problem aligning two synthetic point cloud plane realimage domain adaptation problem digit ', 'case  dual formulation yield iterative procedure give stable monotonic improvement time ']","Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results., However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization., We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy., Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions., We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits., In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.",8,5.798780487804878,20.5
320,"['  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized.', 'Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective.', 'In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations.', 'Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies.', 'Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs.', 'Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution.', 'These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations.', 'We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude).', 'Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.1875, 0.07999999821186066, 0.0833333283662796, 0.10526315122842789, 0.13793103396892548, 0.0, 0.0714285671710968, 0.2857142686843872, 0.0]",Sk6fD5yCb,"['state-of-the-art computational performance implementation of binary neural networks', 'The paper presents a library written in C/CUDA that features all the functionalities required for the forward propagation of BCNNs', 'This paper builds on Binary-NET and expands it to CNN architectures, provides optimizations that improve the speed of the forward pass, and provides optimized code for Binary CNN.']","['many application scenario computational performance memory footprint prediction phase deep neural network  dnns  need optimized ', 'binary deep neural network  bdnns  shown effective way achieving objective ', 'paper  show convolutional neural network  cnns  implemented using binary representation ', 'espresso compact  yet powerful library written ccuda feature functionality required forward propagation cnns  binary file le 400kb  without external dependency ', 'although mainly designed take advantage massive gpu parallelism  espresso also provides equivalent cpu implementation cnns ', 'espresso provides special convolutional dense layer bcnns  leveraging bitpacking bitwise computation efficient execution ', 'technique provide speedup matrixmultiplication routine  time  reduce memory usage storing parameter activation ', 'experimentally show espresso significantly faster existing implementation optimized binary neural network   2 order magnitude  ', 'espresso released apache 20 license available http  githubcomorganizationproject ']","  There are many applications scenarios for which the computational
  performance and memory footprint of the prediction phase of Deep
  Neural Networks (DNNs) need to be optimized., Binary Deep Neural
  Networks (BDNNs) have been shown to be an effective way of achieving
  this objective., In this paper, we show how Convolutional Neural
  Networks (CNNs) can be implemented using binary
  representations., Espresso is a compact, yet powerful
  library written in C/CUDA that features all the functionalities
  required for the forward propagation of CNNs, in a binary file less
  than 400KB, without any external dependencies., Although it is mainly
  designed to take advantage of massive GPU parallelism, Espresso also
  provides an equivalent CPU implementation for CNNs., Espresso
  provides special convolutional and dense layers for BCNNs,
  leveraging bit-packing and bit-wise computations
  for efficient execution., These techniques provide a speed-up of
  matrix-multiplication routines, and at the same time, reduce memory
  usage when storing parameters and activations., We experimentally
  show that Espresso is significantly faster than existing
  implementations of optimized binary neural networks (~ 2
  orders of magnitude)., Espresso is released under the Apache 2.0
  license and is available at http://github.com/organization/project.",16,6.048648648648649,11.5625
321,"['Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization.', 'In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.', 'We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task.', 'Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set.', 'We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training.', 'The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.']","[0, 1, 0, 0, 0, 0]","[0.3243243098258972, 0.8888888955116272, 0.10526315122842789, 0.09090908616781235, 0.25531914830207825, 0.0952380895614624]",r1GaAjRcF7,"['We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.', ""Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'Differentiable Greedy Network' (DGN)."", 'Proposes a neural network that aims to select a subset of elements (e.g. selecting k sentences that are mostly related to a claim from a set of retrieved docs)']","['optimal selection subset item given set hard problem requires combinatorial optimization ', 'paper  propose subset selection algorithm trainable gradient based method yet achieves near optimal performance via submodular optimization ', 'focus task identifying relevant set sentence claim verification context fever task ', 'conventional method task look sentence individual merit thus optimize informativeness sentence set ', 'show proposed method build idea unfolding greedy algorithm computational graph allows interpretability gradient based training ', 'proposed differentiable greedy network  dgn  outperforms discrete optimization algorithm well baseline method term precision recall ']","Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization., In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization., We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task., Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set., We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training., The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.",7,5.434782608695652,19.714285714285715
322,"['The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years.', 'In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations.', 'To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.', 'Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model.', 'Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components.', 'This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy.', 'In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features.', 'We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.4571428596973419, 0.24390242993831635, 0.800000011920929, 0.09090908616781235, 0.19999998807907104, 0.12121211737394333, 0.24390242993831635, 0.1666666567325592]",H1ERcs09KQ,"['We introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.', 'The paper proposes using the nested CRP as a clustering model rather than a topic model', 'Presents a novel hierarchical clustering method over an embedding space where both the embedding space and the heirarchical clustering are simultaneously learnt']","['joint optimization representation learning clustering embedding space experienced breakthrough recent year ', 'spite advance  clustering representation learning limited flatlevel category  oftentimes involves cohesive clustering focus instance relation ', 'overcome limitation flat clustering  introduce hierarchically clustered representation learning  hcrl   simultaneously optimizes representation learning hierarchical clustering embedding space ', 'specifically  place nonparametric bayesian prior embeddings handle dynamic mixture hierarchy variational autoencoder framework  adopt generative process hierarchicalversioned gaussian mixture model ', 'compared prior work focusing unifying representation learning hierarchical clustering  hcrl first model consider generation deep embeddings every component hierarchy  leaf component ', 'generation process enables meaningful separation merger cluster via branch hierarchy ', 'addition obtaining hierarchically clustered embeddings  reconstruct data various abstraction level  infer intrinsic hierarchical structure  learn levelproportion feature ', 'conducted evaluation image text domain  quantitative analysis showed competent likelihood best accuracy compared baseline ']","The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years., In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations., To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space., Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model., Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components., This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy., In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features., We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.",20,6.399014778325123,10.15
323,"['We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers.', 'Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry.', 'Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers.', 'This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition).', 'We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.']","[1, 0, 0, 0, 0]","[0.5, 0.08888888359069824, 0.1395348757505417, 0.2222222238779068, 0.21621620655059814]",BJeapjA5FX,"['We develop a statistical-geometric unsupervised learning augmentation framework for deep neural networks to make them robust to adversarial attacks.', 'Transfroms traditional deep neural networks into adversarial robust calssifiers using GRNs', 'Proposes a defense based on class-conditional feature distributions to turn deep neural netwroks into robust classifiers']","['introduce novel geometric perspective unsupervised model augmentation framework transforming traditional deep  convolutional  neural network adversarially robust classifier ', 'classconditional probability density based bayesian nonparametric mixture factor analyzer  bnpmfa  input space used design soft decision label feature label isometry ', 'classconditional distribution feature also learned using bnpmfa develop plugin maximum posterior  map  classifier replace traditional multinomial logistic softmax classification layer ', 'novel unsupervised augmented framework  call geometrically robust network  grn   applied cifar10  cifar100  radioml  time series dataset radio modulation recognition  ', 'demonstrate robustness grn model adversarial attack fast gradient sign method  carliniwagner  projected gradient descent ']","We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers., Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry., Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers., This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition)., We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.",11,6.701612903225806,11.272727272727273
324,"['Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient.', 'Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover.', 'In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces.', 'The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space.', 'This approach enables lower sample complexity, while preserving policy expressivity.', 'In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning.', 'The benefits of our learning approach are that', '1) it is principled,', '2) simple to implement,', '3) easily scalable to settings with many actions and', '4) easily composable with existing deep learning approaches.', 'We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space.', 'In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE).', 'We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines.', 'Moreover, we show that our hierarchical structure leads to meaningful agent coordination.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.4375, 0.0, 0.37837836146354675, 0.17391303181648254, 0.0, 0.2222222238779068, 0.0833333283662796, 0.0, 0.0, 0.07999999821186066, 0.25, 0.20512819290161133, 0.060606054961681366, 0.2380952388048172, 0.0714285671710968]",HyunpgbR-,"['Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.', ""A method to coordinate agent behaviour by using policies that have shared latent structure, a variational policy optimization method to optimize the coordinated policies, and a derivation of the authors' variational, hierarchical update."", 'This paper suggests an algorithmic innovation consisting of hierarchical latent variables for coordinated exploration in multi-agent settings']","['reinforcement learning environment large stateaction space challenging  exploration highly inefficient ', 'even dynamic simple  optimal policy combinatorially hard discover ', 'work  propose hierarchical approach structured exploration improve sample efficiency onpolicy exploration large stateaction space ', 'key idea model stochastic policy hierarchical latent variable model  learn lowdimensional structure stateaction space  define exploration sampling lowdimensional latent space ', 'approach enables lower sample complexity  preserving policy expressivity ', 'order make learning tractable  derive joint learning exploration strategy combining hierarchical variational inference actorcritic learning ', 'benefit learning approach', '1  principled ', '2  simple implement ', '3  easily scalable setting many action', '4  easily composable existing deep learning approach ', 'demonstrate effectiveness approach learning deep centralized multiagent policy  multiagent environment naturally exponentially large stateaction space ', 'setting  latent hierarchy implement form multiagent coordination exploration execution  mace  ', 'demonstrate empirically mace efficiently learn optimal policy challenging multiagent game large number  20  agent  compared conventional baseline ', 'moreover  show hierarchical structure lead meaningful agent coordination ']","Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient., Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover., In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces., The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space., This approach enables lower sample complexity, while preserving policy expressivity., In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning., The benefits of our learning approach are that, 1) it is principled,, 2) simple to implement,, 3) easily scalable to settings with many actions and, 4) easily composable with existing deep learning approaches., We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space., In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE)., We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines., Moreover, we show that our hierarchical structure leads to meaningful agent coordination.",26,6.07725321888412,8.961538461538462
325,"['Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance.', 'Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks.', 'However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks.', 'We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks.', 'In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR.', 'Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size.', 'This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data.', 'Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent.', 'Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.08510638028383255, 0.05882352590560913, 0.05882352590560913, 0.24242423474788666, 0.0952380895614624, 0.13636362552642822, 0.10256409645080566, 0.1764705777168274, 0.05405404791235924]",ryfMLoCqtQ,"['We provide many insights into neural network generalization from the theoretically tractable linear case.', 'The authors study a simple model of linear networks towards understanding generalization and transfer learning']","['much attention devoted recently generalization puzzle deep learning  large  deep network generalize well  existing theory bounding generalization error exceedingly loose  thus explain striking performance ', 'furthermore  major hope knowledge may transfer across task  multitask learning improve generalization individual task ', 'however lack analytic theory quantitatively predict degree knowledge transfer depends relationship task ', 'develop analytic theory nonlinear dynamic generalization deep linear network  within across task ', 'particular  theory provides analytic solution training testing error deep network function training time  number example  network size initialization  task structure snr ', 'theory reveals deep network progressively learn important task structure first  generalization error early stopping time primarily depends task structure independent network size ', 'suggests tight bound generalization error must take account task structure  explains observation real data learned faster random data ', 'intriguingly theory also reveals existence learning algorithm proveably outperforms neural network training gradient descent ', 'finally  transfer learning  theory reveals knowledge transfer depends sensitively  computably  snrs input feature alignment pair task ']","Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance., Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks., However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks., We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks., In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR., Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size., This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data., Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent., Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.",25,5.781512605042017,9.52
326,"['We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem.', 'Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation.', 'As a result, the gradient vanishing/explosion problem is avoided.', 'Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets.']","[0, 0, 1, 0]","[0.10256409645080566, 0.09090908616781235, 0.10526315122842789, 0.0]",r1Kr3TyAb,"['Batch normalisation maintains gradient variance throughout training, thus stabilizing optimization.', 'This paper analyzed the effect of batch normalization on gradient backpropagation in residual networks']","['conduct mathematical analysis batch normalization  bn  effect gradient backpropagation residual network training work  believed play critical role addressing gradient vanishingexplosion problem ', 'specifically  analyzing mean variance behavior input gradient forward backward pass bn residual branch  respectively  show work together confine gradient variance certain range across residual block backpropagation ', 'result  gradient vanishingexplosion problem avoided ', 'furthermore  use analysis discus tradeoff depth width residual network demonstrate shallower yet wider resnets stronger learning performance deeper yet thinner resnets ']","We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem., Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation., As a result, the gradient vanishing/explosion problem is avoided., Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets.",10,5.64,12.5
327,"['To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.', 'These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.', 'Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.', 'We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.']","[1, 0, 0, 0]","[0.3333333432674408, 0.21621620655059814, 0.19999998807907104, 0.25]",ryxSrhC9KX,"['Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks', 'This paper describes a large-scale experiment on human object/sematic representations and a model of such representations.', 'This paper develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images.', 'A new approach to learn a sparse, positive, interpretable semantic space that maximizes human similarity judgements by training to specifically maximize the prediction of human similarity judgements.']","['study mental object representation related behavior  estimated sparse  nonnegative representation object using human behavioral judgment image representative 1854 object category ', 'representation predicted latent similarity structure object  captured explainable variance human behavioral judgment ', 'individual dimension lowdimensional embedding found highly reproducible interpretable conveying degree taxonomic membership  functionality  perceptual attribute ', 'demonstrated predictive power embeddings explaining form human behavior  including categorization  typicality judgment  feature rating  suggesting dimension reflect human conceptual representation object beyond specific task ']","To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories., These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments., Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes., We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.",13,6.7889908256880735,8.384615384615385
328,"['We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. \n\n', 'We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.', 'The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. \n\n', 'The reformulation system is trained end-to-end to maximize answer quality using policy gradient.', 'We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!.', 'The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.\n\nWe also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.']","[0, 1, 0, 0, 0, 0]","[0.19512194395065308, 0.936170220375061, 0.2916666567325592, 0.10810810327529907, 0.1666666567325592, 0.202531635761261]",S1CChZ-CZ,"['We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers', 'This paper proposes active question answering via a reinforcement learning approach that learns to rephrase questions in a way to provide the best possible answers.', 'Clearly describes how the researchers designed and actively trained two models for question reformulation and answer selection during question answering episodes']","['frame question answering  qa  reinforcement learning task  approach call active question answering ', 'propose agent sits user black box qa system learns reformulate question elicit best possible answer ', 'agent probe system  potentially many  natural language reformulations initial question aggregate returned evidence yield best answer ', 'reformulation system trained endtoend maximize answer quality using policy gradient ', 'evaluate searchqa  dataset complex question extracted jeopardy  ', 'agent outperforms stateoftheart base model  playing role environment  benchmark  also analyze language agent learned interacting question answering system  find successful question reformulations look quite different natural language paraphrase  agent able discover nontrivial reformulation strategy resemble classic information retrieval technique term reweighting  tfidf  stemming ']","We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. 

, We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers., The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. 

, The reformulation system is trained end-to-end to maximize answer quality using policy gradient., We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!., The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.

We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.",12,5.820987654320987,10.8
329,"['Most deep latent factor models choose simple priors for simplicity, tractability\n', 'or not knowing what prior to use.', 'Recent studies show that the choice of\n', 'the prior may have a profound effect on the expressiveness of the model,\n', 'especially when its generative network has limited capacity.', 'In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n', '(AAEs).', 'We introduce the notion of code generators to transform manually selected\n', 'simple priors into ones that can better characterize the data distribution.', 'Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\n', 'AAEs in both supervised and unsupervised settings.', 'Lastly, we present its\n', 'ability to do cross-domain translation in a  text-to-image synthesis task.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.1249999925494194, 0.0, 0.0, 0.0, 0.0, 0.09999999403953552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]",rJSr0GZR-,"['Learning Priors for Adversarial Autoencoders', 'Proposes a simple extension of adversarial auto-encoders for conditional image generation.', 'Focuses on adversarial autoencoders and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution']","['deep latent factor model choose simple prior simplicity  tractability', 'knowing prior use ', 'recent study show choice', 'prior may profound effect expressiveness model ', 'especially generative network limited capacity ', 'paper  propose learn proper prior data adversarial autoencoders', ' aaes  ', 'introduce notion code generator transform manually selected', 'simple prior one better characterize data distribution ', 'experimental result show proposed model generate better image quality learn better disentangled representation', 'aaes supervised unsupervised setting ', 'lastly  present', 'ability crossdomain translation texttoimage synthesis task ']","Most deep latent factor models choose simple priors for simplicity, tractability
, or not knowing what prior to use., Recent studies show that the choice of
, the prior may have a profound effect on the expressiveness of the model,
, especially when its generative network has limited capacity., In this paper, we propose to learn a proper prior from data for adversarial autoencoders
, (AAEs)., We introduce the notion of code generators to transform manually selected
, simple priors into ones that can better characterize the data distribution., Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than
, AAEs in both supervised and unsupervised settings., Lastly, we present its
, ability to do cross-domain translation in a  text-to-image synthesis task.",16,5.626016260162602,7.6875
330,"['In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs).', 'GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer.', 'In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data.', 'Attempts have been made for utilizing GANs with word embeddings for text generation.', 'This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures.', 'The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.']","[0, 0, 0, 0, 0, 1]","[0.277777761220932, 0.05714285373687744, 0.14999999105930328, 0.2222222238779068, 0.31578946113586426, 0.3684210479259491]",SkGMOi05FQ,"['Generating text using sentence embeddings from Skip-Thought Vectors with the help of Generative Adversarial Networks.', 'Describes application of generative adversarial networks for modeling textual data with the help of ski-thought vectors and experiments with different flavors of GANs for two different datasets.']","['past year  various advancement made generative model owing formulation generative adversarial network  gans  ', 'gans shown perform exceedingly well wide variety task pertaining image generation style transfer ', 'field natural language processing  word embeddings word2vec glove stateoftheart method applying neural network model textual data ', 'attempt made utilizing gans word embeddings text generation ', 'work present approach text generation using skipthought sentence embeddings conjunction gans based gradient penalty function fmeasures ', 'result using sentence embeddings gans generating text conditioned input information comparable approach word embeddings used ']","In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs)., GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer., In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data., Attempts have been made for utilizing GANs with word embeddings for text generation., This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures., The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.",8,5.682170542635659,16.125
331,"['The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models.', 'It works in a streaming fashion and avoids backtracking through past activations and inputs.', 'UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  ', 'Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed.', 'On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  ', 'On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.', 'For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n']","[1, 0, 0, 0, 0, 0, 0]","[0.1818181723356247, 0.07999999821186066, 0.14814814925193787, 0.10810810327529907, 0.0, 0.0, 0.0]",rJQDjk-0b,"['Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.', 'The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the imitation of truncated backpropagation through time', 'This paper approaches online training of RNNs in a principled way, and proposes a modification to RTRL and to use forward approach for gradient calculation.']","['novel emph  unbiased online recurrent optimization   uoro  algorithm allows online learning general recurrent computational graph recurrent network model ', 'work streaming fashion avoids backtracking past activation input ', 'uoro computationally costly emph  truncated backpropagation time   truncated bptt   widespread algorithm online learning recurrent network cite  jaeger2002tutorial   uoro modification emph  nobacktrack  cite  dblp  journalscorrollivierc15  bypass need model sparsity make implementation easy current deep learning framework  even complex model ', 'like nobacktrack  uoro provides unbiased gradient estimate  unbiasedness core hypothesis stochastic gradient descent theory  without convergence local optimum guaranteed ', 'contrary  truncated bptt provide property  leading possible divergence ', 'synthetic task truncated bptt shown diverge  uoro converges ', 'instance  parameter positive shortterm negative longterm influence  truncated bptt diverges unless truncation span significantly longer intrinsic temporal range interaction  uoro performs well thanks unbiasedness gradient ']","The novel \emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models., It works in a streaming fashion and avoids backtracking through past activations and inputs., UORO is computationally as costly as \emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \cite{jaeger2002tutorial}.  UORO is a modification of \emph{NoBackTrack} \cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  , Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed., On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  , On synthetic tasks where truncated BPTT is shown to diverge, UORO converges., For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.
",17,6.392265193370166,10.055555555555555
332,"['We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels.', 'This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs.', 'This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available.', 'Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.\n', 'We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution.', 'We compare our algorithm with models that are trained on high-resolution data and show that', '1) we can achieve similar performance using only low-resolution data; and', '2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training.', 'We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3255814015865326, 0.10256409645080566, 0.05405404791235924, 0.17391304671764374, 0.11428570747375488, 0.07407406717538834, 0.0833333283662796, 0.0, 0.14999999105930328]",rkxwShA9Ym,"['Super-resolving coarse labels into pixel-level labels, applied to aerial imagery and medical scans.', 'A method to super-resolve coarse low-res segmentation labels if the joint distribution of low-res and high-res labels are known.']","['present deep learningbased method superresolving coarse  lowresolution  label assigned group image pixel pixellevel  highresolution  label  given joint distribution low highresolution label ', 'method involves novel loss function minimizes distance distribution determined set model output corresponding distribution given lowresolution label set output ', 'setup require highresolution class match lowresolution class used highresolution semantic segmentation task highresolution labeled data available ', 'furthermore  proposed method able utilize data lowresolution label available highresolution label  show improves performance compared network trained amount highresolution data ', 'test proposed algorithm challenging land cover mapping task superresolve label 30m resolution separate set label 1m resolution ', 'compare algorithm model trained highresolution data show', '1  achieve similar performance using lowresolution data ', '2  achieve better performance incorporate small amount highresolution data training ', 'also test approach medical imaging problem  resolving lowresolution probability map highresolution segmentation lymphocyte accuracy equal fully supervised model ']","We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels., This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs., This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available., Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.
, We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution., We compare our algorithm with models that are trained on high-resolution data and show that, 1) we can achieve similar performance using only low-resolution data; and, 2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training., We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.",13,5.771551724137931,17.846153846153847
333,"['We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions.', 'Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems.', 'Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process.', 'Importantly, we do not assume any individual correspondence (partial or complete) between data points.', 'Instead, we rely on our assumption that a subset of data features have correspondence across datasets.', 'We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets.', 'We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features.', 'We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets.', 'Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together.', 'We show that this approach successfully corrects misalignment artifacts, and allows for integrated data.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.32258063554763794, 0.2702702581882477, 0.08695651590824127, 0.0, 0.1875, 0.1538461446762085, 0.2702702581882477, 0.1818181723356247, 0.1818181723356247, 0.13333332538604736]",SkGNrnC9FQ,"['We propose a method for aligning the latent features learned from different datasets using harmonic correlations.', 'Proposes using feature correspondences to preform manifold alignment between batches of data from the same samples to avoid the collection of noisy measurements.']","['propose novel framework combining datasets via alignment associated intrinsic dimension ', 'approach assumes two datasets sampled common latent space  ie  measure equivalent system ', 'thus  expect exist natural  albeit unknown  alignment data manifold associated intrinsic geometry datasets  perturbed measurement artifact sampling process ', 'importantly  assume individual correspondence  partial complete  data point ', 'instead  rely assumption subset data feature correspondence across datasets ', 'leverage assumption estimate relation intrinsic manifold dimension  given diffusion map coordinate datasets ', 'compute correlation matrix diffusion coordinate datasets considering graph  manifold  fourier coefficient corresponding data feature ', 'orthogonalize correlation matrix form isometric transformation diffusion map datasets ', 'finally  apply transformation diffusion coordinate construct unified diffusion geometry datasets together ', 'show approach successfully corrects misalignment artifact  allows integrated data ']","We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions., Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems., Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process., Importantly, we do not assume any individual correspondence (partial or complete) between data points., Instead, we rely on our assumption that a subset of data features have correspondence across datasets., We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets., We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features., We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets., Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together., We show that this approach successfully corrects misalignment artifacts, and allows for integrated data.",19,5.937823834196891,10.157894736842104
334,"['Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments.', 'When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment.', 'However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations.', ""Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. \n"", 'We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure.', 'Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance.', 'We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. \n', 'We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena.', 'Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. \n\n', 'A video is available at: www.youtube.com/watch?v=eei6Rgom3YY']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.10526315122842789, 0.34285715222358704, 0.21052631735801697, 0.3030303120613098, 0.15789473056793213, 0.1764705777168274, 0.10526315122842789, 0.21276594698429108, 0.10256409645080566, 0.0]",BJgWl3A5YX,"['Evolving the shape of the body in RL controlled agents improves their performance (and help learning)', 'PEOM algorithm that incorporates Shapley value to accelerate the evolution by identifying contribution of each body part']","['reinforcement learning  rl  proven powerful paradigm deriving complex behavior simple reward signal wide range environment ', 'applying rl continuous control agent simulated physic environment  body usually considered part environment ', 'however  evolution physical body biological organism controlling brain coevolved  thus exploring much larger space actuatorcontroller configuration ', 'put differently  intelligence reside agent mind  also design body ', 'propose method uncovering strong agent  consisting good combination body policy  based combining rl evolutionary procedure ', 'given resulting agent  also propose approach identifying body change contributed agent performance ', 'use shapley value cooperative game theory find fair contribution individual component  taking account synergy component ', 'evaluate method environment similar recently proposed robosumo task  agent 3d environment simulated physic compete tipping opponent pushing arena ', 'result show proposed method indeed capable generating strong agent  significantly outperforming baseline focus optimizing agent policy alone ', 'video available  wwwyoutubecomwatch  veei6rgom3yy']","Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments., When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment., However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations., Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. 
, We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure., Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance., We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. 
, We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena., Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. 

, A video is available at: www.youtube.com/watch?v=eei6Rgom3YY",21,5.598253275109171,10.904761904761905
335,"['Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never.', 'Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy.', 'In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.', 'Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe.', 'This score acts as a penalty on the Q-learning objective.', 'Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions.', 'Our analysis also shows robustness to classification errors.', 'Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.']","[0, 0, 0, 0, 0, 0, 0, 1]","[0.1428571343421936, 0.0, 0.17142856121063232, 0.06896550953388214, 0.0, 0.06451612710952759, 0.09999999403953552, 0.19999998807907104]",B16yEqkCZ,"['Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.', 'An RL algorithm that combines the DQN algorithm with a fear model trained in parallel to predict catastropohic states.', 'The paper studies catastrophic forgetting in RL, by emphasizing tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting.']","['many practical reinforcement learning problem contain catastrophic state optimal policy visit infrequently never ', 'even toy problem  deep reinforcement learner periodically revisit state  forgotten new policy ', 'paper  introduce intrinsic fear  learned reward shaping accelerates deep reinforcement learning guard oscillating policy periodic catastrophe ', 'approach incorporates second model trained via supervised learning predict probability imminent catastrophe ', 'score act penalty qlearning objective ', 'theoretical analysis demonstrates perturbed objective yield average return strong assumption  epsilon  close average return weaker assumption ', 'analysis also show robustness classification error ', 'equipped intrinsic fear  dqns solve toy environment improve atari game seaquest  asteroid  freeway ']","Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never., Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy., In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes., Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe., This score acts as a penalty on the Q-learning objective., Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\epsilon$-close average return under weaker assumptions., Our analysis also shows robustness to classification errors., Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.",15,6.233576642335766,9.133333333333333
336,"['Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks.', 'Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges.', 'In this work, we propose a novel `""volumetric convolution"" operation that can effectively convolve arbitrary functions in B^3.', 'We develop a theoretical framework for ""volumetric convolution"" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks.', 'Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks.', 'We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.']","[0, 1, 0, 0, 0, 0]","[0.0, 0.1666666567325592, 0.06896550953388214, 0.05714285373687744, 0.052631575614213943, 0.06451612710952759]",SkfhIo0qtQ,"['A novel convolution operator for automatic representation learning inside unit ball', 'This work is related to the recent spherical CNN and SE(n) equivariant network papers and extends previous ideas to volumetric data in the unit ball.', 'Proposes using volumetric convolutions on convolutions networks in order to learn unit ball and discusses methodology and results of process.']","['convolution efficient technique obtain abstract feature representation using hierarchical layer deep network ', 'although performing convolution euclidean geometry fairly straightforward  extension topological space  sphere s2 unit ball b3  entail unique challenge ', 'work  propose novel   volumetric convolution  operation effectively convolve arbitrary function b3 ', 'develop theoretical framework  volumetric convolution  based zernike polynomial efficiently implement differentiable easily pluggable layer deep network ', 'furthermore  formulation lead derivation novel formula measure symmetry function b3 around arbitrary axis  useful 3d shape analysis task ', 'demonstrate efficacy proposed volumetric convolution operation possible usecase ie  3d object recognition task ']","Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks., Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges., In this work, we propose a novel `""volumetric convolution"" operation that can effectively convolve arbitrary functions in B^3., We develop a theoretical framework for ""volumetric convolution"" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks., Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks., We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.",11,5.955882352941177,12.363636363636363
337,"['Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agents learning through trial-and-error.', 'For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large.', 'Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions.', 'We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from.', 'Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions.', 'In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively.', 'We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions.', 'We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions.', 'The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.10810810327529907, 0.08163265138864517, 0.0416666604578495, 0.1395348757505417, 0.1249999925494194, 0.09302324801683426, 0.20512819290161133, 0.09756097197532654, 0.0555555522441864]",BJemQ209FQ,"['We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.', 'Develops a curriculum learning method for training an RL agent to navigate a web, based on the idea of decomposing an instruction in to multiple sub-instructions.']","['learning environment large state action space  sparse reward  hinder reinforcement learning  rl  agent  learning trialanderror ', 'instance  following natural language instruction web  booking flight ticket  lead rl setting input vocabulary number actionable element page grow large ', 'even though recent approach improve success rate relatively simple environment help human demonstration guide exploration  still fail environment set possible instruction reach million ', 'approach aforementioned problem different perspective propose guided rl approach generate unbounded amount experience agent learn ', 'instead learning complicated instruction large vocabulary  decompose multiple subinstructions schedule curriculum agent tasked gradually increasing subset relatively easier subinstructions ', 'addition  expert demonstration available  propose novel metalearning framework generates new instruction following task train agent effectively ', 'train dqn  deep reinforcement learning agent  qvalue function approximated novel qweb neural network architecture smaller  synthetic instruction ', 'evaluate ability agent generalize new instruction onworld bit benchmark  form 100 element  supporting 14 million possible instruction ', 'qweb agent outperforms baseline without using human demonstration achieving 100  success rate several difficult environment ']","Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agents learning through trial-and-error., For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large., Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions., We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from., Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions., In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively., We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions., We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions., The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",21,5.8063241106719365,12.047619047619047
338,"['Labeled text classification datasets are typically only available in a few select languages.', 'In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options.', 'The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder.', 'The encoder will give almost identical representations to multilingual versions of the same text.', 'This means that labeled data in one language can be used to train a classifier that works for the rest of the languages.', 'The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  ', 'We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.']","[0, 0, 1, 0, 0, 0, 0]","[0.0, 0.0, 0.040816325694322586, 0.0, 0.0, 0.0, 0.0]",S1XXq6lRW,"['Cross Language Text Classification by universal encoding', 'This paper proposes an approach to cross-lingual text classification through the use of comparable corpora.', 'Learning cross-lingual embeddings and training a classifier using labelled data in the source language to address learning a cross-language text categorizer with no labelled information in the target language']","['labeled text classification datasets typically available select language ', 'order train model eg news categorization language  lt  without suitable text classification dataset two option ', 'first option create new labeled dataset hand  second option transfer label information existing labeled dataset source language  l  target language  lt   paper propose method sharing label information across language mean language independent text encoder ', 'encoder give almost identical representation multilingual version text ', 'mean labeled data one language used train classifier work rest language ', 'encoder trained independently concrete classification task therefore subsequently used classification task ', 'show possible obtain good performance even case comparable corpus text available ']","Labeled text classification datasets are typically only available in a few select languages., In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options., The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder., The encoder will give almost identical representations to multilingual versions of the same text., This means that labeled data in one language can be used to train a classifier that works for the rest of the languages., The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  , We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.",8,5.080459770114943,19.333333333333332
339,"['Syntax is a powerful abstraction for language understanding.', 'Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs).', 'Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text.', 'To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.', 'Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains.', 'Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks.', 'Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method.', 'DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.1249999925494194, 0.1666666567325592, 0.07999999821186066, 0.5714285373687744, 0.0, 0.045454539358615875, 0.1538461446762085, 0.09999999403953552]",HJeq43AqF7,"['In this work we propose deep inside-outside recursive auto-encoders(DIORA)  a  fully  unsupervised  method  of  discovering  syntax  while  simultaneously learning representations for discovered constituents. ', 'A neural latent tree model trained with an auto-encoding objective that achieves state of the art on unsupervised constituency parsing and captures syntactic structure better than other latent tree models.', 'The paper proposes a model for unsupervised dependency parsing (latent tree induction) that is based on a combination of the inside-outside algorithm with neural modeling (recursive auto-encoders). ']","['syntax powerful abstraction language understanding ', 'many downstream task require segmenting input text meaningful constituent chunk  eg  noun phrase entity   generally  model learning semantic representation text benefit integrating syntax form parse tree  eg  treelstms  ', 'supervised parser traditionally used obtain tree  lately interest increased unsupervised method induce syntactic representation directly unlabeled text ', 'end  propose deep insideoutside recursive autoencoder  diora   fullyunsupervised method discovering syntax simultaneously learns representation constituent within induced tree ', 'unlike many prior approach  diora rely supervision auxiliary downstream task thus constrained particular domain ', 'furthermore  competing approach learn explicit phrase representation along tree structure  limit applicability phrasebased task ', 'extensive experiment unsupervised parsing  segmentation  phrase clustering demonstrate efficacy method ', 'diora achieves state art unsupervised parsing  469 f1  benchmark wsj dataset ']","Syntax is a powerful abstraction for language understanding., Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs)., Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text., To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree., Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains., Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks., Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method., DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.",19,6.354651162790698,9.052631578947368
340,"['Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training.', 'There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled.', 'But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training.', 'We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias.', 'We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons.', 'We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area.', 'We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.']","[0, 0, 0, 0, 0, 0, 1]","[0.07692307233810425, 0.10810810327529907, 0.060606058686971664, 0.23076923191547394, 0.22857142984867096, 0.1666666567325592, 0.27586206793785095]",H1MczcgR-,"['We investigate the bias in the short-horizon meta-optimization objective.', 'This paper proposes a simplified model and problem to demonstrate the short-horizon bias of the learning rate meta-optimization.', 'This paper studies the issue of truncated backpropagation for meta-optimization through a number of experiments on a toy problem']","['careful tuning learning rate  even schedule thereof  crucial effective neural net training ', 'much recent interest gradientbased metaoptimization  one tune hyperparameters  even learns optimizer  order minimize expected loss training procedure unrolled ', 'training procedure must unrolled thousand time  metaobjective must defined ordersofmagnitude shorter time horizon typical neural net training ', 'show shorthorizon metaobjectives cause serious bias towards small step size  effect term shorthorizon bias ', 'introduce toy problem  noisy quadratic cost function  analyze shorthorizon bias deriving comparing optimal schedule short long time horizon ', 'run metaoptimization experiment  offline online  standard benchmark datasets  showing metaoptimization chooses small learning rate multiple order magnitude  even run moderately long time horizon  100 step  typical work area ', 'believe shorthorizon bias fundamental problem need addressed metaoptimization scale practical neural net training regime ']","Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training., There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled., But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training., We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias., We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons., We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area., We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.",18,5.538461538461538,10.833333333333334
341,"['Mainstream captioning models often follow a sequential structure to generate cap-\n', 'tions, leading to issues such as introduction of irrelevant semantics, lack of diversity\n', 'in the generated captions, and inadequate generalization performance.', 'In this paper,\n', 'we present an alternative paradigm for image captioning, which factorizes the\n', 'captioning procedure into two stages: (1) extracting an explicit semantic represen-\n', 'tation from the given image; and (2) constructing the caption based on a recursive\n', 'compositional procedure in a bottom-up manner.', 'Compared to conventional ones,\n', 'our paradigm better preserves the semantic content through an explicit factorization\n', 'of semantics and syntax.', 'By using the compositional generation procedure, caption\n', 'construction follows a recursive structure, which naturally fits the properties of\n', 'human language.', 'Moreover, the proposed compositional procedure requires less\n', 'data to train, generalizes better, and yields more diverse captions.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.31578946113586426, 0.09999999403953552, 0.1249999925494194, 0.0, 0.0, 0.0, 0.1904761791229248, 0.2857142686843872, 0.1666666567325592, 0.0, 0.1666666567325592, 0.13333332538604736, 0.10526315122842789, 0.13333332538604736, 0.3333333432674408]",SJxyZ81IYQ,"['a hierarchical and compositional way to generate captions', 'This paper presents a more interpretable method for image captioning.']","['mainstream captioning model often follow sequential structure generate cap', 'tions  leading issue introduction irrelevant semantics  lack diversity', 'generated caption  inadequate generalization performance ', 'paper ', 'present alternative paradigm image captioning  factorizes', 'captioning procedure two stage   1  extracting explicit semantic represen', 'tation given image   2  constructing caption based recursive', 'compositional procedure bottomup manner ', 'compared conventional one ', 'paradigm better preserve semantic content explicit factorization', 'semantics syntax ', 'using compositional generation procedure  caption', 'construction follows recursive structure  naturally fit property', 'human language ', 'moreover  proposed compositional procedure requires le', 'data train  generalizes better  yield diverse caption ']","Mainstream captioning models often follow a sequential structure to generate cap-
, tions, leading to issues such as introduction of irrelevant semantics, lack of diversity
, in the generated captions, and inadequate generalization performance., In this paper,
, we present an alternative paradigm for image captioning, which factorizes the
, captioning procedure into two stages: (1) extracting an explicit semantic represen-
, tation from the given image; and (2) constructing the caption based on a recursive
, compositional procedure in a bottom-up manner., Compared to conventional ones,
, our paradigm better preserves the semantic content through an explicit factorization
, of semantics and syntax., By using the compositional generation procedure, caption
, construction follows a recursive structure, which naturally fits the properties of
, human language., Moreover, the proposed compositional procedure requires less
, data to train, generalizes better, and yields more diverse captions.",25,6.2631578947368425,5.32
342,"['While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data.', 'Measures for characterizing and monitoring structural properties, however, have not been developed.', 'In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs.', 'To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization.', 'Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.']","[0, 0, 1, 0, 0]","[0.09999999403953552, 0.12903225421905518, 0.29999998211860657, 0.2222222238779068, 0.1428571343421936]",ByxkijC5FQ,"['We develop a new topological complexity measure for deep neural networks and demonstrate that it captures their salient properties.', 'This paper proposes the notion of neural persistence, a topological measure to assign scores to fully-connected layers in a neural network.', 'Paper proposes to analyze the complexity of a neural network using its zero-th persistent homology.']","['many approach make neural network fathomable proposed  restricted interrogating network input data ', 'measure characterizing monitoring structural property  however  developed ', 'work  propose neural persistence  complexity measure neural network architecture based topological data analysis weighted stratified graph ', 'demonstrate usefulness approach  show neural persistence reflects best practice developed deep learning community dropout batch normalization ', 'moreover  derive neural persistencebased stopping criterion shortens training process achieving comparable accuracy early stopping based validation loss ']","While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data., Measures for characterizing and monitoring structural properties, however, have not been developed., In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs., To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization., Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.",12,6.166666666666667,9.0
343,"['Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs.', 'Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models.', 'However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls.', 'In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models.', 'First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.', 'These examples successfully evade a defense that only considers a small ball around an input instance.', 'Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes.', 'We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples.', 'Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples.', 'Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.0, 0.0624999962747097, 0.060606054961681366, 0.09302324801683426, 0.13333332538604736, 0.32258063554763794, 0.3414634168148041, 0.13793103396892548, 0.05714285373687744, 0.09999999403953552]",BkpiPMbA-,"['Looking at decision boundaries around an input gives you more information than a fixed small neighborhood', 'The authors present a novel attack for generating adversarial examples where they attack classifiers created by randomly classifying L2 small perturbations', 'A new approach to generate adversarial attacks to a neural network, and a method to defend a neural network from those attacks.']","['deep neural network  dnns  vulnerable adversarial example  carefully crafted instance aiming cause prediction error dnns ', 'recent research adversarial example examined local neighborhood input space dnn model ', 'however  previous work limited region consider  focusing either lowdimensional subspace small ball ', 'paper  argue information larger neighborhood  direction greater distance  better characterize relationship adversarial example dnn model ', 'first  introduce attack  optmargin  generates adversarial example robust small perturbation ', 'example successfully evade defense considers small ball around input instance ', 'second  analyze larger neighborhood around input instance looking property surrounding decision boundary  namely distance boundary adjacent class ', 'find boundary around adversarial example resemble boundary around benign example ', 'finally  show  scrutiny surrounding decision boundary  optmargin example convincingly mimic benign example ', 'although experiment limited specific attack  hope finding motivate new  evasive attack ultimately  effective defense ']","Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs., Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models., However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls., In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models., First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations., These examples successfully evade a defense that only considers a small ball around an input instance., Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes., We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples., Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples., Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.",27,5.921182266009852,7.518518518518518
344,"['Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  ', 'We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  ', 'Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  ', 'We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  ', 'We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.']","[0, 0, 1, 0, 0]","[0.19999998807907104, 0.375, 0.800000011920929, 0.32258063554763794, 0.25]",SJIA6ZWC-,"['We train a neural network to output approximately optimal weights as a function of hyperparameters.', '\nHyper-networks for hyper-parameter optimization in neural networks.']","['machine learning model usually tuned nesting optimization model weight inside optimization hyperparameters ', 'give method collapse nested optimization joint stochastic optimization weight hyperparameters ', 'method train neural network output approximately optimal weight function hyperparameters ', 'show method converges locally optimal weight hyperparameters sufficiently large hypernets ', 'compare method standard hyperparameter optimization strategy demonstrate effectiveness tuning thousand hyperparameters ']","Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  , We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  , Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  , We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  , We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.",5,6.364705882352941,17.0
345,"['Estimating covariances between financial assets plays an important role in risk management.', 'In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.', 'Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).', 'Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.', 'Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.', 'Finally, we discuss some financial applications of the GP-LVM model.']","[0, 0, 1, 0, 0, 0]","[0.1666666567325592, 0.06451612710952759, 0.2857142686843872, 0.1249999925494194, 0.11428570747375488, 0.1818181723356247]",ryEquiR9KX,"['Covariance matrix estimation of financial assets with Gaussian Process Latent Variable Models', 'Illustrates how the Gaussian Process Latent Variable Model (GP-LVM) can replace classical linear factor models for the estimation of covariance matrices in portfolio optimization problems.', 'This paper uses standard GPLVMs to model the covariance structure and a latent space representation of S&P500 financial time series, to optimize portfolios and predict missing values.', 'This paper proposes to use a GPLVM to model financial returns']","['estimating covariance financial asset play important role risk management ', 'practice  sample size small compared number variable  empirical estimate known unstable ', ' propose novel covariance estimator based gaussian process latent variable model  gplvm  ', 'estimator considered nonlinear extension standard factor model readily interpretable parameter reminiscent market beta ', 'furthermore  bayesian treatment naturally shrink sample covariance matrix towards structured matrix given prior thereby systematically reduces estimation error ', 'finally  discus financial application gplvm model ']","Estimating covariances between financial assets plays an important role in risk management., In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable., Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM)., Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas., Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors., Finally, we discuss some financial applications of the GP-LVM model.",11,5.88785046728972,9.727272727272727
346,"[""We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution."", 'In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator.', ""As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables."", 'In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training.', 'We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.']","[0, 0, 1, 0, 0]","[0.307692289352417, 0.17391303181648254, 0.43478259444236755, 0.25641024112701416, 0.15789473056793213]",rkeZRGbRW,"[""We introduce meta-adversarial learning, a new technique to regularize GANs, and propose a training method by explicitly controlling the discriminator's output distribution."", 'The paper proposes variance regularizing adversarial learning for training GANs to ensure that the gradient for the generator does not vanish']","['study  generative adversarial network  variance discriminator output affect generator ability learn data distribution ', 'particular  contrast result various wellknown technique training gans discriminator nearoptimal updated multiple time per update generator ', 'alternative  propose additional method train gans explicitly modeling discriminator output bimodal gaussian distribution realfake indicator variable ', 'order  train gaussian classifier match target bimodal distribution implicitly metaadversarial training ', 'observe new method  trained together strong discriminator  provides meaningful  nonvanishing gradient ']","We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution., In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator., As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables., In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training., We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.",13,6.089285714285714,8.615384615384615
347,"['We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agents policy can be used to aid efficient exploration.', 'The parameters of the noise are learned with gradient descent along with the remaining network weights.  ', 'NoisyNet is straightforward to implement and adds little computational overhead.', 'We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.']","[1, 0, 0, 0]","[0.6666666865348816, 0.1818181723356247, 0.0714285671710968, 0.13793103396892548]",rywHCPkAW,"['A deep reinforcement learning agent with parametric noise added to its weights can be used to aid efficient exploration.', 'This paper introduces NoisyNets, neural networks whose parameters are perturbed by a parametric noise function, that obtain substantial performance improvement over baseline deep reinforcement learning algorithms.', ""New exploration method for deep RL by injecting noise into deep networks' weights, with the noise taking various forms""]","['introduce noisynet  deep reinforcement learning agent parametric noise added weight  show induced stochasticity agent  policy used aid efficient exploration ', 'parameter noise learned gradient descent along remaining network weight ', 'noisynet straightforward implement add little computational overhead ', 'find replacing conventional exploration heuristic a3c  dqn dueling agent  entropy reward epsilongreedy respectively  noisynet yield substantially higher score wide range atari game  case advancing agent sub superhuman performance ']","We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agents policy can be used to aid efficient exploration., The parameters of the noise are learned with gradient descent along with the remaining network weights.  , NoisyNet is straightforward to implement and adds little computational overhead., We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",8,5.782178217821782,12.625
348,"['Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment.', 'Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent.', 'We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently.', 'The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization.', 'Active Neural Localizer is trained end-to-end with reinforcement learning.', 'We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine.', ""The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations."", 'We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.060606054961681366, 0.0, 0.8125, 0.12244897335767746, 0.23076923191547394, 0.0476190410554409, 0.04255318641662598, 0.1538461446762085]",ry6-G_66b,"['""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.', ""This paper formulates the problem of localisation on a known map using a belief network as an RL problem where the agent's goal is to minimise the number of steps to localise itself."", 'This is a clear and interesting paper that builds a parameterized network to select actions for a robot in a simulated environment']","['localization problem estimating location autonomous agent observation map environment ', 'traditional method localization  filter belief based observation  suboptimal number step required  decide action taken agent ', 'propose  active neural localizer   fully differentiable neural network learns localize efficiently ', 'proposed model incorporates idea traditional filteringbased localization method  using structured belief state multiplicative interaction propagate belief  combine policy model minimize number step required localization ', 'active neural localizer trained endtoend reinforcement learning ', 'use variety simulation environment experiment include random 2d maze  random maze doom game engine photorealistic environment unreal game engine ', 'result 2d environment show effectiveness learned policy idealistic setting result 3d environment demonstrate model capability learning policy perceptual model jointly rawpixel based rgb observation ', 'also show model trained random texture doom environment generalizes well photorealistic office space environment unreal engine ']","Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment., Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent., We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently., The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization., Active Neural Localizer is trained end-to-end with reinforcement learning., We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine., The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations., We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.",15,5.596244131455399,14.2
349,"['Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy.', 'Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency.', 'Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time.', 'However, they could only achieve inferior accuracy compared with ART models.', 'To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.', 'We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models.', 'Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks.', 'In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines.', 'It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.']","[0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.08888888359069824, 0.19999998807907104, 0.10526315122842789, 0.054054051637649536, 0.25531914830207825, 0.2448979616165161, 0.0416666604578495, 0.11764705181121826, 0.4897959232330322]",r1gGpjActQ,"['We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ', 'Distills knowledge from intermediary hidden states and attention weights to improve non-autoregressive neural machine translation.', 'Proposes to leverage well trained autoregressive model to inform the hidden states and the word alignment of non-autoregressive Neural Machine Translation models.']","['machine translation important realworld application  neural networkbased autoregressive translation  art  model achieved promising accuracy ', 'due unparallelizable nature autoregressive factorization  art model generate token one one decoding thus suffer high inference latency ', 'recently  nonautoregressive translation  nart  model proposed reduce inference time ', 'however  could achieve inferior accuracy compared art model ', 'improve accuracy nart model  paper  propose leverage hint welltrained art model train nart model ', 'define two hint machine translation task  hint hidden state hint word alignment  use hint regularize optimization nart model ', 'experimental result show nart model trained hint could achieve significantly better translation performance previous nart model several task ', 'particular  wmt14 ende deen task  obtain bleu score 2520 2952 respectively  largely outperforms previous nonautoregressive baseline ', 'even comparable strong lstmbased art model  2460 wmt14 ende   one order magnitude faster inference ']","Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy., Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency., Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time., However, they could only achieve inferior accuracy compared with ART models., To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model., We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models., Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks., In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines., It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.",20,5.690721649484536,9.7
350,"['Artificial neural networks are built on the basic operation of linear combination and non-linear activation function.', 'Theoretically this structure can approximate any continuous function with three layer architecture.', 'But in practice learning  the parameters of such network can be hard.', 'Also the choice of activation function can greatly impact the performance of the network.', 'In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function.', 'To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons.', 'We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks.', 'The results show that our network perform favorably when compared with similar structured network.', 'We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.2222222238779068, 0.25, 0.1875, 0.25806450843811035, 0.1818181723356247, 0.29999998211860657, 0.2083333283662796, 0.060606054961681366, 0.1249999925494194]",SyxknjC9KQ,"['Using mophological operation (dilation and erosion) we have defined a class of network which can approximate any continious function. ', 'This paper proposes to replace standard RELU/tanh units with a combination of dilation and erosion operations, observing that the new operator creates more hyper-planes and has more expressive power.', 'The authors introduce Morph-Net, a single layer neural network where the mapping is performed using morphological dilation and erosion.']","['artificial neural network built basic operation linear combination nonlinear activation function ', 'theoretically structure approximate continuous function three layer architecture ', 'practice learning parameter network hard ', 'also choice activation function greatly impact performance network ', 'paper proposing replace basic linear combination operation nonlinear operation away need additional nonlinear activation function ', 'end proposing use elementary morphological operation  dilation erosion  basic operation neuron ', 'show network  denoted morphnet  morphological operation approximate smooth function requiring le number parameter necessary normal neural network ', 'result show network perform favorably compared similar structured network ', 'carried experiment mnist  fashionmnist  cifar10 cifar100 ']","Artificial neural networks are built on the basic operation of linear combination and non-linear activation function., Theoretically this structure can approximate any continuous function with three layer architecture., But in practice learning  the parameters of such network can be hard., Also the choice of activation function can greatly impact the performance of the network., In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function., To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons., We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks., The results show that our network perform favorably when compared with similar structured network., We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.",11,5.853503184713376,14.272727272727273
351,"['With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment.', 'This work aims to advance the compression beyond the weights to the activations of DNNs.', 'We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.', 'Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency.', 'The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets.', 'With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.']","[0, 1, 0, 0, 0, 0]","[0.1818181723356247, 0.5517241358757019, 0.3333333432674408, 0.1428571343421936, 0.1111111044883728, 0.19230768084526062]",HyevnsCqtQ,"['This work advances DNN compression beyond the weights to the activations by integrating the activation pruning with the weight pruning. ', 'An integral model compression method that handles both weight and activation pruning, leading to more efficient network computation and effective reduction of the number of multiply-and-accumulate.', 'This article presents a novel approach to reduce the computation cost of deep neural networks by integrating activation pruning along with weight pruning and show that common techniques of exclusive weight pruning  increases the number of non-zero activations after ReLU.']","['rapidly scaling deep neural network  dnns   extensive research study network model compression weight pruning performed efficient deployment ', 'work aim advance compression beyond weight activation dnns ', 'propose integral pruning  ip  technique integrates activation pruning weight pruning ', 'learning different importance neuron response connection  generated network  namely ipnet  balance sparsity activation weight therefore improves execution efficiency ', 'feasibility effectiveness ipnet thoroughly evaluated various network model different activation function different datasets ', ' 05  disturbance testing accuracy  ipnet save 711   9635  computation cost  compared original dense model 58x 10x reduction activation weight number  respectively ']","With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment., This work aims to advance the compression beyond the weights to the activations of DNNs., We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning., Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency., The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets., With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.",13,5.909090909090909,11.0
352,"['The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\n', 'Standard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO).', 'Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD).', 'However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible.', 'Till now, there\nexist no simple solutions to circumvent this problem.\n', 'In this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations.', 'Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling.', 'In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.21621620655059814, 0.0555555522441864, 0.10810810327529907, 0.3243243098258972, 0.06666666269302368, 0.4444444477558136, 0.12121211737394333, 0.14999999105930328]",SkNSOjR9Y7,"['We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling', 'Introducting an importance sampling distribution and using samples from distribution to compute importance-weighted estimate of the gradient', 'This paper proposes to use important sampling to optimize VAE with discrete latent variables.']","['variational auto encoder  vae  popular generative latent variable model often applied representation learning ', 'standard vaes assume continuous valued latent variable trained maximization evidence lower bound  elbo  ', 'conventional method obtain differentiable estimate elbo reparametrized sampling optimize stochastic gradient descend  sgd  ', 'however  possible want train vaes discrete valued latent variable  since reparametrized sampling possible ', 'till  exist simple solution circumvent problem ', 'paper  propose easy method train vaes binary categorically valued latent representation ', 'therefore  use differentiable estimator elbo based importance sampling ', 'experiment  verify approach train two different vaes architecture bernoulli categorically distributed latent representation two different benchmark datasets ']","The Variational Auto Encoder (VAE) is a popular generative 
latent variable model that is often 
applied for representation learning.
, Standard VAEs assume continuous valued 
latent variables and are trained by maximization
of the evidence lower bound (ELBO)., Conventional methods obtain a 
differentiable estimate of the ELBO with reparametrized sampling and
optimize it with Stochastic Gradient Descend (SGD)., However, this is not possible if 
we want to train VAEs with discrete valued latent variables, 
since reparametrized sampling is not possible., Till now, there
exist no simple solutions to circumvent this problem.
, In this paper, we propose an easy method to train VAEs 
with binary or categorically valued latent representations., Therefore, we use a differentiable
estimator for the ELBO which is based on importance sampling., In experiments, we verify the approach and
train two different VAEs architectures with Bernoulli and 
Categorically distributed latent representations on two different benchmark
datasets.	",14,5.877551020408164,10.5
353,"['Distributed computing can significantly reduce the training time of neural networks.', 'Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers.', 'In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.\n    \n', 'We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead.', 'DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.\n\n', 'Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule.', 'For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.']","[0, 0, 0, 0, 0, 1, 0]","[0.0, 0.18518517911434174, 0.04651162400841713, 0.23076923191547394, 0.07843136787414551, 0.3333333432674408, 0.1538461446762085]",SkGQujR5FX,"['A new distributed asynchronous SGD algorithm that achieves state-of-the-art accuracy on existing architectures without any additional tuning or overhead.', 'Proposes an improvement to existing ASGD approaches at mid-size scaling using momentum with SGD for asynchronous training across a distributed worker pool.', 'This paper addresses the gradient staleness vs parallel performance problem in distributed deep learning training, and proposes an approach to estimate future model parameters at the slaves to reduce communication latency effects.']","['distributed computing significantly reduce training time neural network ', 'despite potential  however  distributed training widely adopted  scaling training process difficult  existing sgd method require substantial tuning hyperparameters learning schedule achieve sufficient accuracy increasing number worker ', 'practice  tuning prohibitively expensive given huge number potential hyperparameter configuration effort required test one ', 'propose dana  novel approach scale outofthebox large cluster using hyperparameters learning schedule optimized training single worker  maintaining similar final accuracy without additional overhead ', 'dana estimate future value model parameter adapting nesterov accelerated gradient distributed setting  mitigates effect gradient staleness  one main difficulty scaling sgd worker ', 'evaluation three stateoftheart network architecture three datasets show dana scale well better existing work without tune hyperparameters tweak learning schedule ', 'example  dana achieves 7573  accuracy imagenet training resnet50 16 worker  similar nondistributed baseline ']","Distributed computing can significantly reduce the training time of neural networks., Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers., In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.
    
, We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead., DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.

, Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule., For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.",17,5.958974358974359,11.470588235294118
354,"['This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.', 'The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes.', 'We propose a cascaded learning strategy for these local networks.', 'In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement.', 'Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.']","[1, 0, 0, 0, 0]","[0.4571428596973419, 0.08695651590824127, 0.307692289352417, 0.1904761791229248, 0.1875]",B1x-LjAcKX,"['We propose a new learning algorithm of deep neural networks, which unlocks the layer-wise dependency of backpropagation.', 'An alternative training paradigm for DNIs in which the auxiliary module is trained to approximate directly the final output of the original model, offering side benefits.', 'Describes a method of training neural networks without update locking.']","['paper proposes novel approach train deep neural network unlocking layerwise dependency backpropagation training ', 'approach employ additional module called local critic network besides main network model trained  used obtain error gradient without complete feedforward backward propagation process ', 'propose cascaded learning strategy local network ', 'addition  approach also useful multimodel perspective  including structural optimization neural network  computationally efficient progressive inference  ensemble classification performance improvement ', 'experimental result show effectiveness proposed approach suggest guideline determining appropriate algorithm parameter ']","This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training., The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes., We propose a cascaded learning strategy for these local networks., In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement., Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.",10,6.679611650485437,10.3
355,"['\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs.', 'Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  ', 'However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory.', 'We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.', 'ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation.', 'We check the viability of ARTBP on two tasks.', 'First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably.', 'Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.0, 0.04878048226237297, 0.15789473056793213, 0.1764705777168274, 0.25806450843811035, 0.08695651590824127, 0.13333332538604736, 0.0714285671710968]",rkrWCJWAW,"['Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly.', 'Proposes stochastic determination methods for truncation points in backpropagation through time.', 'A new approximation to backpropagation through time to overcome the computational and memory loads that arise when having to learn from long sequences.']","['emph  truncated backpropagation time   truncated bptt  cite  jaeger2002tutorial   widespread method learning recurrent computational graph ', 'truncated bptt keep computational benefit emph  backpropagation time   bptt cite  werbos  bptt   relieving need complete backtrack whole data sequence every step ', 'however  truncation favor shortterm dependency  gradient estimate truncated bptt biased  benefit convergence guarantee stochastic gradient theory ', 'introduce emph  anticipated reweighted truncated backpropagation   artbp   algorithm keep computational benefit truncated bptt  providing unbiasedness ', 'artbp work using variable truncation length together carefully chosen compensation factor backpropagation equation ', 'check viability artbp two task ', 'first  simple synthetic task careful balancing temporal dependency different scale needed  truncated bptt display unreliable performance  worst case scenario  divergence  artbp converges reliably ', 'second  penn treebank characterlevel language modelling cite  ptbproc   artbp slightly outperforms truncated bptt ']","\emph{Truncated Backpropagation Through Time} (truncated BPTT, \cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs., Truncated BPTT keeps the computational benefits of \emph{Backpropagation Through Time} (BPTT \cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  , However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory., We introduce \emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness., ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation., We check the viability of ARTBP on two tasks., First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably., Second, on Penn Treebank character-level language modelling \cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.
",19,6.84472049689441,8.473684210526315
356,"['Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.\n', 'Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes.', 'However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks.', 'In this paper, we investigate methods attacking GCNs by adding fake nodes.', 'A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones.', 'In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.  ', 'Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node.']","[0, 0, 0, 1, 0, 0, 0]","[0.07407406717538834, 0.20689654350280762, 0.06896550953388214, 0.3636363446712494, 0.1875, 0.17142856121063232, 0.307692289352417]",rke8ZhCcFQ,"['non-targeted and targeted attack on GCN by adding fake nodes', 'The authors propose a new adversarial technique to add ""fake"" nodes to fool a GCN-based classifier']","['graph convolutional network  gcns  widely used classifying graph node semisupervised setting ', 'previous work shown gcns vulnerable perturbation adjacency feature matrix existing node ', 'however  unrealistic change connection existing node many application  existing user social network ', 'paper  investigate method attacking gcns adding fake node ', 'greedy algorithm proposed generate adjacency feature matrix fake node  aiming minimize classification accuracy existing one ', 'additional  introduce discriminator classify fake node real node  propose greedygan algorithm simultaneously update discriminator attacker  make fake node indistinguishable real one ', 'nontargeted attack decrease accuracy gcn 010  targeted attack reach success rate 099 attacking whole datasets  094 average attacking single node ']","Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.
, Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes., However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks., In this paper, we investigate methods attacking GCNs by adding fake nodes., A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones., In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.  , Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node.",16,5.438271604938271,10.125
357,"['Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain.', 'Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer.', 'RNN uses a chain of repeating cells to model the sequence data.', 'However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling.', 'Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.\n\n', 'In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer.', 'ART is in a recurrent manner that different cells share the same parameters.', 'Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain.', 'This strategy enables ART to capture the word collocation across domains in a more flexible way.', 'We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis).', 'ART outperforms the state-of-the-arts over all experiments.\n']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3571428656578064, 0.1249999925494194, 0.17391303181648254, 0.29411762952804565, 0.2222222238779068, 0.23076923191547394, 0.0, 0.0714285671710968, 0.2222222238779068, 0.0714285671710968, 0.0]",ByldlhAqYQ,"['Transfer learning for sequence via learning to align cell-level information across domains.', 'The paper proposed to use RNN/LSTM with collocation alignment as a representation learning method for transfer learning/domain adaptation in NLP.']","['transfer learning aim solve data sparsity specific domain applying information another domain ', 'given sequence  eg  natural language sentence   transfer learning  usually enabled recurrent neural network  rnn   represent sequential information transfer ', 'rnn us chain repeating cell model sequence data ', 'however  previous study neural network based transfer learning simply transfer information across whole layer  unfeasible seq2seq sequence labeling ', 'meanwhile  layerwise transfer learning mechanism also lose finegrained celllevel information source domain ', 'paper  proposed aligned recurrent transfer  art  achieve celllevel information transfer ', 'art recurrent manner different cell share parameter ', 'besides transferring corresponding information position  art transfer information collocated word source domain ', 'strategy enables art capture word collocation across domain flexible way ', 'conducted extensive experiment sequence labeling task  po tagging  ner  sentence classification  sentiment analysis  ', 'art outperforms stateofthearts experiment ']","Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain., Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer., RNN uses a chain of repeating cells to model the sequence data., However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling., Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.

, In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer., ART is in a recurrent manner that different cells share the same parameters., Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain., This strategy enables ART to capture the word collocation across domains in a more flexible way., We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis)., ART outperforms the state-of-the-arts over all experiments.
",22,6.021978021978022,7.913043478260869
358,"['Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world.', 'We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution.', 'Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function.', 'To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state.', 'Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.']","[0, 1, 0, 0, 0]","[0.1111111044883728, 0.39344263076782227, 0.17391303181648254, 0.12765957415103912, 0.2857142686843872]",SJGvns0qK7,"['We formulate model uncertainty in Reinforcement Learning as a continuous Bayes-Adaptive Markov Decision Process and present a method for practical and scalable Bayesian policy optimization.', 'Using a Bayesian approach, there is a better trade-off between exploration and exploitation in RL']","['addressing uncertainty critical autonomous system robustly adapt real world ', 'formulate problem model uncertainty continuous bayesadaptive markov decision process  bamdp   agent maintains posterior distribution latent model parameter given history observation maximizes expected longterm reward respect belief distribution ', 'algorithm  bayesian policy optimization  build recent policy optimization algorithm learn universal policy navigates explorationexploitation tradeoff maximize bayesian value function ', 'address challenge discretizing continuous latent parameter space  propose new policy network architecture encodes belief distribution independently observable state ', 'method significantly outperforms algorithm address model uncertainty without explicitly reasoning belief distribution competitive stateoftheart partially observable markov decision process solver ']","Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world., We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution., Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function., To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state., Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.",9,6.635036496350365,15.222222222222221
359,"['For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.\n', 'We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model.', 'In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions.', ""The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples."", 'We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.\n']","[0, 1, 0, 0, 0]","[0.14035087823867798, 0.4583333134651184, 0.21276594698429108, 0.13333332538604736, 0.14814814925193787]",HkxKH2AcFm,"['We argue that GAN benchmarks must require a large sample from the model to penalize memorization and investigate whether neural network divergences have this property.', 'Authors propose criterion for evaluating the quality of samples produced by a Generative Adversarial Network.']","['many evaluation metric commonly used benchmark unconditional image generation  trivially memorizing training set attains better score model considered stateoftheart  consider problematic ', 'clarify necessary condition evaluation metric behave way  estimating function must require large sample model ', 'search metric  turn neural network divergence  nnds   defined term neural network trained distinguish distribution ', 'resulting benchmark   training set memorization  still perceptually correlated computable sample ', 'survey past work using nnds evaluation  implement example blackbox metric based idea  validate experimentally measure notion generalization ']","For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.
, We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model., In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions., The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples., We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.
",11,5.666666666666667,12.0
360,"['Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data.', 'In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.', 'The dialogue acts are generally designed and reveal how people engage in social chat.', 'Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation.', 'With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.']","[0, 1, 0, 0, 0]","[0.3199999928474426, 0.4000000059604645, 0.20000000298023224, 0.09999999403953552, 0.043478257954120636]",Bym0cU1CZ,"['open domain dialogue generation with dialogue acts', 'The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues', 'The paper describes a technique to incorporate dialog acts into neural conversational agents']","['conventional method model open domain dialogue generation black box endtoend learning large scale conversation data ', 'work  make first step open black box introducing dialogue act open domain dialogue generation ', 'dialogue act generally designed reveal people engage social chat ', 'inspired analysis real data  propose jointly modeling dialogue act selection response generation  perform learning humanhuman conversation tagged dialogue act classifier reinforcement approach optimizing model longterm conversation ', 'dialogue act  achieve significant improvement stateoftheart method response quality given context longterm conversation machinemachine simulation humanmachine conversation  also capable explaining achievement made ']","Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data., In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation., The dialogue acts are generally designed and reveal how people engage in social chat., Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation., With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.",10,5.882352941176471,13.6
361,"['We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics.', 'Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed.', 'Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping.', 'This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.\n\n', 'We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.', 'A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique.', 'The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.\n\n', 'Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping.', 'These are verified extensively.', 'In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.2380952388048172, 0.05882352590560913, 0.11764705181121826, 0.060606054961681366, 0.2222222238779068, 0.24390242993831635, 0.2181818187236786, 0.23529411852359772, 0.0, 0.19354838132858276]",H1VjBebR-,"['Our hypothesis is that given two domains, the lowest complexity mapping that has a low discrepancy approximates the target mapping.', 'The paper addresses the problem of learning mappings between different domains without any supervision, stating three conjectures.', 'Demonstrates that in unsupervised learning on unaligned data it is possible to learn the between domains mapping using GAN only without a reconstruction loss.']","['discus feasibility following learning problem  given unmatched sample two domain nothing else  learn mapping two  preserve semantics ', 'due lack paired sample without definition semantic information  problem might seem illposed ', 'specifically  typical case  seems possible build infinitely many alternative mapping every target mapping ', 'apparent ambiguity stand sharp contrast recent empirical success solving problem ', 'identify abstract notion aligning two domain semantic way concrete term minimal relative complexity ', 'theoretical framework measuring complexity composition function developed order show reasonable expect minimal complexity mapping unique ', 'measured complexity used directly related depth neural network learned semantically aligned mapping could captured simply learning using architecture much bigger minimal architecture ', 'various prediction made based hypothesis semantic alignment captured minimal mapping ', 'verified extensively ', 'addition  new mapping algorithm proposed shown lead better mapping result ']","We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics., Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed., Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping., This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.

, We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity., A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique., The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.

, Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping., These are verified extensively., In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.",16,5.463768115942029,12.9375
362,"['We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming.', 'This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.']","[0, 1]","[0.09090908616781235, 0.19999998807907104]",HJgeEh09KQ,"['We refine the over-approximation results from incomplete verifiers using MILP solvers to prove more robustness properties than state-of-the-art. ', 'Introduces a verifier that obtains improvement on precision of incomplete verifiers and scalability of the complete verifiers using over-parameterization, mixed integer linear programming and linear programming relaxation.', 'A mixed strategy to obtain better precision on robustness verifications of feed-forward neural networks with piecewise linear activation functions, achieving better precision than incomplete verifiers and more scalability than complete verifiers.']","['present novel approach certification neural network adversarial perturbation combine scalable overapproximation method precise  mixed integer  linear programming ', 'result significantly better precision stateoftheart verifier challenging feedforward convolutional neural network piecewise linear activation function ']","We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming., This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.",2,7.391304347826087,23.0
363,"['A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data.', 'In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)).', 'Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs?', 'In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization.', 'In particular, we investigate three key design factorsindependence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearitiesin order to pin down their empirical effects.', 'We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction.']","[0, 0, 0, 1, 0, 0]","[0.19512194395065308, 0.20408162474632263, 0.2926829159259796, 0.6399999856948853, 0.19607841968536377, 0.2800000011920929]",rJxEso0osm,"['Are HMMs a special case of RNNs? We investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization and provide new insights.', 'This paper explores if HMMs are a special case of RNNs using language modeling and POS tagging']","['distinct commonality hmms rnns learn hidden representation sequential data ', 'addition  noted backward computation baumwelch algorithm hmms special case backpropagation algorithm used neural network  eisner  2016   ', 'observation suggest  despite many apparent difference  hmms special case rnns ', 'paper  show indeed case  investigate series architectural transformation hmms rnns  theoretical derivation empirical hybridization ', 'particular  investigate three key design factorsindependence assumption hidden state observation  placement softmaxes  use nonlinearitiesin order pin empirical effect ', 'present comprehensive empirical study provide insight interplay expressivity interpretability model family respect language modeling partsofspeech induction ']","A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data., In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016))., Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs?, In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization., In particular, we investigate three key design factorsindependence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearitiesin order to pin down their empirical effects., We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction.",15,5.849673202614379,10.2
364,"['Deep neural networks have been tremendously successful in a number of tasks.\n', 'One of the main reasons for this is their capability to automatically\n', 'learn representations of data in levels of abstraction,\n', 'increasingly disentangling the data as the internal transformations are applied.\n', 'In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\n', 'This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\n', ""The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\n"", 'Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.19999998807907104, 0.13793103396892548, 0.1666666567325592, 0.07407406717538834, 0.7317073345184326, 0.21276594698429108, 0.20512819290161133, 0.0555555522441864]",ByzvHagA-,"['We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.', 'This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network in order to disentangle the latent representation']","['deep neural network tremendously successful number task ', 'one main reason capability automatically', 'learn representation data level abstraction ', 'increasingly disentangling data internal transformation applied ', 'paper propose novel regularization method penalize covariance dimension hidden layer network  something benefit disentanglement ', 'make network learn nonlinear representation linearly uncorrelated  yet allows model obtain good result number task  demonstrated experimental evaluation ', 'proposed technique used find dimensionality underlying data  effectively disables dimension nt needed ', 'approach simple computationally cheap  applied regularizer gradientbased learning model ']","Deep neural networks have been tremendously successful in a number of tasks.
, One of the main reasons for this is their capability to automatically
, learn representations of data in levels of abstraction,
, increasingly disentangling the data as the internal transformations are applied.
, In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.
, This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.
, The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.
, Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.",13,5.671428571428572,10.76923076923077
365,"['This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.', 'Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only.', 'Classification of an input can be inferred from the maximum response of the filters.', 'A multiple check with multiple versions of filters can diminish fluctuation and yields better performance.', 'This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure.', 'In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.']","[0, 0, 0, 0, 0, 1]","[0.12903225421905518, 0.15094339847564697, 0.14814814925193787, 0.06896550953388214, 0.1904761791229248, 0.7878788113594055]",r1gKNs0qYX,"['The proposed scheme mimics the classification process mediated by a series of one component picking.', 'A method to increase accuracy of deep-nets on multi-class classification tasks seemingly by a reduction of multi-class to binary classification.', 'A novel classification procedure of discerning, maxmum response, and multiple check to improve accuracy of mediocre networks and enhance feedforward networks.']","['report introduces training recognition scheme  classification realized via classwise discerning ', 'trained datasets whose label randomly shuffled except one class interest  neural network learns classwise parameter value  remolds feature sorter feature filter  discerns object belonging one class ', 'classification input inferred maximum response filter ', 'multiple check multiple version filter diminish fluctuation yield better performance ', 'scheme discerning  maximum response multiple check method general viability improve performance feedforward network  filter training promising feature abstraction procedure ', 'contrast direct sorting  scheme mimic classification process mediated series one component picking ']","This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning., Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only., Classification of an input can be inferred from the maximum response of the filters., A multiple check with multiple versions of filters can diminish fluctuation and yields better performance., This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure., In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.",13,5.642857142857143,10.76923076923077
366,"['A long-held conventional wisdom states that larger models train more slowly when using gradient descent.', 'This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step.', 'In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\n', 'We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal.', ""Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n""]","[0, 0, 1, 0, 0]","[0.25, 0.25, 0.2666666507720947, 0.1875, 0.06451612710952759]",S1lPShAqFm,"['Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.', 'This paper shows that wider RNNs improve convergence speed when applied to NLP problems, and by extension the effect of increasing the widths in deep neural networks on the convergence of optimization', 'This paper characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge, and presents further empirical observations on the effects of over-parametrization in neural network training.']","['longheld conventional wisdom state larger model train slowly using gradient descent ', 'work challenge widelyheld belief  showing larger model potentially train faster despite increasing computational requirement training step ', 'particular  study effect network structure  depth width  halting time show larger model  wider model particular  take fewer training step converge ', 'design simple experiment quantitatively characterize effect overparametrization weight space traversal ', 'result show halting time improves growing model width three different application  improvement come factor  distance initialized weight converged weight shrink powerlawlike relationship  average step size grows powerlawlike relationship  gradient vector become aligned traversal ']","A long-held conventional wisdom states that larger models train more slowly when using gradient descent., This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step., In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.

, We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal., Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.
",10,6.192592592592592,13.5
367,"['Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research.', 'Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes.', 'In this work, we consider a recently identified class of bugs called variable-misuse bugs.', 'The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction.', 'We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.', 'We present multi-headed pointer networks for this purpose, with one head each for localization and repair.', 'The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.']","[0, 0, 0, 0, 1, 0, 0]","[0.1818181723356247, 0.13793103396892548, 0.07692307233810425, 0.05882352590560913, 0.2666666507720947, 0.2142857164144516, 0.12121211737394333]",ByloJ20qtm,"['Multi-headed Pointer Networks for jointly learning to localize and repair Variable Misuse bugs', 'Proposes a LSTM based model with pointers to break the problem of VarMisuse down into multiple steps.', 'This paper presents an LSTM-based model for bug detection and repair of the VarMisuse bug, and demonstrates significant improvements compared to prior approaches on several datasets.']","['due potential improve programmer productivity software quality  automated program repair active topic research ', 'newer technique harness neural network learn directly example buggy program fix ', 'work  consider recently identified class bug called variablemisuse bug ', 'stateoftheart solution variable misuse enumerates potential fix possible bug location program  selecting best prediction ', 'show beneficial train model jointly directly localizes repair variablemisuse bug ', 'present multiheaded pointer network purpose  one head localization repair ', 'experimental result show joint model significantly outperforms enumerative solution us pointer based model repair alone ']","Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research., Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes., In this work, we consider a recently identified class of bugs called variable-misuse bugs., The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction., We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs., We present multi-headed pointer networks for this purpose, with one head each for localization and repair., The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.",11,5.7,11.818181818181818
368,"['Classification and clustering have been studied separately in machine learning and computer vision.', 'Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution.', 'We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints.', 'Instead, we emphasize on the compositionality of the real world structures and objects.', 'In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms.', 'The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance.', 'This, by no means, suggests that other methods do not hold merits.', 'For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.06896551698446274, 0.0, 0.05128204822540283, 0.040816325694322586, 0.0, 0.0]",Skvin0GWM,"['Human-like Clustering with CNNs', 'The paper validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods by noting their ability to interpret the context of every input point due to a large field of view.', 'This work combines deep learning for feature representation with the task of human-like unsupervised grouping.']","['classification clustering studied separately machine learning computer vision ', 'inspired recent success deep learning model solving various vision problem  eg  object recognition  semantic segmentation  fact human serve gold standard assessing clustering algorithm   advocate unified treatment two problem suggest hierarchical framework progressively build complex pattern top simpler one  eg  convolutional neural network  offer promising solution ', 'dwell much learning mechanism framework still matter debate  respect biological constraint ', 'instead  emphasize compositionality real world structure object ', 'particular  show cnns  trained end end using back propagation noisy label  able cluster data point belonging several overlapping shape  much better state art algorithm ', 'main takeaway lesson study mechanism human vision  particularly hierarchal organization visual ventral stream taken account clustering algorithm  eg  learning representation unsupervised manner minimum supervision  reach human level clustering performance ', ' mean  suggests method hold merit ', 'example  method relying pairwise affinity  eg  spectral clustering  successful many case still fail case  eg  overlapping cluster  ']","Classification and clustering have been studied separately in machine learning and computer vision., Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution., We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints., Instead, we emphasize on the compositionality of the real world structures and objects., In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms., The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance., This, by no means, suggests that other methods do not hold merits., For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).",26,5.524590163934426,9.384615384615385
369,"['Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features.', 'Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  ', 'This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models.', 'We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  ', 'In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  ', 'We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory.', 'We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.']","[0, 0, 0, 1, 0, 0, 0]","[0.2448979616165161, 0.29629629850387573, 0.1599999964237213, 0.6037735939025879, 0.21276594698429108, 0.23999999463558197, 0.12244897335767746]",S1E3Ko09F7,"['We develop two linear-complexity algorithms for model-agnostic model interpretation based on the Shapley value, in the settings where the contribution of features to the target is well-approximated by a graph-structured factorization.', 'The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability.', 'This paper proposes two methods for instance-wise feature importance scoring using Shapely values, and provides two efficient methods of computing approximate Shapely values when there is a known structure relating the features.']","['instancewise feature scoring method model interpretation  yield  test instance  vector importance score associated feature ', 'method based shapley score proposed fair way computing feature attribution  incur exponential complexity number feature ', 'combinatorial explosion arises definition shapley value prevents method scalable large data set complex model ', 'focus setting data graph structure  contribution feature target variable wellapproximated graphstructured factorization ', 'setting  develop two algorithm linear complexity instancewise feature importance scoring blackbox model ', 'establish relationship method shapley value closely related concept known myerson value cooperative game theory ', 'demonstrate language image data algorithm compare favorably method using quantitative metric human evaluation ']","Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features., Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  , This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models., We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  , In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  , We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory., We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.",13,5.586826347305389,12.846153846153847
370,"['According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations.', 'This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct.', 'However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks.', 'In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.', 'We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data.', 'Using a 1-hot output code drastically decreases the number of local codes on the hidden layer.', 'The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks.', 'This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex.', 'The findings highlight how local codes should not be dismissed out of hand.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.2222222238779068, 0.13333332538604736, 0.3125, 0.21621620655059814, 0.04999999701976776, 0.0833333283662796, 0.11764705181121826, 0.1818181723356247, 0.09090908616781235]",HJXOfZ-AZ,"['Local codes have been found in feed-forward neural networks', 'A method for determining to what degree individual neurons in a hidden layer of an MLP encode a localist code, which is studied for different input representations.', 'Studies the development of localist representations in the hidden layers of feed-forward neural networks.']","['according parallel distributed processing  pdp  theory psychology  neural network  nn  learn distributed rather interpretable localist representation ', 'view held strongly researcher analysed single unit determine assumption correct ', 'however  recent result psychology  neuroscience computer science shown occasional existence local code emerging artificial biological neural network ', 'paper  undertake first systematic survey local code emerge feedforward neural network  using generated input output data known quality ', 'find number local code emerge nn follows welldefined distribution across number hidden layer neuron  peak determined size input data  number example presented sparsity input data ', 'using 1hot output code drastically decrease number local code hidden layer ', 'number emergent local code increase percentage dropout applied hidden layer  suggesting localist encoding may offer resilience noisy network ', 'data suggests localist coding emerge feedforward pdp network suggests condition may lead interpretable localist representation cortex ', 'finding highlight local code dismissed hand ']","According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations., This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct., However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks., In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities., We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data., Using a 1-hot output code drastically decreases the number of local codes on the hidden layer., The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks., This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex., The findings highlight how local codes should not be dismissed out of hand.",17,5.4774774774774775,13.058823529411764
371,"['Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.', 'Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values.', 'In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities.', 'We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities.', 'We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy.', 'Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.']","[0, 0, 1, 0, 0, 0]","[0.1428571343421936, 0.09090908616781235, 0.2702702581882477, 0.05714285373687744, 0.07692307233810425, 0.25]",By03VlJGG,"['Extending relational modeling to support multimodal data using neural encoders.', 'This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information, and presents a model able to encode all sorts of information when scoring triples.', 'The paper is about incorporating information from different modalities into link prediction approaches']","['representing entity relation embedding space wellstudied approach machine learning relational data ', 'existing approach however primarily focus simple link structure finite set entity  ignoring variety data type often used relational database  text  image  numerical value ', 'approach  propose multimodal embedding using different neural encoders variety data  combine existing model learn embeddings entity ', 'extend existing datasets create two novel benchmark  yago10plus movielens100kplus  contain additional relation textual description image original entity ', 'demonstrate model utilizes additional information effectively provide gain accuracy ', 'moreover  test learned multimodal embeddings using predict missing multimodal attribute ']","Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data., Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values., In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities., We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities., We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy., Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.",15,5.8478260869565215,9.2
372,"['An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. \n', 'In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\n', 'In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.', 'In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\n', 'In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.\n', 'This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.\n', 'For example, in LSTM based language modeling (LM), we obtain 21\\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\\% of model parameters and 70\\% of computations in total, as compared to the baseline large LSTM LM.\n', 'To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.21052631735801697, 0.2916666567325592, 0.23076923191547394, 0.16326530277729034, 0.17910447716712952, 0.08695651590824127, 0.14492753148078918, 0.25]",r1uOhfb0W,"['Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. ', 'The authors propose a procedure to generate an ensemble of sparse structured models', 'A new framework for training ensemble neural networks that uses SG-MCMC methods within deep learning, and then increases computational efficiency by group sparsity+pruning.', 'This paper explores the use of FNN and LSTMs to make bayesian model averaging more computationally feasible and improve average model performance.']","['ensemble neural network known robust accurate individual network  however usually linearlyincreased cost training testing ', 'work  propose twostage method learn sparse structured ensemble  ss  neural network ', 'first stage  run sgmcmc group sparse prior draw ensemble sample posterior distribution network parameter ', 'second stage  apply weightpruning sampled network perform retraining remained connection ', 'way learning ss sgmcmc pruning  achieve high prediction accuracy since sgmcmc enhances exploration modelparameter space  also reduce memory computation cost significantly training testing nn ensemble ', 'thoroughly evaluated experiment learning sse ensemble fnns lstms ', 'example  lstm based language modeling  lm   obtain 21  relative reduction lm perplexity learning sse 4 large lstm model  30  model parameter 70  computation total  compared baseline large lstm lm ', 'best knowledge  work represents first methodology empirical study integrating sgmcmc  group sparse prior network pruning together learning nn ensemble ']","An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. 
, In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.
, In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters., In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.
, In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.
, This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.
, For example, in LSTM based language modeling (LM), we obtain 21\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\% of model parameters and 70\% of computations in total, as compared to the baseline large LSTM LM.
, To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.",20,5.237442922374429,10.95
373,"['This paper introduces a new framework for data efficient and versatile learning.', 'Specifically:\n', '1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction.', 'ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. \n', '2) We introduce \\Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass.', '\\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.\n', '3) We evaluate \\Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time.', 'The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.25641024112701416, 0.14999999105930328, 0.4390243887901306, 0.2222222238779068, 0.19230768084526062, 0.1428571343421936, 0.1395348757505417]",HkxStoC5F7,"['Novel framework for meta-learning that unifies and extends a broad class of existing few-shot learning methods. Achieves strong performance on few-shot learning benchmarks without requiring iterative test-time inference.   ', 'This work tackles few-shot learning from a probabilistic inference viewpoint, achieving state-of-the-art despite simpler setup than many competitors']","['paper introduces new framework data efficient versatile learning ', 'specifically ', '1  develop mlpip  general framework metalearning approximate probabilistic inference prediction ', 'mlpip extends existing probabilistic interpretation metalearning cover broad class method ', '2  introduce versa    instance framework employing flexible versatile amortization network take fewshot learning datasets input  arbitrary number shot  output distribution taskspecific parameter single forward pas ', 'versa   substitute optimization test time forward pass inference network  amortizing cost inference relieving need second derivative training ', '3  evaluate versa   benchmark datasets method set new stateoftheart result  handle arbitrary number shot  classification  arbitrary number class train test time ', 'power approach demonstrated challenging fewshot shapenet view reconstruction task ']","This paper introduces a new framework for data efficient and versatile learning., Specifically:
, 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction., ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 
, 2) We introduce \Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass., \Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.
, 3) We evaluate \Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time., The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.",16,6.064102564102564,9.75
374,"['In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions.', 'However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results.', 'This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state.', 'To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives.', 'Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS).', 'We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.']","[0, 0, 0, 0, 1, 0]","[0.1621621549129486, 0.1395348757505417, 0.07843136787414551, 0.16326530277729034, 0.17391303181648254, 0.11999999731779099]",rkx0g3R5tX,"['Defining a partially mutual exclusive softmax loss for postive data and implementing a cooperative based sampling scheme', 'This paper presents Cooperative Importance Sampling towards resolving the problem of the mutually exclusive assumption of traditional softmax being biased when negative samples are not explicitly defined', 'This paper proposes PMES methods to relax the exclusive outcome assumption in softmax loss, demonstrating empirical merit in improving word2vec type of embedding models.']","['recent year  softmax together fast approximation become defacto loss function deep neural network multiclass prediction ', 'however  softmax used many problem fully fit multiclass framework softmax assumption mutually exclusive outcome lead biased result ', 'often case application language modeling  next event prediction matrix factorization  many potential outcome mutually exclusive  likely independent conditionally state ', 'end  set problem positive unlabeled data  propose relaxation original softmax formulation   given observed state  outcome conditionally independent share common set negative ', 'since operate regime explicit negative missing  create adversariallytrained model negative derive new negative sampling weighting scheme denote cooperative importance sampling  ci  ', 'show empirically advantage newly introduced negative sampling scheme pluging word2vec algorithm benching extensively negative sampling scheme language modeling matrix factorization task show large lift performance ']","In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions., However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results., This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state., To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives., Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS)., We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.",17,5.497584541062802,12.176470588235293
375,"['Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention.', 'Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos.', 'In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.', 'Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video.', 'The idea is to then train the student to minimize', ' (i)  the difference between the final representation computed by the student and the teacher and/or', '(ii) the difference between the distributions predicted by the teacher and the student.', 'This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification.', 'We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.09999999403953552, 0.20000000298023224, 0.20000000298023224, 0.0, 0.0, 0.0, 0.23529411852359772, 0.0]",H1GWAoRcKX,"['Teacher-Student framework for efficient video classification using fewer frames ', 'The paper proposes an idea to distill from a full video classification model a small model that only receives smaller number of frames.', 'The authors present a teacher-student network to solve video classification problem, proposing serial and parallel training algorithms aimed at reducing computational costs.']","['past year  various task involving video classification  description  summarization question answering received lot attention ', 'current model task compute encoding video treating sequence image going every image sequence  becomes computationally expensive longer video ', 'paper  focus task video classification aim reduce computational cost using idea distillation ', 'specifically  propose teacherstudent network wherein teacher look frame video student look small fraction frame video ', 'idea train student minimize', '  difference final representation computed student teacher andor', ' ii  difference distribution predicted teacher student ', 'smaller student network involves fewer computation still learns mimic teacher employed inference time video classification ', 'experiment youtube8m dataset show proposed student network reduce inference time upto 30  negligent drop performance ']","Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention., Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos., In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation., Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video., The idea is to then train the student to minimize,  (i)  the difference between the final representation computed by the student and the teacher and/or, (ii) the difference between the distributions predicted by the teacher and the student., This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification., We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.",15,5.151960784313726,13.6
376,"['Deep generative models have achieved impressive success in recent years.', 'Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively.', 'This paper aims to establish formal connections between GANs and VAEs through a new formulation of them.', 'We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively.', 'The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way.', 'For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples.', 'Experiments show generality and effectiveness of the transfered techniques.']","[0, 0, 0, 0, 0, 0, 1]","[0.1904761791229248, 0.10256409645080566, 0.0714285671710968, 0.08888888359069824, 0.1666666567325592, 0.054054051637649536, 0.19999998807907104]",rylSzl-R-,"['A unified statistical view of the broad class of deep generative models ', 'The paper develops a framework interpreting GAN algorithms as performing a form of variational inference on a generative model reconstructing an indicator variable of whether a sample is from the true of generative data distributions.']","['deep generative model achieved impressive success recent year ', 'generative adversarial network  gans  variational autoencoders  vaes   powerful framework deep generative model learning  largely considered two distinct paradigm received extensive independent study respectively ', 'paper aim establish formal connection gans vaes new formulation ', 'interpret sample generation gans performing posterior inference  show gans vaes involve minimizing kl divergence respective posterior inference distribution opposite direction  extending two learning phase classic wakesleep algorithm  respectively ', 'unified view provides powerful tool analyze diverse set existing model variant  enables transfer technique across research line principled way ', 'example  apply importance weighting method vae literature improved gan learning  enhance vaes adversarial mechanism leverage generated sample ', 'experiment show generality effectiveness transfered technique ']","Deep generative models have achieved impressive success in recent years., Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively., This paper aims to establish formal connections between GANs and VAEs through a new formulation of them., We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively., The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way., For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples., Experiments show generality and effectiveness of the transfered techniques.",15,6.182389937106918,10.6
377,"['Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications.', 'However, the interpretability of those models are often lacking.', 'On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists).', 'In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes.', 'The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists.', 'Thanks to the prediction power of neural networks, the rule lists from\t\t\t\t prototypes are more concise and hence provide better interpretability.', 'On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%.', 'Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.09090908616781235, 0.0, 0.08888888359069824, 0.12121211737394333, 0.2142857164144516, 0.1428571343421936, 0.21052631735801697, 0.17391303181648254]",r1gnQ20qYX,"['a method combining rule list learning and prototype learning ', 'Presents a new interpretable prediction framework, which combines rule based learning, prototype learning, and NNs, that is particularly applicable to longitudinal data.', 'This paper aims at tackling the lack of interpretability of deep learning models, and propose Prototype lEArning via Rule Lists (PEARL), which combines rule learning and prototype learning to achieve more accurate classification and makes the task of interpretability simpler.']","['deep neural network demonstrated promising prediction classification performance many healthcare application ', 'however  interpretability model often lacking ', 'hand  classical interpretable model rule list decision tree lead level accuracy deep neural network often complex interpret  due potentially large depth rule list  ', 'work  present pearl  prototype learning via rule list  iteratively us rule list guide neural network learn representative data prototype ', 'resulting prototype neural network provides accurate prediction  prediction easily explained prototype guiding rule list ', 'thanks prediction power neural network  rule list prototype concise hence provide better interpretability ', 'two realworld electronic healthcare record  ehr  datasets  pearl consistently outperforms baseline across datasets  especially achieving performance improvement conventional rule learning 28  prototype learning 3  ', 'experimental result also show resulting interpretation pearl simpler standard rule learning ']","Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications., However, the interpretability of those models are often lacking., On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists)., In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes., The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists., Thanks to the prediction power of neural networks, the rule lists from				 prototypes are more concise and hence provide better interpretability., On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%., Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.",17,5.7287234042553195,11.058823529411764
378,"['Generative Adversarial Networks (GANs) are powerful tools for realistic image generation.', 'However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time.', 'In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \\cite{deligan}.', 'The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process.', 'Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence.', 'We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.']","[0, 1, 0, 0, 0, 0]","[0.1249999925494194, 0.22727271914482117, 0.1702127605676651, 0.1818181723356247, 0.09090908616781235, 0.19999998807907104]",HyMuaiAqY7,"['This paper proposes a new Generative Adversarial Network that is more stable, more efficient, and produces better images than those of status-quo ', 'This paper combines Fisher-GAN and Deli-GAN', 'This paper combines Deli-GAN, which has a mixture prior distribution in latent space, and Fisher GAN, which uses Fisher IPM instead of JSD as an objective.']","['generative adversarial network  gans  powerful tool realistic image generation ', 'however  major drawback gans especially hard train  often requiring large amount data long training time ', 'paper propose delifisher gan  gan generates photorealistic image enforcing structure latent generative space using similar approach cite  deligan  ', 'structure latent space consider paper modeled mixture gaussians  whose parameter learned training process ', 'furthermore  improve stability efficiency  use fisher integral probability metric divergence measure gan model  instead jensenshannon divergence ', 'show experiment delifisher gan performs better dcgan  wgan  fisher gan measured inception score ']","Generative Adversarial Networks (GANs) are powerful tools for realistic image generation., However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time., In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \cite{deligan}., The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process., Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence., We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.",15,5.4338235294117645,9.066666666666666
379,"['Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially.', 'We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups.', 'This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise.', 'We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset.', 'The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset.', 'Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.']","[0, 1, 0, 0, 0, 0]","[0.12244897335767746, 0.5306122303009033, 0.1395348757505417, 0.20689654350280762, 0.11428570747375488, 0.16326530277729034]",Bk346Ok0W,"['We introduce a modular multi-sensor network architecture with an attentional mechanism that enables dynamic sensor selection on real-world noisy data from CHiME-3.', 'A generic neural architecture able to learn the attention that must be payed to different input channels depending on the relative quality of each sensor with respect to the others.', ' Considers the use of attention for sensor or channel selection with results on TIDIGITS and GRID showing a benefit of attention over concatentation of features.']","['recent work encoderdecoder model sequencetosequence mapping shown integrating temporal spatial attentional mechanism neural network increase performance system substantially ', 'report new modular network architecture applies attentional mechanism temporal spatial region input  sensor selection multisensor setup ', 'network called sensor transformation attention network  stan  evaluated scenario include presence natural noise synthetic dynamic noise ', 'demonstrate attentional signal responds dynamically changing noise level sensorspecific noise  leading reduced word error rate  wers  audio visual task using tidigits grid  also chime3  multimicrophone realworld noisy dataset ', 'improvement grows channel corrupted demonstrated chime3 dataset ', 'moreover  proposed stan architecture naturally introduces number advantage including ease removing sensor existing architecture  attentional interpretability  increased robustness variety noise environment ']","Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially., We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups., This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise., We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset., The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset., Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.",12,6.132530120481928,13.833333333333334
380,"['Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints.', 'Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection.', 'To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud.', 'The local neural network (NN) is used to generate the feature representations.', 'To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs.', 'The cloud NN is then trained based on the extracted intermediate representations for the target learning task.', 'We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task.', 'Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage.', 'The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset.']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.2222222238779068, 0.16326530277729034, 0.9375, 0.1860465109348297, 0.30434781312942505, 0.1702127605676651, 0.24561403691768646, 0.27586206793785095, 0.0476190447807312]",HJcjQTJ0W,"['To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate data representations, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud.', 'This paper proposes a technique to privatize data by learning a feature representation that is difficult to use for image reconstruction, but helpful for image classification.']","['massive data exist among user local platform usually support deep neural network  dnn  training due computation storage resource constraint ', 'cloudbased training scheme provide beneficial service suffer potential privacy risk due excessive user data collection ', 'enable cloudbased dnn training protecting data privacy simultaneously  propose leverage intermediate representation data  achieved splitting dnns deploying separately onto local platform cloud ', 'local neural network  nn  used generate feature representation ', 'avoid local training protect data privacy  local nn derived pretrained nns ', 'cloud nn trained based extracted intermediate representation target learning task ', 'validate idea dnn splitting characterizing dependency privacy loss classification accuracy local nn topology convolutional nn  cnn  based image classification task ', 'based characterization  propose privynet determine local nn topology  optimizes accuracy target learning task constraint privacy loss  local computation  storage ', 'efficiency effectiveness privynet demonstrated cifar10 dataset ']","Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints., Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection., To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud., The local neural network (NN) is used to generate the feature representations., To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs., The cloud NN is then trained based on the extracted intermediate representations for the target learning task., We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task., Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage., The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset.",16,5.768844221105527,12.4375
381,"['Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.', 'In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.', 'RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.', 'In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.', 'We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.', 'We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.', 'We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.', 'This is demonstrated on digit classification from serialised MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.', 'We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.']","[0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.05714285373687744, 0.22727271914482117, 0.13333332538604736, 0.0, 0.043478257954120636, 0.17391303181648254, 0.04999999329447746, 0.1428571343421936, 0.12765957415103912]",B1ZZTfZAW,"['Conditional recurrent GANs for real-valued medical sequences generation, showing novel evaluation approaches and an empirical privacy analysis.', 'Proposes to use synthetic data generated by GANs as a replacement for personally identifiable data in training ML models for privacy-sensitive applications', 'The authors propose a novel recurrent GAN architecture that generates continuous domain sequences, and evaluate it on several synthetic tasks and an ICU timeseries data task.', 'Proposes to use RGANs and RCGANs to generate synthetic sequences of actual data.']","['generative adversarial network  gans  shown remarkable success framework training model produce realisticlooking data ', 'work  propose recurrent gan  rgan  recurrent conditional gan  rcgan  produce realistic realvalued multidimensional time series  emphasis application medical data ', 'rgans make use recurrent neural network  rnns  generator discriminator ', 'case rcgans  rnns conditioned auxiliary information ', 'demonstrate model set toy datasets  show visually quantitatively  using sample likelihood maximum mean discrepancy  successfully generate realistic timeseries ', 'also describe novel evaluation method gans  generate synthetic labelled training dataset  evaluate real test set performance model trained synthetic data  viceversa ', 'illustrate metric rcgans generate timeseries data useful supervised training  minor degradation performance real test data ', 'demonstrated digit classification  serialised  mnist training early warning system medical dataset 17000 patient intensive care unit ', 'discus analyse privacy concern may arise using rcgans generate realistic synthetic medical time series data  demonstrate result differentially private training rcgan ']","Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data., In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data., RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator., In the case of RCGANs, both of these RNNs are conditioned on auxiliary information., We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series., We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa., We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data., This is demonstrated on digit classification from serialised MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit., We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.",18,5.628318584070796,12.555555555555555
382,"['Emphasis effects  visual changes that make certain elements more\n', 'prominent  are commonly used in information visualization to draw\n', 'the users attention or to indicate importance.', 'Although theoretical\n', 'frameworks of emphasis exist (that link visually diverse emphasis\n', 'effects through the idea of visual prominence compared to background\n', 'elements), most metrics for predicting how emphasis effects\n', 'will be perceived by users come from abstract models of human\n', 'vision which may not apply to visualization design.', 'In particular,\n', 'it is difficult for designers to know, when designing a visualization,\n', 'how different emphasis effects will compare and what level of one\n', 'effect is equivalent to what level of another.', 'To address this gap,\n', 'we carried out two studies that provide empirical evidence about\n', 'how users perceive different emphasis effects, using three visual\n', 'variables (colour, size, and blur/focus) and eight strength levels.\n', 'Results from gaze tracking, mouse clicks, and subjective responses\n', 'show that there are significant differences between visual variables\n', 'and between levels, and allow us to develop an initial understanding\n', 'of perceptual equivalence.', 'We developed a model from the data in\n', 'our first study, and used it to predict the results in the second; the\n', 'model was accurate, with high correlations between predictions and\n', 'real values.', 'Our studies and empirical models provide valuable new\n', 'information for designers who want to understand and control how\n', 'emphasis effects will be perceived by users.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.05882352590560913, 0.11764705181121826, 0.06451612710952759, 0.0624999962747097, 0.11764705181121826, 0.25, 0.34285715222358704, 0.0624999962747097, 0.17142856121063232, 0.2857142686843872, 0.0624999962747097, 0.0, 0.1764705777168274, 0.1818181723356247, 0.060606054961681366, 0.060606054961681366, 0.0, 0.11764705181121826, 0.0, 0.0, 0.1111111044883728, 0.060606054961681366, 0.5, 0.5882353186607361, 0.4516128897666931]",NxvF-PleYy,"['Our studies and empirical models provide valuable new information for designers who want to understand and control how emphasis effects will be perceived by users', 'This paper considers which visual highlighting is perceived faster in data visualization and how different highlighting methods compare to each other', 'Two studies on the efficacy of emphasis effects, one assessing levels of useful differences, and one more applied using actual different visualizations for a more ecologically valid investigation.']","['emphasis effect  visual change make certain element', 'prominent  commonly used information visualization draw', 'user  attention indicate importance ', 'although theoretical', 'framework emphasis exist  link visually diverse emphasis', 'effect idea visual prominence compared background', 'element   metric predicting emphasis effect', 'perceived user come abstract model human', 'vision may apply visualization design ', 'particular ', 'difficult designer know  designing visualization ', 'different emphasis effect compare level one', 'effect equivalent level another ', 'address gap ', 'carried two study provide empirical evidence', 'user perceive different emphasis effect  using three visual', 'variable  colour  size  blurfocus  eight strength level ', 'result gaze tracking  mouse click  subjective response', 'show significant difference visual variable', 'level  allow u develop initial understanding', 'perceptual equivalence ', 'developed model data', 'first study  used predict result second ', 'model accurate  high correlation prediction', 'real value ', 'study empirical model provide valuable new', 'information designer want understand control', 'emphasis effect perceived user ']","Emphasis effects  visual changes that make certain elements more
, prominent  are commonly used in information visualization to draw
, the users attention or to indicate importance., Although theoretical
, frameworks of emphasis exist (that link visually diverse emphasis
, effects through the idea of visual prominence compared to background
, elements), most metrics for predicting how emphasis effects
, will be perceived by users come from abstract models of human
, vision which may not apply to visualization design., In particular,
, it is difficult for designers to know, when designing a visualization,
, how different emphasis effects will compare and what level of one
, effect is equivalent to what level of another., To address this gap,
, we carried out two studies that provide empirical evidence about
, how users perceive different emphasis effects, using three visual
, variables (colour, size, and blur/focus) and eight strength levels.
, Results from gaze tracking, mouse clicks, and subjective responses
, show that there are significant differences between visual variables
, and between levels, and allow us to develop an initial understanding
, of perceptual equivalence., We developed a model from the data in
, our first study, and used it to predict the results in the second; the
, model was accurate, with high correlations between predictions and
, real values., Our studies and empirical models provide valuable new
, information for designers who want to understand and control how
, emphasis effects will be perceived by users.",38,5.572052401746725,6.026315789473684
383,"['Memory Network based models have shown a remarkable progress on the task of relational reasoning.\n', 'Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \n', 'Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\n', 'We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \n', ""We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \n"", 'As a result, our model is as simple as RN but the computational complexity is reduced to linear time.\n', 'It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset.']","[0, 0, 1, 0, 0, 0, 0]","[0.1904761791229248, 0.0476190410554409, 0.2857142686843872, 0.23255813121795654, 0.2083333283662796, 0.27272728085517883, 0.1904761791229248]",ByquB-WC-,"['A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.', 'Introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN).']","['memory network based model shown remarkable progress task relational reasoning ', 'recently  simpler yet powerful neural network module called relation network  rn  introduced ', 'despite architectural simplicity  time complexity relation network grows quadratically data  hence limiting application task largescaled memory ', 'introduce related memory network  endtoend neural network architecture exploiting memory network relation network structure ', 'follow memory network four component component operates similar relation network without taking pair object ', 'result  model simple rn computational complexity reduced linear time ', 'achieves stateoftheart result jointly trained babi10k storybased question answering babi dialog dataset ']","Memory Network based models have shown a remarkable progress on the task of relational reasoning.
, Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. 
, Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.
, We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. 
, We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. 
, As a result, our model is as simple as RN but the computational complexity is reduced to linear time.
, It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset.",12,5.90625,10.666666666666666
384,"['We investigate in this paper the architecture of deep convolutional networks.', 'Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.', 'We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.', 'The use of branches brings an additional form of regularization.', 'In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.', 'The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.', 'We refer to this branched architecture as ""coupled ensembles"".', 'The approach is very generic and can be applied with almost any neural network architecture.', 'With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.', 'For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  ', 'With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.19999998807907104, 0.2978723347187042, 0.2926829159259796, 0.1428571343421936, 0.4000000059604645, 0.1904761791229248, 0.0714285671710968, 0.1764705777168274, 0.09090908616781235, 0.1538461446762085, 0.08888888359069824]",Hk2MHt-3-,"['We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further.', 'The work proposes a reconfiguration of the existing state-of-the-art CNN model using a new branching architecture, with better performance.', 'This paper shows parameter-saving benefits of coupled ensembling.', 'Presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores.']","['investigate paper architecture deep convolutional network ', 'building existing state art model  propose reconfiguration model parameter several parallel branch global network level  branch standalone cnn ', 'show arrangement efficient way significantly reduce number parameter time improving performance ', 'use branch brings additional form regularization ', 'addition splitting parameter parallel branch  propose tighter coupling branch averaging logprobabilities ', 'tighter coupling favour learning better representation  even level individual branch  compared branch trained independently ', 'refer branched architecture  coupled ensemble  ', 'approach generic applied almost neural network architecture ', 'coupled ensemble densenetbc parameter budget 25m  obtain error rate 292   1568  150  respectively cifar10  cifar100 svhn task ', 'parameter budget  densenetbc error rate 346   1718   18  respectively ', 'ensemble coupled ensemble  densenetbc network  50m total parameter  obtain error rate 272   1513  142  respectively task ']","We investigate in this paper the architecture of deep convolutional networks., Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN., We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance., The use of branches brings an additional form of regularization., In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities., The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently., We refer to this branched architecture as ""coupled ensembles""., The approach is very generic and can be applied with almost any neural network architecture., With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks., For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  , With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.",26,5.47906976744186,8.26923076923077
385,"['Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few.', 'Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices.', 'To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly.', 'Yet, this acceleration comes at the cost of a larger error.', 'The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy.', 'This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations.', 'We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.']","[0, 0, 0, 0, 0, 1, 0]","[0.05405404791235924, 0.1111111044883728, 0.05405404791235924, 0.0, 0.10810810327529907, 0.1463414579629898, 0.0624999962747097]",HyfyN30qt7,"['Combine noise injection, gradual quantization and activation clamping learning to achieve state-of-the-art 3,4 and 5 bit quantization', 'Proposes to inject noise during training and clamp parameter values in a layer as well as activation output in neural network quantization.', 'A method for quantization of deep neural networks for classification and regression, using noise injection, clamping with learned maximum activations, and gradual block quantization to perform on-par or better than state-of-the-art methods.']","['convolutional neural network  cnn  popular many field including computer vision  speech recognition  natural language processing  name ', 'though deep learning lead groundbreaking performance domain  network used demanding computationally far realtime even gpu  power efficient therefore suit low power system mobile device ', 'overcome challenge  solution proposed quantizing weight activation network  accelerate runtime significantly ', 'yet  acceleration come cost larger error ', 'nice method proposed work train quantized neural network noise injection learned clamping  improve accuracy ', 'lead stateoftheart result various regression classification task  eg  imagenet classification architecture resnet183450 low 3bit weight 3 bit activation ', 'implement proposed solution fpga demonstrate applicability low power realtime application ']","Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few., Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices., To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly., Yet, this acceleration comes at the cost of a larger error., The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy., This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations., We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.",18,5.710843373493976,9.222222222222221
386,"['In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks.', 'Approaches that transfer information contained only in the final parameters of a source model will therefore struggle.', 'Instead, transfer learning at at higher level of abstraction is needed.', 'We propose Leap, a framework that achieves this by transferring knowledge across learning processes.', 'We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path.', 'Our framework leverages only information obtained during training and can be computed on the fly at negligible cost.', 'We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks.', 'Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.05405404791235924, 0.14999999105930328, 0.060606054961681366, 0.5945945978164673, 0.3461538553237915, 0.19512194395065308, 0.22727271914482117, 0.22727271914482117]",HygBZnRctX,"[""We propose Leap, a framework that transfers knowledge across learning processes by  minimizing the expected distance the training process travels on a task's loss surface."", 'The article proposes a novel meta-learning objective aimed at outperforming state-of-the-art approaches when dealing with collections of tasks that exhibit substantial between-task diversity']","['complex transfer learning scenario new task might tightly linked previous task ', 'approach transfer information contained final parameter source model therefore struggle ', 'instead  transfer learning higher level abstraction needed ', 'propose leap  framework achieves transferring knowledge across learning process ', 'associate task manifold training process travel initialization final parameter construct metalearning objective minimizes expected length path ', 'framework leverage information obtained training computed fly negligible cost ', 'demonstrate framework outperforms competing method  metalearning transfer learning  set computer vision task ', 'finally  demonstrate leap transfer knowledge across learning process demanding reinforcement learning environment  atari  involve million gradient step ']","In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks., Approaches that transfer information contained only in the final parameters of a source model will therefore struggle., Instead, transfer learning at at higher level of abstraction is needed., We propose Leap, a framework that achieves this by transferring knowledge across learning processes., We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path., Our framework leverages only information obtained during training and can be computed on the fly at negligible cost., We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks., Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.",13,5.88,11.538461538461538
387,"['Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned.', 'Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.', 'We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones.', 'DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance.', 'When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy.', 'The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains.', 'We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.']","[0, 0, 0, 1, 0, 0, 0]","[0.08695651590824127, 0.037735845893621445, 0.08695651590824127, 0.2545454502105713, 0.04081632196903229, 0.04347825422883034, 0.12765957415103912]",ryj0790hb,"['An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks.', 'Controller modules for increment learning on image classification datasets']","['given existing trained neural network  often desirable learn new capability without hindering performance already learned ', 'existing approach either learn suboptimal solution  require joint training  incur substantial increment number parameter added task  typically many original network ', 'propose method called deep adaptation network  dan  constrains newly learned filter linear combination existing one ', 'dans preserve performance original task  require fraction  typically 13   number parameter compared standard finetuning procedure converge le cycle training comparable better level performance ', 'coupled standard network quantization technique  reduce parameter cost around 3  original negligible loss accuracy ', 'learned architecture controlled switch various learned representation  enabling single network solve task multiple different domain ', 'conduct extensive experiment showing effectiveness method range image classification task explore different aspect behavior ']","Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned., Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network., We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones., DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance., When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy., The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains., We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.",14,5.683060109289618,13.071428571428571
388,"['High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications.', 'This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including', '1) channel-wise scale factors of weights, especially for depthwise convolution,', '2) Winograd convolution, and', '3) topology-wise 8-bit support.', 'We experiment the techniques on top of a widely-used deep learning framework.', 'The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining.', 'We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution.', 'We show that the inference throughput\n', 'and latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline.', 'We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks.', 'All the code and models will be publicly available soon.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3125, 0.7096773982048035, 0.07999999821186066, 0.0, 0.10526315122842789, 0.2222222238779068, 0.17142856121063232, 0.4000000059604645, 0.1904761791229248, 0.0, 0.3636363446712494, 0.0]",SklzIjActX,"['We present a general technique toward 8-bit low precision inference of convolutional neural networks. ', 'This paper designs a system to automatically quantize the CNN pretrained models']","['high throughput low latency inference deep neural network critical deployment deep learning application ', 'paper present general technique toward 8bit low precision inference convolutional neural network  including', '1  channelwise scale factor weight  especially depthwise convolution ', '2  winograd convolution ', '3  topologywise 8bit support ', 'experiment technique top widelyused deep learning framework ', '8bit optimized model automatically generated calibration process fp32 model without need finetuning retraining ', 'perform systematical comprehensive study 18 widelyused convolutional neural network demonstrate effectiveness 8bit low precision inference across wide range application use case  including image classification  object detection  image segmentation  super resolution ', 'show inference throughput', 'latency improved 16x 15x respectively minimal within 06  1to loss accuracy fp32 baseline ', 'believe methodology provide guidance reference design 8bit low precision inference framework ', 'code model publicly available soon ']","High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications., This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including, 1) channel-wise scale factors of weights, especially for depthwise convolution,, 2) Winograd convolution, and, 3) topology-wise 8-bit support., We experiment the techniques on top of a widely-used deep learning framework., The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining., We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution., We show that the inference throughput
, and latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline., We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks., All the code and models will be publicly available soon.",19,5.917582417582418,9.578947368421053
389,"['Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space.', 'We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.', 'By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model.', 'Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference.', ""Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.""]","[0, 1, 0, 0, 0]","[0.23529411852359772, 0.41860464215278625, 0.1538461446762085, 0.260869562625885, 0.15686273574829102]",rJxHsjRqFQ,"['We propose to incorporate inductive biases and operations coming from hyperbolic geometry to improve the attention mechanism of the neural networks.', 'This paper replaces the dot-product similarity used in attention mechanisms with the negative hyperbolic distance, and applies it to the existing Transformer model, graph attention networks, and Relation Networks', 'The authors propose a novel approach to improve relational-attention by changing the matching and aggregation functions to use hyperbolic geometric. ']","['recent approach successfully demonstrated benefit learning parameter shallow network hyperbolic space ', 'extend line work imposing hyperbolic geometry embeddings used compute ubiquitous attention mechanism different neural network architecture ', 'changing geometry embedding object representation  use embedding space efficiently without increasing number parameter model ', 'mainly number object grows exponentially semantic distance query  hyperbolic geometry  opposed euclidean geometry  encode object without interference ', 'method show improvement generalization neural machine translation wmt14  english german   learning graph  synthetic realworld graph task  visual question answering  clevr  task keeping neural representation compact ']","Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space., We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures., By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model., Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference., Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.",8,6.029411764705882,17.0
390,"['We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies.', 'We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies.', 'Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies.', 'This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region.', 'Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm.', 'Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst"" possible adversarial disturbances.']","[0, 1, 0, 0, 0, 0]","[0.1875, 0.25, 0.04444443807005882, 0.1818181723356247, 0.1702127605676651, 0.1428571343421936]",rkc_hGb0Z,"['This paper demonstrates how H-infinity control theory can help better design robust deep policies for robot motor taks', 'Proposes to incorporate elements of robust control into guided policy research in order to devise a method that is resilient to perturbations and model mismatch.', 'The paper presents a method for evaluating the sensitivity and robustness of deep RL policies, and proposes a dynamic game approach for learning robust policies.']","['present method evaluating sensitivity deep reinforcement learning  rl  policy ', 'also formulate zerosum dynamic game designing robust deep reinforcement learning policy ', 'approach mitigates brittleness policy agent trained simulated environment later exposed real world hazardous employ rl policy ', 'framework training deep rl policy involve zerosum dynamic game adversarial agent  goal drive system dynamic saddle region ', 'using variant guided policy search algorithm  agent learns adopt robust policy require le sample learning dynamic performs better gps algorithm ', 'without loss generality  demonstrate deep rl policy trained fashion maximally robust  worst  possible adversarial disturbance ']","We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies., We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies., Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies., This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region., Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm., Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst"" possible adversarial disturbances.",9,5.3776223776223775,15.88888888888889
391,"['Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.', 'In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.', 'Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).', 'We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.', 'Under such conditions, we prove the existence of small universal perturbations.', 'Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.']","[0, 1, 0, 0, 0, 0]","[0.20512820780277252, 0.4444444477558136, 0.375, 0.31111109256744385, 0.3333333432674408, 0.12903225421905518]",ByrZyglCb,"['Analysis of vulnerability of classifiers to universal perturbations and relation to the curvature of the decision boundary.', 'The paper provides an interesting analysis linking the geometry of classifier decision boundaries to small universal adversarial perturbations.', 'This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points.', 'The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks']","['deep network recently shown vulnerable universal perturbation  exist small imageagnostic perturbation cause natural image misclassified classifier ', 'paper  provide quantitative analysis robustness classifier universal perturbation  draw formal link robustness universal perturbation  geometry decision boundary ', 'specifically  establish theoretical bound robustness classifier two decision boundary model  flat curved model  ', 'show particular robustness deep network universal perturbation driven key property curvature  exist shared direction along decision boundary deep network systematically positively curved ', 'condition  prove existence small universal perturbation ', 'analysis provides novel geometric method computing universal perturbation  addition explaining property ']","Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers., In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary., Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models)., We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved., Under such conditions, we prove the existence of small universal perturbations., Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.",12,5.97986577181208,12.416666666666666
392,"['Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning.', 'However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration.', 'Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines.', 'However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task.', 'In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill.', 'Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly.', 'In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.']","[0, 0, 0, 0, 0, 0, 1]","[0.12903225421905518, 0.04878048226237297, 0.12765957415103912, 0.09999999403953552, 0.09999999403953552, 0.060606054961681366, 0.14999999105930328]",HkgSEnA5KQ,"['We propose a meta-learning method for interactively correcting policies with natural language.', 'This paper provides a meta learning framework that shows how to learn new tasks in an interactive setup. Each task is learned through a reinforcement learning setup, and then the task is being updated by observing new instructions.', 'This paper teaches agents to complete tasks via natural language instructions in an iterative process.']","['behavioral skill policy autonomous agent conventionally learned reward function  via reinforcement learning  demonstration  via imitation learning ', 'however  mode task specification disadvantage  reward function require manual engineering  demonstration require human expert able actually perform task order generate demonstration ', 'instruction following natural language instruction provides appealing alternative  way specify goal human simply speaking writing  would like able specify task machine ', 'however  single instruction may insufficient fully communicate intent  even  may insufficient autonomous agent actually understand perform desired task ', 'work  propose interactive formulation task specification problem  iterative language correction provided autonomous agent  guiding acquiring desired skill ', 'proposed languageguided policy learning algorithm integrate instruction sequence correction acquire new skill quickly ', 'experiment  show method enable policy follow instruction correction simulated navigation manipulation task  substantially outperforming direct  noninteractive instruction following ']","Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning., However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration., Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines., However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task., In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill., Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly., In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",22,5.846153846153846,9.454545454545455
393,"['Deep generative models such as Generative Adversarial Networks (GANs) and\n', 'Variational Auto-Encoders (VAEs) are important tools to capture and investigate\n', 'the properties of complex empirical data.', 'However, the complexity of their inner\n', 'elements makes their functionment challenging to assess and modify.', 'In this\n', 'respect, these architectures behave as black box models.', 'In order to better\n', 'understand the function of such networks, we analyze their modularity based on\n', 'the counterfactual manipulation of their internal variables.', 'Our experiments on the\n', 'generation of human faces with VAEs and GANs support that modularity between\n', 'activation maps distributed over channels of generator architectures is achieved\n', 'to some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.\n', 'erate and edit the content of generated images.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.2222222238779068, 0.1111111044883728, 0.2857142686843872, 0.2857142686843872, 0.0, 0.1249999925494194, 0.0, 0.29999998211860657, 0.2666666507720947, 0.1666666567325592, 0.19999998807907104, 0.1111111044883728, 0.1249999925494194, 0.25]",Byldr3RqKX,"['We investigate the modularity of deep generative models.', 'The paper provides a way to investigate the modular structure of the deep generative model, with the key concept to distribute over channels of generator architectures.']","['deep generative model generative adversarial network  gans ', 'variational autoencoders  vaes  important tool capture investigate', 'property complex empirical data ', 'however  complexity inner', 'element make functionment challenging ass modify ', '', 'respect  architecture behave black box model ', 'order better', 'understand function network  analyze modularity based', 'counterfactual manipulation internal variable ', 'experiment', 'generation human face vaes gans support modularity', 'activation map distributed channel generator architecture achieved', 'degree  used better understand system operate allow meaningful transformation generated image without training ', 'erate edit content generated image ']","Deep generative models such as Generative Adversarial Networks (GANs) and
, Variational Auto-Encoders (VAEs) are important tools to capture and investigate
, the properties of complex empirical data., However, the complexity of their inner
, elements makes their functionment challenging to assess and modify., In this
, respect, these architectures behave as black box models., In order to better
, understand the function of such networks, we analyze their modularity based on
, the counterfactual manipulation of their internal variables., Our experiments on the
, generation of human faces with VAEs and GANs support that modularity between
, activation maps distributed over channels of generator architectures is achieved
, to some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.
, erate and edit the content of generated images.",19,5.924242424242424,6.947368421052632
394,"['Relational databases store a significant amount of the worlds data.', 'However, accessing this data currently requires users to understand a query language such as SQL.', 'We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.', 'Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss.', 'Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem.', 'In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets.', 'By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.']","[0, 0, 0, 0, 0, 1, 0]","[0.10810810327529907, 0.1904761791229248, 0.3255814015865326, 0.20338982343673706, 0.22727271914482117, 0.4406779706478119, 0.20689654350280762]",Syx6bz-Ab,"['We introduce Seq2SQL, which translates questions to SQL queries using rewards from online query execution, and WikiSQL, a SQL table/question/query dataset orders of magnitude larger than existing datasets.', 'A new semantic parsing dataset which focuses on generating SQL from natural language using a reinforcement-learning based model']","['relational database store significant amount world data ', 'however  accessing data currently requires user understand query language sql ', 'propose seq2sql  deep neural network translating natural language question corresponding sql query ', 'model us reward loop query execution database learn policy generate query  contains unordered part le suitable optimization via cross entropy loss ', 'moreover  seq2sql leverage structure sql prune space generated query significantly simplify generation problem ', 'addition model  release wikisql  dataset 80654 handannotated example question sql query distributed across 24241 table fromwikipedia order magnitude larger comparable datasets ', 'applying policy based reinforcement learning query execution environment wikisql  seq2sql outperforms stateoftheart semantic parser  improving execution accuracy 359  594  logical form accuracy 234  483  ']","Relational databases store a significant amount of the worlds data., However, accessing this data currently requires users to understand a query language such as SQL., We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries., Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss., Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem., In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets., By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.",15,5.689024390243903,10.933333333333334
395,"['We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.', ""We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness."", ""We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis."", 'Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components.', 'We show that models learnt with our approach perform remarkably well against a wide-range of attacks.', 'Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.']","[0, 1, 0, 0, 0, 0]","[0.1666666567325592, 0.27272728085517883, 0.1111111044883728, 0.11428570747375488, 0.0, 0.17777776718139648]",rkMk9j0qYm,"['Noise modeling at the input during discriminative training improves adversarial robustness. Propose PCA based evaluation metric for adversarial robustness', 'This paper proposes, ExL, an adversarial training method using multiplicate noise that is shown to be helpful in defending against blackbox attacks on three datasets.', 'This paper includes multiplicative noise N in training data to achieve adversarial robustness, when training on both model parameters theta and on the noise itself.']","['introduce explainable adversarial learning  exl  approach training neural network intrinsically robust adversarial attack ', 'find implicit generative modeling random noise loss function used posterior maximization  improves model understanding data manifold furthering adversarial robustness ', 'prove approach efficacy provide simplistic visualization tool understanding adversarial data  using principal component analysis ', 'analysis reveals adversarial robustness  general  manifest model higher variance along highranked principal component ', 'show model learnt approach perform remarkably well widerange attack ', 'furthermore  combining exl stateoftheart adversarial training extends robustness model  even beyond adversarially trained  whitebox blackbox attack scenario ']","We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks., We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness., We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis., Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components., We show that models learnt with our approach perform remarkably well against a wide-range of attacks., Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.",15,6.297709923664122,8.733333333333333
396,"['We propose a method which can visually explain the classification decision of deep neural networks (DNNs).', 'There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  ', 'All of these methods try to gain insight into why the network ""chose class A"" as an answer.', 'Humans, when searching for explanations, ask two types of questions.', 'The first question is, ""Why did you choose this answer?', '""', 'The second question asks, ""Why did you not choose answer B over A?""', 'The previously proposed methods are either not able to provide the latter directly or efficiently.\n\n', 'We introduce a method capable of answering the second question both directly and efficiently.', 'In this work, we limit the inputs to be images.', 'In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation.', 'We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2142857164144516, 0.05882352590560913, 0.19999998807907104, 0.09090908616781235, 0.0, 0.1599999964237213, 0.1428571343421936, 0.07692307233810425, 0.09090908616781235, 0.06666666269302368, 0.06666666269302368]",HyNmRiCqtm,"['A method to answer ""why not class B?"" for explaining deep networks', 'The paper proposes an approach to provide contrastive visual explanations for deep neural networks.']","['propose method visually explain classification decision deep neural network  dnns  ', 'many proposed method machine learning computer vision seeking clarify decision machine learning black box  specifically dnns ', 'method try gain insight network  chose class  answer ', 'human  searching explanation  ask two type question ', 'first question   choose answer ', '', 'second question asks   choose answer b  ', 'previously proposed method either able provide latter directly efficiently ', 'introduce method capable answering second question directly efficiently ', 'work  limit input image ', 'general  proposed method generates explanation input space model capable efficient evaluation gradient evaluation ', 'provide result  showing superiority approach gaining insight inner representation machine learning model ']","We propose a method which can visually explain the classification decision of deep neural networks (DNNs)., There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  , All of these methods try to gain insight into why the network ""chose class A"" as an answer., Humans, when searching for explanations, ask two types of questions., The first question is, ""Why did you choose this answer?, "", The second question asks, ""Why did you not choose answer B over A?"", The previously proposed methods are either not able to provide the latter directly or efficiently.

, We introduce a method capable of answering the second question both directly and efficiently., In this work, we limit the inputs to be images., In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation., We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.",20,5.233918128654971,8.55
397,"['We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.', 'We provide theoretical and numerical results on the inverse of ReLU-layers.', 'First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation.', 'Next, we move to robustness via analyzing local effects on the inverse.', 'To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.']","[1, 0, 0, 0, 0]","[0.47058823704719543, 0.4615384638309479, 0.1875, 0.14814814925193787, 0.04878048226237297]",SyxYEoA5FX,"['We analyze the invertibility of deep neural networks by studying preimages of ReLU-layers and the stability of the inverse.', 'This paper studies the volume of preimage of a ReLU networks activation at a certain layer, and it builds on the piecewise linearity of a ReLU networks forward function. ', 'This paper presents an analysis of the inverse invariance of ReLU networks and provides upper bounds on singular values of a train network.']","['flip usual approach study invariance robustness neural network considering nonuniqueness instability inverse mapping ', 'provide theoretical numerical result inverse relulayers ', 'first  derive necessary sufficient condition existence invariance provides geometric interpretation ', 'next  move robustness via analyzing local effect inverse ', 'conclude  show reverse point view provides insight key effect  also enables view adversarial example different perspective ']","We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping., We provide theoretical and numerical results on the inverse of ReLU-layers., First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation., Next, we move to robustness via analyzing local effects on the inverse., To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.",9,5.571428571428571,10.11111111111111
398,"['While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs.', 'This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important.', 'Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks.', 'In this paper we investigate how this approach scales as we increase the computational budget given to the defender.', 'We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model.', 'Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.']","[0, 0, 0, 0, 0, 1]","[0.17543859779834747, 0.10526315122842789, 0.31578946113586426, 0.05714285373687744, 0.24561403691768646, 0.41025641560554504]",HJguLo0cKQ,"['Adversarial training of ensembles provides robustness to adversarial examples beyond that observed in adversarially trained models and independently-trained ensembles thereof.', ' Proposes to train an ensemble of models jointly, where at each time step, a set of examples that are adversarial for the ensemble itself is incorporated in the learning.']","['deep learning led remarkable result number challenging problem  researcher discovered vulnerability neural network adversarial setting  small carefully chosen perturbation input make model produce extremely inaccurate output ', 'make model particularly unsuitable safetycritical application domain  eg  selfdriving car  robustness extremely important ', 'recent work shown augmenting training adversarially generated data provides degree robustness testtime attack ', 'paper investigate approach scale increase computational budget given defender ', 'show increasing number parameter adversariallytrained model increase robustness  particular ensembling smaller model adversarially training entire ensemble single model efficient way spending said budget simply using larger single model ', 'crucially  show adversarial training ensemble  rather ensembling adversarially trained model  provides robustness ']","While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs., This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important., Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks., In this paper we investigate how this approach scales as we increase the computational budget given to the defender., We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model., Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.",12,5.988095238095238,12.923076923076923
399,"['Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer.', 'To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm.', 'A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks.', 'A function block may be any neural network  for example a fully-connected or a convolutional layer.', 'Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached.', 'In this way the routing network dynamically composes different function blocks for each input.', 'We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks.', 'We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets.', 'Our experiments demonstrate a significant improvement in accuracy, with sharper convergence.', 'In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks.', 'On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.\n']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.23255813121795654, 0.23529411852359772, 0.31578946113586426, 0.23529411852359772, 0.1818181723356247, 0.25, 0.17142856121063232, 0.10810810327529907, 0.06896550953388214, 0.10810810327529907, 0.0]",ry8dvM-R-,"['routing networks: a new kind of neural network which learns to adaptively route its input for multi-task learning', 'The paper suggests to use a modular network with a controller which makes decisions, at each time step, regarding the next nodule to apply.', 'The paper presents a novel formulation for learning the optimal architecture of a neural network in a multi-task learning framework by using multi-agent reinforcement learning to find a policy, and shows improvement over hard-coded architectures with shared layers.']","['multitask learning  mtl  neural network leverage commonality task improve performance  often suffers task interference reduces benefit transfer ', 'address issue introduce routing network paradigm  novel neural network training algorithm ', 'routing network kind selforganizing neural network consisting two component  router set one function block ', 'function block may neural network  example fullyconnected convolutional layer ', 'given input router make routing decision  choosing function block apply passing output back router recursively  terminating fixed recursion depth reached ', 'way routing network dynamically composes different function block input ', 'employ collaborative multiagent reinforcement learning  marl  approach jointly train router function block ', 'evaluate model crossstitch network sharedlayer baseline multitask setting mnist  miniimagenet  cifar100 datasets ', 'experiment demonstrate significant improvement accuracy  sharper convergence ', 'addition  routing network nearly constant pertask training cost crossstitch network scale linearly number task ', 'cifar100  20 task  obtain crossstitch performance level 85  average reduction training time ']","Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer., To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm., A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks., A function block may be any neural network  for example a fully-connected or a convolutional layer., Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached., In this way the routing network dynamically composes different function blocks for each input., We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks., We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets., Our experiments demonstrate a significant improvement in accuracy, with sharper convergence., In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks., On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.
",19,5.758139534883721,11.31578947368421
400,"['We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero.', 'Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization.', 'AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization.', 'However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function.', 'We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero.', 'We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters.', ""We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid."", 'The parameters of the distribution over the gates can then be jointly optimized with the original network parameters.', 'As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way.', 'We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.21276594698429108, 0.04878048226237297, 0.10810810327529907, 0.17777776718139648, 0.23255813121795654, 0.30434781312942505, 0.25, 0.20512820780277252, 0.25531914830207825, 0.2702702581882477]",H1Y8hhg0b,"['We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.', 'The authors introduce a gradient-based approach to minimize an objective function with an L0 sparse penalty to help learn sparse neural networks']","['propose practical method  l0  norm regularization neural network  pruning network training encouraging weight become exactly zero ', 'regularization interesting since  1  greatly speed training inference   2  improve generalization ', 'aic bic  wellknown model selection criterion  special case  l0  regularization ', 'however  since  l0  norm weight nondifferentiable  incorporate directly regularization term objective function ', 'propose solution inclusion collection nonnegative stochastic gate  collectively determine weight set zero ', 'show  somewhat surprisingly  certain distribution gate  expected  l0  regularized objective differentiable respect distribution parameter ', 'propose emph  hard concrete  distribution gate  obtained  stretching  binary concrete distribution transforming sample hardsigmoid ', 'parameter distribution gate jointly optimized original network parameter ', 'result method allows straightforward efficient learning model structure stochastic gradient descent allows conditional computation principled way ', 'perform various experiment demonstrate effectiveness resulting approach regularizer ']","We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero., Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization., AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization., However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function., We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero., We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters., We further propose the \emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid., The parameters of the distribution over the gates can then be jointly optimized with the original network parameters., As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way., We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",20,5.933962264150943,10.6
401,"['Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches.', 'These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer.', 'Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models.', 'This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small.', 'This in turn allows a room for designing more innovative propagation layers.', 'Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph.', 'The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions.', 'In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods.', 'By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2666666507720947, 0.10810810327529907, 0.1395348757505417, 0.10526315122842789, 0.1249999925494194, 0.1666666567325592, 0.09999999403953552, 0.15789473056793213, 0.0476190410554409]",rJg4YGWRb,"['We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets', 'The authors propose two extensions of GCNs, by removing intermediate non-linearities from the GCN computation and adding an attention mechanism in the aggregation layer.', 'The paper proposes a semi supervised learning algorithm for graph node classification with is inspired from Graph Neural Networks.']","['recently popularized graph neural network achieve stateoftheart accuracy number standard benchmark datasets graphbased semisupervised learning  improving significantly existing approach ', 'architecture alternate propagation layer aggregate hidden state local neighborhood fullyconnected layer ', 'perhaps surprisingly  show linear model  remove intermediate fullyconnected layer  still able achieve performance comparable stateoftheart model ', 'significantly reduces number parameter  critical semisupervised learning number labeled example small ', 'turn allows room designing innovative propagation layer ', 'based insight  propose novel graph neural network remove intermediate fullyconnected layer  replaces propagation layer attention mechanism respect structure graph ', 'attention mechanism allows u learn dynamic adaptive local summary neighborhood achieve accurate prediction ', 'number experiment benchmark citation network datasets  demonstrate approach outperforms competing method ', 'examining attention weight among neighbor  show model provides interesting insight neighbor influence ']","Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches., These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer., Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models., This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small., This in turn allows a room for designing more innovative propagation layers., Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph., The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions., In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods., By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.",18,6.116161616161616,11.0
402,"['Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality.', 'We argue that this discrepancy may contribute to the difficulties in training generative models.', 'We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space.', 'The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model.', 'With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities.', 'When combined with VAE, PGA can generate sharper samples than vanilla VAE.']","[0, 0, 0, 0, 1, 0]","[0.04999999701976776, 0.13793103396892548, 0.04878048226237297, 0.1111111044883728, 0.34285715222358704, 0.07407406717538834]",rkxkr8UKuN,"['A framework for training autoencoder-based generative models, with non-adversarial losses and unrestricted neural network architectures.', 'This paper uses autoencoders to do distribution matching in high dimensional space.']","['modern generative model usually designed match target distribution directly data space  intrinsic dimensionality data much lower ambient dimensionality ', 'argue discrepancy may contribute difficulty training generative model ', 'therefore propose map generated target distribution latent space using encoder standard autoencoder  train generator  decoder  match target distribution latent space ', 'resulting method  perceptual generative autoencoder  pga   incorporated maximum likelihood variational autoencoder  vae  objective train generative model ', 'maximum likelihood  pga generalizes idea reversible generative model unrestricted neural network architecture arbitrary latent dimensionality ', 'combined vae  pga generate sharper sample vanilla vae ']","Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality., We argue that this discrepancy may contribute to the difficulties in training generative models., We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space., The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model., With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities., When combined with VAE, PGA can generate sharper samples than vanilla VAE.",12,5.962962962962963,11.25
403,"['The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\n', 'Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\n', 'We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\n', 'We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\n', 'Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\n', 'We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\n', 'On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset.', 'We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\n', 'points in Spearman rank correlation on similarity tasks\n', 'and 3.4 points on analogy accuracy.\n']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.1621621549129486, 0.145454540848732, 0.3199999928474426, 0.27272728085517883, 0.12121211737394333, 0.09999999403953552, 0.11999999731779099, 0.1428571343421936, 0.0, 0.0]",HJxeWnCcF7,"['Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.', 'Proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The algorithm is based on matching the geodesic distances on the product manifold to graph distances.']","['quality representation achieved embeddings determined well geometry embedding space match structure data ', 'euclidean space workhorse embeddings  recently hyperbolic spherical space gained popularity due ability better embed new type structured data  hierarchical data  data structured uniformly ', 'address problem proposing learning embeddings product manifold combining multiple copy model space  spherical  hyperbolic  euclidean   providing space heterogeneous curvature suitable wide variety structure ', 'introduce heuristic estimate sectional curvature graph data directly determine appropriate signature  number component space dimension  product manifold ', 'empirically  jointly learn curvature embedding product space via riemannian optimization ', 'discus define compute intrinsic quantity mean  challenging notion product manifold  provably learnable optimization function ', 'range datasets reconstruction task  product space embeddings outperform single euclidean hyperbolic space used previous work  reducing distortion 3255  facebook social network dataset ', 'learn word embeddings find product hyperbolic space 50 dimension consistently improves baseline euclidean hyperbolic embeddings  26', 'point spearman rank correlation similarity task', '34 point analogy accuracy ']","The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.
, Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.
, We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.
, We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.
, Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.
, We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.
, On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset., We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6
, points in Spearman rank correlation on similarity tasks
, and 3.4 points on analogy accuracy.
",17,5.914529914529915,13.764705882352942
404,"['Synthesizing user-intended programs from a small number of input-output exam-\n', 'ples is a challenging problem with several important applications like spreadsheet\n', 'manipulation, data wrangling and code refactoring.', 'Existing synthesis systems\n', 'either completely rely on deductive logic techniques that are extensively hand-\n', 'engineered or on purely statistical models that need massive amounts of data, and in\n', 'general fail to provide real-time synthesis on challenging benchmarks.', 'In this work,\n', 'we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\n', 'that combines the best of both symbolic logic techniques and statistical models.\n', 'Thus, it produces programs that satisfy the provided specifications by construction\n', 'and generalize well on unseen examples, similar to data-driven systems.', 'Our\n', 'technique effectively utilizes the deductive search framework to reduce the learning\n', 'problem of the neural component to a simple supervised learning setup.', 'Further,\n', 'this allows us to both train on sparingly available real-world data and still leverage\n', 'powerful recurrent neural network encoders.', 'We demonstrate the effectiveness\n', 'of our method by evaluating on real-world customer scenarios by synthesizing\n', 'accurate programs with up to 12 speed-up compared to state-of-the-art systems.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.12903225421905518, 0.0624999962747097, 0.07407406717538834, 0.0833333283662796, 0.0, 0.11428570747375488, 0.20000000298023224, 0.0, 0.0624999962747097, 0.1764705777168274, 0.0, 0.12903225421905518, 0.06451612710952759, 0.0624999962747097, 0.11428570747375488, 0.0, 0.07999999821186066, 0.0, 0.12903225421905518]",rywDjg-RW,"['We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.', 'The paper presents a branch-and-bound approach to learn good programs where an LSTM is used to predict which branches in the search tree should lead to good programs', 'Proposes system that synthesizes programs from a single example that generalize better than prior state-of-the-art']","['synthesizing userintended program small number inputoutput exam', 'ples challenging problem several important application like spreadsheet', 'manipulation  data wrangling code refactoring ', 'existing synthesis system', 'either completely rely deductive logic technique extensively hand', 'engineered purely statistical model need massive amount data ', 'general fail provide realtime synthesis challenging benchmark ', 'work ', 'propose neural guided deductive search  ngds   hybrid synthesis technique', 'combine best symbolic logic technique statistical model ', 'thus  produce program satisfy provided specification construction', 'generalize well unseen example  similar datadriven system ', '', 'technique effectively utilizes deductive search framework reduce learning', 'problem neural component simple supervised learning setup ', '', 'allows u train sparingly available realworld data still leverage', 'powerful recurrent neural network encoders ', 'demonstrate effectiveness', 'method evaluating realworld customer scenario synthesizing', 'accurate program 12 speedup compared stateoftheart system ']","Synthesizing user-intended programs from a small number of input-output exam-
, ples is a challenging problem with several important applications like spreadsheet
, manipulation, data wrangling and code refactoring., Existing synthesis systems
, either completely rely on deductive logic techniques that are extensively hand-
, engineered or on purely statistical models that need massive amounts of data, and in
, general fail to provide real-time synthesis on challenging benchmarks., In this work,
, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique
, that combines the best of both symbolic logic techniques and statistical models.
, Thus, it produces programs that satisfy the provided specifications by construction
, and generalize well on unseen examples, similar to data-driven systems., Our
, technique effectively utilizes the deductive search framework to reduce the learning
, problem of the neural component to a simple supervised learning setup., Further,
, this allows us to both train on sparingly available real-world data and still leverage
, powerful recurrent neural network encoders., We demonstrate the effectiveness
, of our method by evaluating on real-world customer scenarios by synthesizing
, accurate programs with up to 12 speed-up compared to state-of-the-art systems.",26,6.166666666666667,6.923076923076923
405,"['Variational auto-encoders\n (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models.', 'However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classication) and human interpretation.', 'We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution.', 'We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efcient as in the standard VAE case.', 'With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models.', 'We show that these sparse representations are advantageous over standard VAE representations on two benchmark classication tasks (MNIST and Fashion-MNIST) by demonstrating improved classication accuracy and signicantly increased robustness to the number of latent dimensions.', 'Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.']","[0, 0, 0, 0, 0, 0, 1]","[0.0, 0.1666666567325592, 0.25641024112701416, 0.12121211737394333, 0.14814814925193787, 0.24390242993831635, 0.260869562625885]",SkeJ6iR9Km,"['We explore the intersection of VAEs and sparse coding.', 'This paper proposes an extension of VAEs with sparse priors and posteriors to learn sparse interpretable representations.']","['variational autoencoders  vaes  offer tractable approach performing approximate inference otherwise intractable generative model ', 'however  standard vaes often produce latent code disperse lack interpretability  thus making resulting representation unsuitable auxiliary task  eg  classication  human interpretation ', 'address issue merging idea variational autoencoders sparse coding  propose explicitly model sparsity latent space vae spike slab prior distribution ', 'derive evidence lower bound using discrete mixture recognition function thereby making approximate posterior inference computational efcient standard vae case ', 'new approach  able infer truly sparse representation generally intractable nonlinear probabilistic model ', 'show sparse representation advantageous standard vae representation two benchmark classication task  mnist fashionmnist  demonstrating improved classication accuracy signicantly increased robustness number latent dimension ', 'furthermore  demonstrate qualitatively sparse element capture subjectively understandable source variation ']","Variational auto-encoders
 (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models., However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classication) and human interpretation., We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution., We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efcient as in the standard VAE case., With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models., We show that these sparse representations are advantageous over standard VAE representations on two benchmark classication tasks (MNIST and Fashion-MNIST) by demonstrating improved classication accuracy and signicantly increased robustness to the number of latent dimensions., Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.",12,6.544378698224852,13.0
406,"['A widely observed phenomenon in deep learning is the degradation problem: increasing\n', 'the depth of a network leads to a decrease in performance on both test and training data.', 'Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms.', 'However, the degradation problem persists in the context of plain feed-forward networks.', 'In this work we propose a simple method to address this issue.', 'The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers.', 'This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner.', 'We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.']","[0, 0, 0, 0, 0, 0, 1, 0]","[0.25, 0.1428571343421936, 0.12121211737394333, 0.3478260934352875, 0.08695651590824127, 0.277777761220932, 0.3636363446712494, 0.045454543083906174]",BJQPG5lR-,"['Phasing out skip-connections in a principled manner avoids degradation in deep feed-forward networks.', 'The authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections', 'The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets without skip connections.']","['widely observed phenomenon deep learning degradation problem  increasing', 'depth network lead decrease performance test training data ', 'novel architecture resnets highway network addressed issue introducing various flavor skipconnections gating mechanism ', 'however  degradation problem persists context plain feedforward network ', 'work propose simple method address issue ', 'proposed method pose learning weight deep network constrained optimization problem presence skipconnections penalized lagrange multiplier ', 'allows skipconnections introduced early stage training subsequently phased principled manner ', 'demonstrate benefit approach experiment mnist  fashionmnist  cifar10 cifar100 proposed method shown greatly decrease degradation effect  compared plain network  often competitive resnets ']","A widely observed phenomenon in deep learning is the degradation problem: increasing
, the depth of a network leads to a decrease in performance on both test and training data., Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms., However, the degradation problem persists in the context of plain feed-forward networks., In this work we propose a simple method to address this issue., The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers., This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner., We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.",11,5.660377358490566,14.454545454545455
407,"['Deep learning is becoming more widespread in its application due to its power in solving complex classification problems.', 'However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications.', 'This work addresses the problem by proposing methods {\\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight.', 'Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem.', 'Our method that mini-batch SVRG with $\\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates.', 'Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\\times$ without affecting their test accuracy.']","[0, 1, 0, 0, 0, 0]","[0.07692307233810425, 0.17142856121063232, 0.052631575614213943, 0.06896550953388214, 0.060606054961681366, 0.08888888359069824]",Sk0pHeZAW,"['Compression of Deep neural networks deployed on embedded device. ', 'The authors present an l-1 regularized SVRG based training algorithm that is able to force many weights of the network to be 0.', 'This work reduces memory requirements.']","['deep learning becoming widespread application due power solving complex classification problem ', 'however  deep learning model often require large memory energy consumption  may prevent deployed effectively embedded platform  limiting application ', 'work address problem proposing method  em weight reduction quantisation  compressing memory footprint model  including reducing number weight number bit store weight ', 'beside  applying sparsityinducing regularization  work focus speeding stochastic variance reduced gradient  svrg  optimization nonconvex problem ', 'method minibatch svrg  ell  1 regularization nonconvex problem faster smoother convergence rate sgd using adaptive learning rate ', 'experimental evaluation approach us mnist cifar10 datasets lenet300100 lenet5 model  showing approach reduce memory requirement convolutional fully connected layer 60  time  without affecting test accuracy ']","Deep learning is becoming more widespread in its application due to its power in solving complex classification problems., However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications., This work addresses the problem by proposing methods {\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight., Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem., Our method that mini-batch SVRG with $\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates., Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\times$ without affecting their test accuracy.",13,6.061728395061729,12.461538461538462
408,"['It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment.', 'For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus.', 'Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment.', 'However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain.', 'In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment.', 'Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm.', 'To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training.', 'Despite the novel features of these objects the model is able to infer the latent representations for them.', 'Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.10256409645080566, 0.12121211737394333, 0.21621620655059814, 0.4285714328289032, 0.2222222238779068, 0.3636363446712494, 0.1395348757505417, 0.19354838132858276, 0.13333332538604736]",Hy8hkYeRb,"['A predictive coding based learning algorithm for building deep neural network models of the brain', 'The paper considers learning of a generative neural network using a predictive coding setup']","['argued brain prediction machine continuously learns make better prediction stimulus received external environment ', 'purpose  build model world around u us model infer external stimulus ', 'predictive coding proposed mechanism brain might able build model external environment ', 'however  clear predictive coding used build deep neural network model brain complying architectural constraint imposed brain ', 'paper  describe algorithm build deep generative model using predictive coding used infer latent representation stimulus received external environment ', 'specifically  used predictive coding train deep neural network realworld image unsupervised learning paradigm ', 'understand capacity network regard modeling external environment  studied latent representation generated model image object never presented model training ', 'despite novel feature object model able infer latent representation ', 'furthermore  reconstruction original image obtained latent representation preserve important detail object ']","It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment., For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus., Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment., However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain., In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment., Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm., To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training., Despite the novel features of these objects the model is able to infer the latent representations for them., Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.",15,5.28,15.0
409,"[""In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  "", ""This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  "", 'The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category.', 'A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  ', 'Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  ', 'We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.']","[1, 0, 0, 0, 0, 0]","[0.3921568691730499, 0.2631579041481018, 0.3478260934352875, 0.1666666567325592, 0.17241378128528595, 0.12121211737394333]",rynniUpQM,"[""Object instance recognition with adversarial autoencoders was performed with a novel 'mental image' target that is canonical representation of the input image."", 'The paper proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose.', 'This paper investigated the task of few shot recognition via a generated mental image as intermediate representation given the input image.']","['paper  propose deep convolutional generative adversarial network  dcgan  learn produce mental image  input image internal representation certain category input data distribution ', 'mental image dcgan imago  input image might look like ideal condition ', 'mental image contains version input iconic  without peculiarity contribute ideal representation input data distribution within category ', 'dcgan learns association training encoder capture salient feature original image decoder convert salient feature associated mental image representation ', 'new approach  refer mental image dcgan  midcgan   learns feature useful recognizing entire class object  turn benefit helping single zero shot recognition ', 'demonstrate approach object instance recognition handwritten digit recognition task ']","In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  , This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  , The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category., A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  , Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  , We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.",11,5.353658536585366,14.909090909090908
410,"['An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill.', 'The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task.', 'We present a framework that combines techniques in \\textit{formal methods} with \\textit{hierarchical reinforcement learning} (HRL).', 'The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning.', 'We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.']","[1, 0, 0, 0, 0]","[0.25641024112701416, 0.13793103396892548, 0.1599999964237213, 0.15686273574829102, 0.0]",BJgVaG-Ab,"['Combine temporal logic with hierarchical reinforcement learning for skill composition', 'The paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula.', 'Proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.']","['obstacle prevents wide adoption  deep  reinforcement learning  rl  control system need large number interaction environment order master skill ', 'learned skill usually generalizes poorly across domain retraining often necessary presented new task ', 'present framework combine technique textit  formal method  textit  hierarchical reinforcement learning   hrl  ', 'set technique provide allows convenient specification task logical expression  learns hierarchical policy  metacontroller lowlevel controller  welldefined intrinsic reward using rl method able construct new skill existing one without additional learning ', 'evaluate proposed method simple grid world simulation well simulation baxter robot ']","An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill., The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task., We present a framework that combines techniques in \textit{formal methods} with \textit{hierarchical reinforcement learning} (HRL)., The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning., We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.",6,5.8076923076923075,21.666666666666668
411,"['The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems.', 'As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance.', 'However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks.', 'In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs.', 'Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties.', 'The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression.', 'For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account.', 'We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.25641024112701416, 0.18867923319339752, 0.051282044500112534, 0.26923075318336487, 0.17777776718139648, 0.10526315122842789, 0.25641024112701416, 0.1818181723356247]",HylDpoActX,"['We propose a quantization scheme for weights and activations of deep neural networks. This reduces the memory footprint substantially and accelerates inference.', 'CNN model compression aand inference acceleration using quantization.']","['tremendous memory computational complexity convolutional neural network  cnns  prevents inference deployment resourceconstrained system ', 'result  recent research focused cnn optimization technique  particular quantization  allows weight activation layer represented bit achieving impressive prediction performance ', 'however  aggressive quantization technique still fail achieve fullprecision prediction performance stateoftheart cnn architecture largescale classification task ', 'work propose method weight activation quantization scalable term quantization level  nary representation  easy compute maintaining performance close fullprecision cnns ', 'weight quantization scheme based trainable scaling factor nestedmeans clustering strategy robust weight update therefore exhibit good convergence property ', 'flexibility nestedmeans clustering enables exploration various nary weight representation potential high parameter compression ', 'activation  propose linear quantization strategy take statistical property batch normalization account ', 'demonstrate effectiveness approach using stateoftheart model imagenet ']","The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems., As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance., However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks., In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs., Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties., The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression., For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account., We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet.",13,6.6,13.846153846153847
412,"['Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently.', 'This means that we must not only specify what to do, but also the much larger space of what not to do.', 'It is easy to forget these preferences, since these preferences are already satisfied in our environment.', 'This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.', 'We can therefore use this implicit preference information from the state to fill in the blanks.', 'We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties.', 'We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.', 'Our code can be found at https://github.com/HumanCompatibleAI/rlsp.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.19230768084526062, 0.23999999463558197, 0.260869562625885, 0.6071428656578064, 0.30434781312942505, 0.27586206793785095, 0.3214285671710968, 0.05128204822540283]",rkevMnRqYQ,"['When a robot is deployed in an environment that humans have been acting in, the state of the environment is already optimized for what humans want, and we can use this to infer human preferences.', 'The authors propose to augment the explicitly stated reward function of an RL agent with auxiliary rewards/costs inferred from the initial state and a model of the state dynamics', 'This work proposes a way to infer the implicit information in the initial state using IRL and combine the inferred reward with a specified reward.']","['reinforcement learning  rl  agent optimize feature specified reward function indifferent anything left inadvertently ', 'mean must specify  also much larger space ', 'easy forget preference  since preference already satisfied environment ', 'motivates key insight  robot deployed environment human act  state environment already optimized human want ', 'therefore use implicit preference information state fill blank ', 'develop algorithm based maximum causal entropy irl use evaluate idea suite proofofconcept environment designed show property ', 'find information initial state used infer side effect avoided well preference environment organized ', 'code found http  githubcomhumancompatibleairlsp ']","Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently., This means that we must not only specify what to do, but also the much larger space of what not to do., It is easy to forget these preferences, since these preferences are already satisfied in our environment., This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want., We can therefore use this implicit preference information from the state to fill in the blanks., We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties., We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized., Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",11,5.076470588235294,15.454545454545455
413,"['Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other.', 'In our work we present a novel, systematic, unifying taxonomy to categorize existing methods.', 'We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures.', 'We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on.', 'We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories.', 'This helps revealing links and fundamental similarities between them.', 'Finally, we include practical recommendations both for users and for developers of new regularization methods.']","[0, 0, 0, 0, 0, 0, 1]","[0.2857142686843872, 0.07999999821186066, 0.1599999964237213, 0.06666666269302368, 0.10810810327529907, 0.19999998807907104, 0.3199999928474426]",SkHkeixAW,"['Systematic categorization of regularization methods for deep learning, revealing their similarities.', 'Attempts to build a taxonomy for regularization techniques employed in deep learning.']","['regularization one crucial ingredient deep learning  yet term regularization various definition  regularization method often studied separately ', 'work present novel  systematic  unifying taxonomy categorize existing method ', 'distinguish method affect data  network architecture  error term  regularization term  optimization procedure ', 'identify atomic building block existing method  decouple assumption enforce mathematical tool rely ', 'provide detail listed method  instead  present overview method sorted meaningful category subcategories ', 'help revealing link fundamental similarity ', 'finally  include practical recommendation user developer new regularization method ']","Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other., In our work we present a novel, systematic, unifying taxonomy to categorize existing methods., We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures., We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on., We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories., This helps revealing links and fundamental similarities between them., Finally, we include practical recommendations both for users and for developers of new regularization methods.",18,5.953488372093023,7.166666666666667
414,"['Deep neural networks are surprisingly efficient at solving practical tasks,\n', 'but the theory behind this phenomenon is only starting to catch up with\n', 'the practice.', 'Numerous works show that depth is the key to this efficiency.\n', 'A certain class of deep convolutional networks  namely those that correspond\n', 'to the Hierarchical Tucker (HT) tensor decomposition  has been\n', 'proven to have exponentially higher expressive power than shallow networks.\n', 'I.e. a shallow network of exponential width is required to realize\n', 'the same score function as computed by the deep architecture.', 'In this paper,\n', 'we prove the expressive power theorem (an exponential lower bound on\n', 'the width of the equivalent shallow network) for a class of recurrent neural\n', 'networks  ones that correspond to the Tensor Train (TT) decomposition.\n', 'This means that even processing an image patch by patch with an RNN\n', 'can be exponentially more efficient than a (shallow) convolutional network\n', 'with one hidden layer.', 'Using theoretical results on the relation between\n', 'the tensor decompositions we compare expressive powers of the HT- and\n', 'TT-Networks.', 'We also implement the recurrent TT-Networks and provide\n', 'numerical evidence of their expressivity.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.1904761791229248, 0.0833333283662796, 0.17391303181648254, 0.17391303181648254, 0.0952380895614624, 0.1818181723356247, 0.260869562625885, 0.09999999403953552, 0.0, 0.27272728085517883, 0.3636363446712494, 0.17391303181648254, 0.0, 0.0, 0.0, 0.1111111044883728, 0.1904761791229248, 0.21052631735801697, 0.1249999925494194]",S1WRibb0Z,"['We prove the exponential efficiency of recurrent-type neural networks over shallow networks.', 'The authors compare the complexity of tensor train networks with networks structured by CP decomposition']","['deep neural network surprisingly efficient solving practical task ', 'theory behind phenomenon starting catch', 'practice ', 'numerous work show depth key efficiency ', 'certain class deep convolutional network  namely correspond', 'hierarchical tucker  ht  tensor decomposition ', 'proven exponentially higher expressive power shallow network ', 'ie  shallow network exponential width required realize', 'score function computed deep architecture ', 'paper ', 'prove expressive power theorem  exponential lower bound', 'width equivalent shallow network  class recurrent neural', 'network  one correspond tensor train  tt  decomposition ', 'mean even processing image patch patch rnn', 'exponentially efficient  shallow  convolutional network', 'one hidden layer ', 'using theoretical result relation', 'tensor decomposition compare expressive power ht', 'ttnetworks ', 'also implement recurrent ttnetworks provide', 'numerical evidence expressivity ']","Deep neural networks are surprisingly efficient at solving practical tasks,
, but the theory behind this phenomenon is only starting to catch up with
, the practice., Numerous works show that depth is the key to this efficiency.
, A certain class of deep convolutional networks  namely those that correspond
, to the Hierarchical Tucker (HT) tensor decomposition  has been
, proven to have exponentially higher expressive power than shallow networks.
, I.e. a shallow network of exponential width is required to realize
, the same score function as computed by the deep architecture., In this paper,
, we prove the expressive power theorem (an exponential lower bound on
, the width of the equivalent shallow network) for a class of recurrent neural
, networks  ones that correspond to the Tensor Train (TT) decomposition.
, This means that even processing an image patch by patch with an RNN
, can be exponentially more efficient than a (shallow) convolutional network
, with one hidden layer., Using theoretical results on the relation between
, the tensor decompositions we compare expressive powers of the HT- and
, TT-Networks., We also implement the recurrent TT-Networks and provide
, numerical evidence of their expressivity.",21,5.456989247311828,8.454545454545455
415,"['Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN).', 'In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior.', 'Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference.', 'Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution.', 'Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.']","[0, 1, 0, 0, 0]","[0.0, 0.25806450843811035, 0.1428571343421936, 0.1818181723356247, 0.1621621549129486]",H1l7bnR5Ym,"['A novel probabilistic treatment for GAN with theoretical guarantee.', 'This paper proposes a Bayesian GAN that has theoretical guarantees of convergence to the real distribution and put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions.']","['probabilistic modelling principled framework perform model aggregation  primary mechanism combat mode collapse context generative adversarial network  gan  ', 'paper  propose novel probabilistic framework gans  probgan  iteratively learns distribution generator carefully crafted prior ', 'learning efficiently triggered tailored stochastic gradient hamiltonian monte carlo novel gradient approximation perform bayesian inference ', 'theoretical analysis reveals treatment first probabilistic framework yield equilibrium generator distribution faithful data distribution ', 'empirical evidence synthetic highdimensional multimodal data image database  cifar10  stl10  imagenet  demonstrates superiority method startoftheart multigenerator gans probabilistic treatment gans ']","Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN)., In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior., Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference., Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution., Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.",11,6.392307692307693,11.818181818181818
416,"[""In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and applies a \\emph{small perturbation} to  $\\bfx$ to produce another point $\\bfx'$ that $F$ classifies \\emph{incorrectly}."", "" In this paper, we propose taking into account \\emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \\|F(\\bfx)\\|_\\infty$ (i.e. how confident $F$ is about its prediction?)"", "". Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \\emph{confident regions of a good model should be well separated}."", 'We give formalizations of this property and examine existing robust training objectives in view of them.', 'Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence.', ""However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions."", 'We thus propose embedding objectives and algorithms, and perform an empirical study using this method.', ""Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.\n""]","[0, 0, 1, 0, 0, 0, 0, 0]","[0.10256409645080566, 0.08888888359069824, 0.10810810327529907, 0.07999999821186066, 0.04651162400841713, 0.0, 0.0, 0.04444444179534912]",Hk-FlMbAZ,"['Defending against adversarial perturbations of neural networks from manifold assumption ', 'The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples.', 'Defending against adversarial attacks based on the manifold assumption of natural data']","['adversarialperturbation problem neural network  adversary start neural network model  f  point  bfx   f  classifies correctly  applies emph  small perturbation   bfx  produce another point  bfx    f  classifies emph  incorrectly  ', 'paper  propose taking account emph  inherent confidence information  produced model studying adversarial perturbation  natural measure  confidence  f  bfx  infty   ie  confident  f  prediction  ', ' motivated thought experiment based manifold assumption  propose  goodness property  model state emph  confident region good model well separated  ', 'give formalization property examine existing robust training objective view ', 'interestingly  find recent objective madry et al  encourages training model satisfies well formal version goodness property  weak control point wrong low confidence ', 'however  madry et al  model indeed good solution objective  good bad point distinguishable try embed uncertain point back closest confident region get  hopefully  correct prediction ', 'thus propose embedding objective algorithm  perform empirical study using method ', 'experimental result encouraging  madry et al  model wrapped embedding procedure achieves almost perfect success rate defending attack base model fails  retaining good generalization behavior ']","In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\bfx$ that $F$ classifies correctly, and applies a \emph{small perturbation} to  $\bfx$ to produce another point $\bfx'$ that $F$ classifies \emph{incorrectly}.,  In this paper, we propose taking into account \emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its prediction?), . Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \emph{confident regions of a good model should be well separated}., We give formalizations of this property and examine existing robust training objectives in view of them., Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence., However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions., We thus propose embedding objectives and algorithms, and perform an empirical study using this method., Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.
",19,5.625498007968128,10.541666666666666
417,"['Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.', 'Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.', 'In DSO-NAS, we provide a novel model pruning view to NAS problem.', 'In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.', 'Next, we impose sparse regularizations to prune useless connections in the architecture.', 'Lastly, we derive an efficient and theoretically sound optimization method to solve it.', 'Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.', 'Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.05882352590560913, 0.0, 0.0, 0.0, 0.1904761791229248, 0.09090908616781235, 0.0, 0.0]",ryxjH3R5KQ,"['single shot neural architecture search via direct sparse optimization', 'Presents an architecture search method where connections are removed with sparse regularization.', 'This paper proposes Direct Sparse Optimization, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.', 'This paper proposes a neural architecture search method based on a direct sparse optimization']","['recently neural architecture search  na  aroused great interest academia industry  however remains challenging huge noncontinuous search space ', 'instead applying evolutionary algorithm reinforcement learning previous work  paper proposes direct sparse optimization na  dsonas  method ', 'dsonas  provide novel model pruning view na problem ', 'specific  start completely connected block  introduce scaling factor scale information flow operation ', 'next  impose sparse regularization prune useless connection architecture ', 'lastly  derive efficient theoretically sound optimization method solve ', 'method enjoys advantage differentiability efficiency  therefore directly applied large datasets like imagenet ', 'particularly  cifar10 dataset  dsonas achieves average test error 284   imagenet dataset dsonas achieves 254  test error 600m flop 8 gpus 18 hour ']","Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space., Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method., In DSO-NAS, we provide a novel model pruning view to NAS problem., In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations., Next, we impose sparse regularizations to prune useless connections in the architecture., Lastly, we derive an efficient and theoretically sound optimization method to solve it., Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet., Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.",19,5.766233766233766,8.105263157894736
418,"['Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning.', 'One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices.', 'This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks.', 'The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels.', 'The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  ', 'We validate both methods through experiments on convolutional and recurrent architectures.', 'We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction.', 'In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n']","[0, 0, 0, 0, 0, 0, 0, 1]","[0.0, 0.0, 0.0555555522441864, 0.08888888359069824, 0.060606054961681366, 0.0, 0.09756097197532654, 0.1875]",S1XolQbRW,"['Obtains state-of-the-art accuracy for quantized, shallow nets by leveraging distillation. ', 'Proposes small and low-cost models by combining distillation and quantization for vision and neural machine translation experiments', 'This paper presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression.']","['deep neural network  dnns  continue make significant advance  solving task image classification translation reinforcement learning ', 'one aspect field receiving considerable attention efficiently executing deep model resourceconstrained environment  mobile embedded device ', 'paper focus problem  proposes two new compression method  jointly leverage weight quantization distillation larger teacher network smaller student network ', 'first method propose called quantized distillation leverage distillation training process  incorporating distillation loss  expressed respect teacher  training student network whose weight quantized limited set level ', 'second method  differentiable quantization  optimizes location quantization point stochastic gradient descent  better fit behavior teacher model ', 'validate method experiment convolutional recurrent architecture ', 'show quantized shallow student reach similar accuracy level fullprecision teacher model  providing order magnitude compression  inference speedup linear depth reduction ', 'sum  result enable dnns resourceconstrained environment leverage architecture accuracy advance developed powerful device ']","Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning., One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices., This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks., The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels., The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  , We validate both methods through experiments on convolutional and recurrent architectures., We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction., In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.
",21,6.248730964467005,9.380952380952381
419,"['Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation.', 'However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful.', 'In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation.', 'We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side.', 'Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.']","[0, 0, 0, 0, 1]","[0.0833333283662796, 0.0, 0.0833333283662796, 0.07999999821186066, 0.1428571343421936]",Bkl1uWb0Z,"['improve NMT with latent trees', 'This paper describes a method to induce source-side dependency structures in service to neural machine translation.']","['previous work demonstrated benefit incorporating additional linguistic annotation syntactic tree neural machine translation ', 'however cost obtaining syntactic annotation expensive many language quality unsupervised learning linguistic structure poor helpful ', 'work  aim improve neural machine translation via source side dependency syntax without explicit annotation ', 'propose set model learn induce dependency tree source side learn use information target side ', 'importantly  also show dependency tree capture important syntactic feature language improve translation quality two language pair ende enru ']","Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation., However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful., In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation., We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side., Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.",7,5.603448275862069,16.571428571428573
420,"['Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards.', 'We explore a method to improve the sample efficiency when we have access to demonstrations.', 'Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task.', ""Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state."", 'Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency.', 'This includes reward shaping, behavioral cloning, and reverse curriculum generation.']","[0, 0, 0, 1, 0, 0]","[0.0952380895614624, 0.1666666567325592, 0.11428570747375488, 0.19999998807907104, 0.1249999925494194, 0.0624999962747097]",H1xk8jAqKQ,"['Learn by working backwards from a single demonstration, even an inefficient one, and progressively have the agent do more of the solving itself.', 'This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. ', 'The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations.']","['modelfree reinforcement learning  rl  requires large number trial learn good policy  especially environment sparse reward ', 'explore method improve sample efficiency access demonstration ', 'approach  backplay  us single demonstration construct curriculum given task ', 'rather starting training episode environment fixed initial state  start agent near end demonstration move starting point backwards course training reach initial state ', 'contribution analytically characterize type environment backplay improve training speed  demonstrate effectiveness backplay large grid world complex four player zerosum game  pommerman   show backplay compare favorably competitive method known improve sample efficiency ', 'includes reward shaping  behavioral cloning  reverse curriculum generation ']","Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards., We explore a method to improve the sample efficiency when we have access to demonstrations., Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task., Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state., Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency., This includes reward shaping, behavioral cloning, and reverse curriculum generation.",14,5.6866666666666665,10.714285714285714
421,"['Episodic memory is a psychology term which refers to the ability to recall specific events from the past.', 'We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful.', 'Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states.', 'The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on.', 'Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory.', 'Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.\n']","[0, 0, 0, 0, 0, 1]","[0.1249999925494194, 0.09756097197532654, 0.22641508281230927, 0.25641024112701416, 0.17142856121063232, 0.277777761220932]",ByJDAIe0b,"['External memory for online reinforcement learning based on estimating gradients over a novel reservoir sampling technique.', 'The paper proposes a modified approach to RL, where an additional ""episodic memory"" is kept by the agent and use a ""query network"" that based on the current state.']","['episodic memory psychology term refers ability recall specific event past ', 'suggest one advantage particular type memory ability easily assign credit specific state remembered information found useful ', 'inspired idea  increasing popularity external memory mechanism handle longterm dependency deep learning system  propose novel algorithm us reservoir sampling procedure maintain external memory consisting fixed number past state ', 'algorithm allows deep reinforcement learning agent learn online preferentially remember state found useful recall later ', 'critically method allows efficient online computation gradient estimate respect write process external memory ', 'thus unlike prior mechanism external memory feasible use online reinforcement learning setting ']","Episodic memory is a psychology term which refers to the ability to recall specific events from the past., We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful., Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states., The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on., Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory., Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.
",8,5.292993630573249,19.625
422,"['We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.', 'Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases.', 'Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.']","[1, 0, 0]","[1.0, 0.15789473056793213, 0.0]",rkMt1bWAZ,"['We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.', 'The goal of this paper is to analyze the effectiveness and generalizability of deep learning by presenting a theoretical analysis of bias-variance decomposition for hierarchical models, specifically Boltzmann Machines  ', 'The paper arrives at the main conclusion that it is possible to reduce both the bias and the variance in a hierarchical model.']","['achieve biasvariance decomposition boltzmann machine using information geometric formulation ', 'decomposition lead interesting phenomenon variance necessarily increase parameter included boltzmann machine  bias always decrease ', 'result give theoretical evidence generalization ability deep learning architecture provides possibility increasing representation power avoiding variance inflation ']","We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation., Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases., Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.",4,6.5,17.0
423,"['Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  ', 'Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  ', 'We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  ', 'With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.  ', 'Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.  ', 'We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.']","[0, 1, 0, 0, 0, 0]","[0.1111111044883728, 0.21052631735801697, 0.1428571343421936, 0.10810810327529907, 0.05405404791235924, 0.060606054961681366]",HkxF5RgC-,"['Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.', 'This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip.']","['recurrent neural network  rnns  powerful tool solving sequencebased problem  efficacy execution time dependent size network ', 'following recent work simplifying network model pruning novel mapping work onto gpus  design efficient implementation sparse rnns ', 'investigate several optimization tradeoff  lamport timestamps  wide memory load  bankaware weight layout ', 'optimization  achieve speedup 6x next best algorithm hidden layer size 2304  batch size 4  density 30  ', ' technique allows model 5x size fit gpu speedup 2x  enabling larger network help advance stateoftheart ', 'perform case study nmt speech recognition task appendix  accelerating recurrent layer 3x ']","Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  , Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  , We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  , With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.  , Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.  , We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.",16,5.068027210884353,9.1875
424,"['Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy.', 'Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources.', 'Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists.', 'In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed.', 'In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm.', 'The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process.', 'Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate.', 'Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.19999998807907104, 0.2222222238779068, 0.24390242993831635, 0.375, 0.17391303181648254, 0.260869562625885, 0.12765957415103912, 0.1860465109348297]",S1D8MPxA-,"['We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process.', 'The authors use Viterbi encoding to dramatically compress the sparse matrix index of a pruned network, reducing one of the main memory overheads and speeding up inference in the parallel setting.']","['weight pruning proven effective method reducing model size computation cost sacrificing model accuracy ', 'conventional sparse matrix format  however  involve irregular index structure large storage requirement sequential reconstruction process  resulting inefficient use highly parallel computing resource ', 'hence  pruning usually restricted inference batch size one  efficient parallel matrixvector multiplication method exists ', 'paper  new class sparse matrix representation utilizing viterbi algorithm high  importantly  fixed index compression ratio regardless pruning rate  proposed ', 'approach  numerous sparse matrix candidate first generated viterbi encoder  one aim minimize model accuracy degradation selected viterbi algorithm ', 'model pruning process based proposed viterbi encoder viterbi algorithm highly parallelizable  implemented efficiently hardware achieve lowenergy  highperformance index decoding process ', 'compared existing magnitudebased pruning method  index data storage requirement compressed 852  mnist 839  alexnet achieving similar pruning rate ', 'even compared relative index compression technique  method still reduce index storage requirement 527  mnist 355  alexnet ']","Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy., Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources., Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists., In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed., In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm., The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process., Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate., Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.",23,5.815668202764977,9.434782608695652
425,"['Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL).', 'It is also a requirement for its deployment in real-world scenarios.', 'This paper proposes a novel framework for efficient multi-task reinforcement learning.', 'Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.', 'This enables agents to continually acquire new skills during different stages of training.', 'Each learned task corresponds to a human language description.', 'Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices.', 'In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills.', 'We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.04878048226237297, 0.0, 0.05882352590560913, 0.2790697515010834, 0.1666666567325592, 0.0624999962747097, 0.27272728085517883, 0.2545454502105713, 0.27272728085517883]",SJJQVZW0b,"['A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.', 'This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar', 'This paper proposes an approach to learning hierarchical policies in a lifelong learning context by stacking policies and then using an explicit ""switch"" policy.']","['learning policy complex task require multiple different skill major challenge reinforcement learning  rl  ', 'also requirement deployment realworld scenario ', 'paper proposes novel framework efficient multitask reinforcement learning ', 'framework train agent employ hierarchical policy decide use previously learned policy learn new skill ', 'enables agent continually acquire new skill different stage training ', 'learned task corresponds human language description ', 'agent access previously learned skill description  agent always provide humaninterpretable description choice ', 'order help agent learn complex temporal dependency necessary hierarchical policy  provide stochastic temporal grammar modulates rely previously learned skill execute new skill ', 'validate approach minecraft game designed explicitly test ability reuse previously learned skill simultaneously learning new skill ']","Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL)., It is also a requirement for its deployment in real-world scenarios., This paper proposes a novel framework for efficient multi-task reinforcement learning., Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill., This enables agents to continually acquire new skills during different stages of training., Each learned task corresponds to a human language description., Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices., In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills., We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.",11,5.752941176470588,15.454545454545455
426,"['Embeddings are a fundamental component of many modern machine learning and natural language processing models.\n', 'Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.\n', 'State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.\n', 'In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.\n', 'This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.\n', 'We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study.', 'The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.']","[0, 0, 0, 1, 0, 0, 0]","[0.043478257954120636, 0.0833333283662796, 0.09836065024137497, 0.2857142686843872, 0.21052631735801697, 0.19230768084526062, 0.178571417927742]",Skz3Q2CcFX,"['We propose to use explicit vector algebraic formulae projection as an alternative way to visualize embedding spaces specifically tailored for goal-oriented analysis tasks and it outperforms t-SNE in our user study.', 'Analysis of embedding psaces in a non-parametric (example-based_ way']","['embeddings fundamental component many modern machine learning natural language processing model ', 'understanding visualizing essential gathering insight information capture behavior model ', 'state art analyzing embeddings consists projecting twodimensional plane without interpretable semantics associated ax projection  make detailed analysis comparison among multiple set embeddings challenging ', 'work  propose use explicit ax defined algebraic formula embeddings project lower dimensional  semantically meaningful subspace  simple yet effective analysis visualization methodology ', 'methodology assigns interpretable semantics measure variability ax visualization  allowing comparison among different set embeddings finegrained inspection embedding space ', 'demonstrate power proposed methodology series case study make use visualization constructed around underlying methodology user study ', 'result show methodology effective providing profound insight classical projection method widely applicable many use case ']","Embeddings are a fundamental component of many modern machine learning and natural language processing models.
, Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.
, State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.
, In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.
, This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.
, We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study., The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.",12,5.862944162436548,16.416666666666668
427,"['Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  ', 'Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   ', 'However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample.', 'In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  ', 'This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  ', 'Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  ', 'Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.']","[0, 0, 0, 1, 0, 0, 0]","[0.3404255211353302, 0.20512820780277252, 0.17777776718139648, 0.3913043439388275, 0.072727270424366, 0.1860465109348297, 0.3448275923728943]",SyG1QnRqF7,"['We propose a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner.', 'Using introspective learning to handle data variations at test time', 'This paper suggests the use of learned transformation networks, embedded within introspective networks to improve classification performance with synthesized examples.']","['learning deep network resist large variation training andtesting data essential build accurate robust image classifier ', 'towardsthis end  typical strategy apply data augmentation enlarge trainingset ', 'however  standard data augmentation essentially bruteforce strategywhich inefficient  performs predefined transformation everytraining sample ', 'paper  propose principled approach train networkswith significantly improved resistance large variation training andtesting data ', 'achieved embedding learnable transformation moduleinto introspective network  jin et al  2017  lazarow et al  2017  lee et al2018   convolutional neural network  cnn  classifier empowered withgenerative capability ', 'approach alternatively synthesizes pseudonegativesamples learned transformation enhances classifier retraining itwith synthesized sample ', 'experimental result verify approach significantly improves ability deep network resist large variation betweentraining testing data achieves classification accuracy improvement onseveral benchmark datasets  including mnist  affnist  svhn cifar10 ']","Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  , Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   , However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample., In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  , This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  , Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  , Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.",17,6.723926380368098,9.588235294117647
428,"['It is well known that it is possible to construct ""adversarial examples""\n', 'for neural networks: inputs which are misclassified by the network\n', 'yet indistinguishable from true data.', 'We propose a simple\n', 'modification to standard neural network architectures, thermometer\n', 'encoding, which significantly increases the robustness of the network to\n', 'adversarial examples.', 'We demonstrate this robustness with experiments\n', 'on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that\n', 'models with thermometer-encoded inputs consistently have higher accuracy\n', 'on adversarial examples, without decreasing generalization.\n', 'State-of-the-art accuracy under the strongest known white-box attack was \n', 'increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.\n', 'We explore the properties of these networks, providing evidence\n', 'that thermometer encodings help neural networks to\n', 'find more-non-linear decision boundaries.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.10526315122842789, 0.0, 0.0, 0.0, 0.13333332538604736, 0.23529411852359772, 0.1428571343421936, 0.0, 0.0, 0.13333332538604736, 0.0, 0.0833333283662796, 0.0, 0.13333332538604736, 0.0]",S18Su--CW,"['Input discretization leads to robustness against adversarial examples', 'The authors present an in-depth study of discretizing / quantizing the input as a defense against adversarial examples']","['well known possible construct  adversarial example ', 'neural network  input misclassified network', 'yet indistinguishable true data ', 'propose simple', 'modification standard neural network architecture  thermometer', 'encoding  significantly increase robustness network', 'adversarial example ', 'demonstrate robustness experiment', 'mnist  cifar10  cifar100  svhn datasets  show', 'model thermometerencoded input consistently higher accuracy', 'adversarial example  without decreasing generalization ', 'stateoftheart accuracy strongest known whitebox attack', 'increased 9320  9430  mnist 5000  7916  cifar10 ', 'explore property network  providing evidence', 'thermometer encoding help neural network', 'find morenonlinear decision boundary ']","It is well known that it is possible to construct ""adversarial examples""
, for neural networks: inputs which are misclassified by the network
, yet indistinguishable from true data., We propose a simple
, modification to standard neural network architectures, thermometer
, encoding, which significantly increases the robustness of the network to
, adversarial examples., We demonstrate this robustness with experiments
, on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that
, models with thermometer-encoded inputs consistently have higher accuracy
, on adversarial examples, without decreasing generalization.
, State-of-the-art accuracy under the strongest known white-box attack was 
, increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.
, We explore the properties of these networks, providing evidence
, that thermometer encodings help neural networks to
, find more-non-linear decision boundaries.",24,6.365853658536586,5.125
429,"['Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.\n', 'Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.\n', 'These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.\n', 'This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.\n', 'In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.\n', 'Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.']","[0, 0, 0, 0, 1, 0]","[0.07999999821186066, 0.20689654350280762, 0.0624999962747097, 0.20000000298023224, 0.3684210479259491, 0.1818181723356247]",ryeX-nC9YQ,"['we proved dimension-independent bounds for low-precision training algorithms', 'This paper discusses conditions under which the convergence of training models with low-precision weights do not rely on model dimension.']","['lowprecision training promising way decreasing time energy cost training machine learning model ', 'previous work analyzed lowprecision training algorithm  lowprecision stochastic gradient descent  derived theoretical bound convergence rate ', 'bound tend depend dimension model   number bit needed achieve particular error bound increase   increase ', 'undesirable motivating application lowprecision training largescale model  deep learning    huge ', 'paper  prove dimensionindependent bound lowprecision training algorithm use fixedpoint arithmetic  let u better understand affect convergence algorithm parameter scale ', 'method also generalize naturally let u prove new convergence bound lowprecision training quantization scheme  lowprecision floatingpoint computation logarithmic quantization ']","Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.
, Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.
, These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.
, This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.
, In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.
, Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.",13,6.0272108843537415,11.307692307692308
430,"['We consider the problem of exploration in meta reinforcement learning.', 'Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2.', ""Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments."", 'We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.']","[0, 0, 0, 1]","[0.09090908616781235, 0.08695651590824127, 0.0714285671710968, 0.23076923191547394]",Skk3Jm96W,"['Modifications to MAML and RL2 that should allow for better exploration. ', 'The paper proposes a trick of extending objective functions to drive exploration in meta-RL on top of two recent meta-RL algorithms']","['consider problem exploration meta reinforcement learning ', 'two new meta reinforcement learning algorithm suggested  emaml erl2 ', 'result presented novel environment call krazy world  set maze environment ', 'show emaml erl2 deliver better performance task exploration important ']","We consider the problem of exploration in meta reinforcement learning., Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2., Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments., We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.",4,5.5576923076923075,13.0
431,"['We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs.', 'The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions.', 'We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively.', 'Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work.']","[1, 0, 0, 0]","[0.260869562625885, 0.1621621549129486, 0.11764705181121826, 0.1702127605676651]",ryxhB3CcK7,"['A probabilistic neural symbolic model with a latent program space, for more interpretable question answering', 'This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning with significant gain in performance and capability.']","['propose new class probabilistic neuralsymbolic model visual question answering  vqa  provide interpretable explanation decision making form program  given small annotated set human program ', 'key idea approach learn rich latent space effectively propagates program annotation known question novel question ', 'formalizing prior work vqa  called module network  andreas  2016  discrete  structured  latent variable model joint distribution question answer given image  devise procedure train model effectively ', 'result dataset compositional question shape  andreas  2016  show model generates interpretable program obtains better accuracy vqa lowdata regime prior work ']","We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs., The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions., We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively., Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work.",11,5.473282442748092,11.909090909090908
432,"['The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.', 'In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.', ""This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks."", 'We propose to address this difficulty through formal verification techniques.', 'We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.', 'We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.', 'We use this technique to assess recently suggested attack and defense techniques.\n']","[0, 0, 0, 1, 0, 0, 0]","[0.1860465109348297, 0.21739129722118378, 0.2222222238779068, 0.37037035822868347, 0.19354838132858276, 0.3199999928474426, 0.3333333432674408]",Hki-ZlbA-,"['We use formal verification to assess the effectiveness of techniques for finding adversarial examples or for defending against adversarial examples.', 'This paper proposes a method for computing adversarial examples with minimum distance to the original inputs.', 'The authors propose to employ provably minimal-distance examples as a tool to evaluate the robustness of a trained network.', 'The paper describes a method for generating adversarial examples that have minimal distance to the training example used to generate them']","['ability deploy neural network realworld  safetycritical system severely limited presence adversarial example  slightly perturbed input misclassified network ', 'recent year  several technique proposed training network robust example  time stronger attack devised  demonstrating shortcoming existing defense ', 'highlight key difficulty designing effective defense  inability ass network robustness future attack ', 'propose address difficulty formal verification technique ', 'construct ground truth  adversarial example provablyminimal distance given input point ', 'demonstrate ground truth serve ass effectiveness attack technique  comparing adversarial example produced attack ground truth  also defense technique  computing distance ground truth defense applied  measuring improvement ', 'use technique ass recently suggested attack defense technique ']","The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network., In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses., This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks., We propose to address this difficulty through formal verification techniques., We construct ground truths: adversarial examples with a provably-minimal distance from a given input point., We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement., We use this technique to assess recently suggested attack and defense techniques.
",13,5.874251497005988,12.846153846153847
433,"['This paper introduces a new framework for open-domain question answering in which the retriever and the reader \\emph{iteratively interact} with each other.', 'The framework is agnostic to the architecture of the machine reading model provided it has \\emph{access} to the token-level hidden representations of the reader.', 'The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs.', 'A gated recurrent unit updates the query at each step conditioned on the \\emph{state} of the reader and the \\emph{reformulated} query is used to re-rank the paragraphs by the retriever.', 'We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.', 'Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\\drqa and \\bidaf) on various large open-domain datasets ---\\tqau, \\quasart, \\searchqa, and \\squado\\footnote{Code and pretrained models are available at \\url{https://github.com/rajarshd/Multi-Step-Reasoning}}.']","[1, 0, 0, 0, 0, 0]","[0.2926829159259796, 0.1538461446762085, 0.10810810327529907, 0.27272728085517883, 0.0555555522441864, 0.21052631735801697]",HkfPSh05K7,"['Paragraph retriever and machine reader interacts with each other via reinforcement learning to yield large improvements on open domain datasets', ""The paper introduces a new framework of bi-directional interaction between document retriever and reader for open-domain question answering with idea of 'reader state' from reader to retriever."", 'The paper proposes a multi-document extractive machine reading model composed of 3 distinct parts and an algorithm.']","['paper introduces new framework opendomain question answering retriever reader emph  iteratively interact  ', 'framework agnostic architecture machine reading model provided emph  access  tokenlevel hidden representation reader ', 'retriever us fast nearest neighbor search allows scale corpus containing million paragraph ', 'gated recurrent unit update query step conditioned emph  state  reader emph  reformulated  query used rerank paragraph retriever ', 'conduct analysis show iterative interaction help retrieving informative paragraph corpus ', 'finally  show multistepreasoning framework brings consistent improvement applied two widely used reader architecture  drqa bidaf  various large opendomain datasets  tqau  quasart  searchqa  squadofootnote  code pretrained model available url  http  githubcomrajarshdmultistepreasoning   ']","This paper introduces a new framework for open-domain question answering in which the retriever and the reader \emph{iteratively interact} with each other., The framework is agnostic to the architecture of the machine reading model provided it has \emph{access} to the token-level hidden representations of the reader., The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs., A gated recurrent unit updates the query at each step conditioned on the \emph{state} of the reader and the \emph{reformulated} query is used to re-rank the paragraphs by the retriever., We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus., Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\drqa and \bidaf) on various large open-domain datasets ---\tqau, \quasart, \searchqa, and \squado\footnote{Code and pretrained models are available at \url{https://github.com/rajarshd/Multi-Step-Reasoning}}.",10,6.324324324324325,14.8
434,"['Many imaging tasks require global information about all pixels in an image.', 'Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output.', 'But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs.', 'To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost.', 'This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution.', 'SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images.', 'SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.']","[0, 0, 0, 0, 0, 1, 0]","[0.17142856121063232, 0.19512194395065308, 0.10526315122842789, 0.19607841968536377, 0.10256409645080566, 0.3636363446712494, 0.21621620655059814]",BJgFcj0qKX,"['Presents new architecture which leverages information globalization power of u-nets in a deeper networks and performs well across tasks without any bells and whistles.', 'A network architecture for semantic image segmentation, based on composing a stack of basic U-Net architectures, that reduces the number of parameters and improves results.', 'This proposes a stacked U-Net architecture for image segmentation.']","['many imaging task require global information pixel image ', 'conventional bottomup classification network globalize information decreasing resolution  feature pooled downsampled single output ', 'semantic segmentation object detection task  network must provide higherresolution pixellevel output ', 'globalize information preserving resolution  many researcher propose inclusion sophisticated auxiliary block  come cost considerable increase network size computational cost ', 'paper proposes stacked unets  sunets   iteratively combine feature different resolution scale maintaining resolution ', 'sunets leverage information globalization power unets deeper net work architecture capable handling complexity natural image ', 'sunets perform extremely well semantic segmentation task using small number parameter ']","Many imaging tasks require global information about all pixels in an image., Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output., But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs., To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost., This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution., SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images., SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.",11,6.3893129770992365,11.909090909090908
435,"['Asking questions is an important ability for a chatbot.', 'This paper focuses on question generation.', 'Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem.', 'In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.', 'The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics.', 'One almost never asks a random question in a conversation.', 'Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use.', 'To solve the problem, we propose a novel neural network that is able to generate topic-specific questions.', 'One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers.', 'Experimental results show that our model outperforms the state-of-the-art baseline.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.2857142686843872, 0.0, 0.12121211737394333, 0.17142856121063232, 0.21621620655059814, 0.0952380895614624, 0.21052631735801697, 0.7586206793785095, 0.1904761791229248, 0.09090908616781235]",rk3pnae0b,"['We propose a neural network that is able to generate topic-specific questions.', 'Presents a neural network-based approach to generate topic-specific questions with the motivation that topical questions are more meaningful in practical applications.', 'Proposes a topic-based generation method using an LSTM to extract topics using a two-stage encoding technique']","['asking question important ability chatbot ', 'paper focus question generation ', 'although existing work question generation based piece descriptive text  remains challenging problem ', 'paper  propose new question generation problem  also requires input target topic addition piece descriptive text ', 'key reason proposing new problem practical application  found useful question need targeted toward relevant topic ', 'one almost never asks random question conversation ', 'due fact given descriptive text  often possible ask many type question  generating question without knowing limited use ', 'solve problem  propose novel neural network able generate topicspecific question ', 'one major advantage model trained directly using questionanswering corpus without requiring additional annotation like annotating topic question answer ', 'experimental result show model outperforms stateoftheart baseline ']","Asking questions is an important ability for a chatbot., This paper focuses on question generation., Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem., In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text., The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics., One almost never asks a random question in a conversation., Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use., To solve the problem, we propose a novel neural network that is able to generate topic-specific questions., One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers., Experimental results show that our model outperforms the state-of-the-art baseline.",17,5.147368421052631,11.176470588235293
436,"['Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option\n', 'to restore voluntary movements after paralysis.', 'These devices are based on the\n', 'ability to extract information about movement intent from neural signals recorded\n', 'using multi-electrode arrays chronically implanted in the motor cortices of the\n', 'brain.', 'However, the inherent loss and turnover of recorded neurons requires repeated\n', 'recalibrations of the interface, which can potentially alter the day-to-day\n', 'user experience.', 'The resulting need for continued user adaptation interferes with\n', 'the natural, subconscious use of the BMI.', 'Here, we introduce a new computational\n', 'approach that decodes movement intent from a low-dimensional latent representation\n', 'of the neural data.', 'We implement various domain adaptation methods\n', 'to stabilize the interface over significantly long times.', 'This includes Canonical\n', 'Correlation Analysis used to align the latent variables across days; this method\n', 'requires prior point-to-point correspondence of the time series across domains.\n', 'Alternatively, we match the empirical probability distributions of the latent variables\n', 'across days through the minimization of their Kullback-Leibler divergence.\n', 'These two methods provide a significant and comparable improvement in the performance\n', 'of the interface.', 'However, implementation of an Adversarial Domain\n', 'Adaptation Network trained to match the empirical probability distribution of the\n', 'residuals of the reconstructed neural signals outperforms the two methods based\n', 'on latent variables, while requiring remarkably few data points to solve the domain\n', 'adaptation problem.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.1249999925494194, 0.07407406717538834, 0.07407406717538834, 0.25, 0.12903225421905518, 0.1249999925494194, 0.06666666269302368, 0.06666666269302368, 0.07407406717538834, 0.07407406717538834, 0.06451612710952759, 0.1599999964237213, 0.29629629850387573, 0.20689654350280762, 0.0, 0.12121211737394333, 0.0624999962747097, 0.06451612710952759, 0.06451612710952759, 0.1818181723356247, 0.0833333283662796, 0.07407406717538834, 0.12903225421905518, 0.19354838132858276, 0.1764705777168274]",Hyx6Bi0qYm,"['We implement an adversarial domain adaptation network to stabilize a fixed Brain-Machine Interface against gradual changes in the recorded neural signals.', 'Describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift. ', 'The authors define a BMI that uses an autoencoder and then address the problem of data drift in BMI.']","['brainmachine interface  bmi  recently emerged clinically viable option', 'restore voluntary movement paralysis ', 'device based', 'ability extract information movement intent neural signal recorded', 'using multielectrode array chronically implanted motor cortex', 'brain ', 'however  inherent loss turnover recorded neuron requires repeated', 'recalibrations interface  potentially alter daytoday', 'user experience ', 'resulting need continued user adaptation interferes', 'natural  subconscious use bmi ', ' introduce new computational', 'approach decodes movement intent lowdimensional latent representation', 'neural data ', 'implement various domain adaptation method', 'stabilize interface significantly long time ', 'includes canonical', 'correlation analysis used align latent variable across day  method', 'requires prior pointtopoint correspondence time series across domain ', 'alternatively  match empirical probability distribution latent variable', 'across day minimization kullbackleibler divergence ', 'two method provide significant comparable improvement performance', 'interface ', 'however  implementation adversarial domain', 'adaptation network trained match empirical probability distribution', 'residual reconstructed neural signal outperforms two method based', 'latent variable  requiring remarkably data point solve domain', 'adaptation problem ']","Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option
, to restore voluntary movements after paralysis., These devices are based on the
, ability to extract information about movement intent from neural signals recorded
, using multi-electrode arrays chronically implanted in the motor cortices of the
, brain., However, the inherent loss and turnover of recorded neurons requires repeated
, recalibrations of the interface, which can potentially alter the day-to-day
, user experience., The resulting need for continued user adaptation interferes with
, the natural, subconscious use of the BMI., Here, we introduce a new computational
, approach that decodes movement intent from a low-dimensional latent representation
, of the neural data., We implement various domain adaptation methods
, to stabilize the interface over significantly long times., This includes Canonical
, Correlation Analysis used to align the latent variables across days; this method
, requires prior point-to-point correspondence of the time series across domains.
, Alternatively, we match the empirical probability distributions of the latent variables
, across days through the minimization of their Kullback-Leibler divergence.
, These two methods provide a significant and comparable improvement in the performance
, of the interface., However, implementation of an Adversarial Domain
, Adaptation Network trained to match the empirical probability distribution of the
, residuals of the reconstructed neural signals outperforms the two methods based
, on latent variables, while requiring remarkably few data points to solve the domain
, adaptation problem.",35,6.283783783783784,6.3428571428571425
437,"['Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds.', 'To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words.', 'While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so.', 'Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions.', 'For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world.', 'We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge.', 'We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.']","[0, 0, 0, 0, 0, 0, 1]","[0.1395348757505417, 0.1666666567325592, 0.1395348757505417, 0.25, 0.24390242993831635, 0.11428570747375488, 0.25641024112701416]",ByZmGjkA-,"['Analysing and understanding how neural network agents learn to understand simple grounded language', 'The authors connect psychological experimental methods to understanding how the black box of deep learning methods solves problems.', 'This paper presents an analysis of the agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information']","['neural networkbased system learn locate referent word phrase image  answer question visual scene  even execute symbolic instruction firstperson actor partiallyobservable world ', 'achieve socalled grounded language learning  model must overcome certain wellstudied learning challenge also fundamental infant learning first word ', 'notable model meaningful prior knowledge overcome learning obstacle  ai researcher practitioner currently lack clear understanding exactly ', 'address question way achieving clearer general understanding grounded language learning  inform future research improve confidence model prediction ', 'maximum control generality  focus simple neural networkbased language learning agent trained via policygradient method interpret synthetic linguistic instruction simulated 3d world ', 'apply experimental paradigm developmental psychology agent  exploring condition established human bias learning effect emerge ', 'propose novel way visualise analyse semantic representation grounded language learning agent yield plausible computational account observed effect ']","Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds., To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words., While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so., Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions., For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world., We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge., We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.",14,6.030927835051546,13.857142857142858
438,"['Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future.', 'In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos.', 'Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future.', 'Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.']","[1, 0, 0, 0]","[0.4000000059604645, 0.17142856121063232, 0.20408162474632263, 0.25641024112701416]",rJe10iC5K7,"['Learning object parts, hierarchical structure, and dynamics by watching how they move', 'Proposes an unsupervised learning model that learns to disentangle objects into parts, predict hierarchical structure for the parts, and based on the disentangled parts and the hierarchy, predict motion.']","['human easily recognize object part hierarchical structure watching move  predict part move future ', 'paper  propose novel formulation simultaneously learns hierarchical  disentangled object representation dynamic model object part unlabeled video ', 'part  structure  dynamic  psd  model learns  first  recognize object part via layered image representation  second  predict hierarchy via structural descriptor composes lowlevel concept hierarchical structure  third  model system dynamic predicting future ', 'experiment multiple real synthetic datasets demonstrate psd model work well three task  segmenting object part  building hierarchical structure  capturing motion distribution ']","Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future., In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos., Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future., Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",14,5.864,8.928571428571429
439,"['A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR).', 'Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality.', 'However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them.', 'In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks.', 'We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs.', 'As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks.', 'Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.']","[0, 0, 0, 0, 1, 0, 0]","[0.06451612710952759, 0.06896550953388214, 0.054054051637649536, 0.054054051637649536, 0.13636364042758942, 0.10810810327529907, 0.0714285671710968]",BkgGmh09FQ,"['We build an understanding of resource-efficient techniques on Super-Resolution', 'The paper proposes a detailed empirical evaluation of the trade-offs achieved by various convolutional neural networks on the super resolution problem.', 'This paper proposed to improve the system resource efficiency for super resolution networks.']","['successful application convolutional architecture increase resolution single lowresolution image  image restoration task called superresolution  sr  ', 'naturally  sr value resource constrained device like mobile phone  electronic photograph frame television enhance image quality ', 'however  sr demand perhaps extreme amount memory compute operation mainstream vision task known today  preventing sr deployed device require ', 'paper  perform early systematic study system resource efficiency sr  within context variety architectural lowprecision approach originally developed discriminative neural network ', 'present rich set insight  representative sr architecture  efficiency tradeoff  example  prioritization way compress model reach specific memory computation target technique compact sr model suitable dsps fpgas ', 'result  manage achieve better comparable performance previous model existing literature  highlighting practicality using existing efficiency technique sr task ', 'collectively  believe result provides foundation research little explored area resource efficiency sr ']","A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR)., Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality., However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them., In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks., We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs., As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks., Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.",19,5.730392156862745,10.736842105263158
440,"['Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy.', 'In this work we present properties of neural networks that complement this aspect of expressivity.', 'By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior.', 'Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples.', 'We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior.', 'Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.']","[0, 0, 0, 1, 0, 0]","[0.060606054961681366, 0.07999999821186066, 0.1463414579629898, 0.19354838132858276, 0.1904761791229248, 0.0555555522441864]",r1gR2sC9FX,"['We investigate ReLU networks in the Fourier domain and demonstrate peculiar behaviour.', 'Fourier analysis of ReLU network, finding that they are biased towards learning low frequency ', 'This paper has theoretical and empirical contributions on topic of Fourier coefficients of neural networks']","['neural network known class highly expressive function able fit even random inputoutput mapping 100  accuracy ', 'work present property neural network complement aspect expressivity ', 'using tool fourier analysis  show deep relu network biased towards low frequency function  meaning local fluctuation without affecting global behavior ', 'intuitively  property line observation overparameterized network find simple pattern generalize across data sample ', 'also investigate shape data manifold affect expressivity showing evidence learning high frequency get easier increasing manifold complexity  present theoretical understanding behavior ', 'finally  study robustness frequency component respect parameter perturbation  develop intuition parameter must finely tuned express high frequency function ']","Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy., In this work we present properties of neural networks that complement this aspect of expressivity., By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior., Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples., We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior., Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.",12,5.8133333333333335,12.5
441,"['Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering.', 'Many metric learning methods represent the input as a single point in the embedding space.', 'Often the distance between points is used as a proxy for match confidence.', 'However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness.', 'This work addresses this issue and explicitly models the uncertainty by hedging the location of each input in the embedding space.', 'We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018).', 'Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of hedging its bets across the embedding space upon encountering ambiguous inputs.', 'This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.11428570747375488, 0.0624999962747097, 0.19354838132858276, 0.0, 0.10810810327529907, 0.15686273574829102, 0.04444443807005882, 0.0833333283662796]",r1xQQhAqKX,"['The paper proposes using probability distributions instead of points for instance embeddings tasks such as recognition and verification.', 'Paper proposes an alternative to current point embedding and a technique to train them.', 'The paper proposes a model using uncertain-embeddings to extend deep learning to Bayesian applications']","['instance embeddings efficient versatile image representation facilitates application like recognition  verification  retrieval  clustering ', 'many metric learning method represent input single point embedding space ', 'often distance point used proxy match confidence ', 'however  fail represent uncertainty arise input ambiguous  eg  due occlusion blurriness ', 'work address issue explicitly model uncertainty  hedging  location input embedding space ', 'introduce hedged instance embedding  hib  embeddings modeled random variable model trained variational information bottleneck principle  alemi et al  2016  achille  soatto  2018  ', 'empirical result new ndigit mnist dataset show method lead desired behavior  hedging bet  across embedding space upon encountering ambiguous input ', 'result improved performance image matching classification task  structure learned embedding space  ability compute perexemplar uncertainty measure correlated downstream performance ']","Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering., Many metric learning methods represent the input as a single point in the embedding space., Often the distance between points is used as a proxy for match confidence., However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness., This work addresses this issue and explicitly models the uncertainty by hedging the location of each input in the embedding space., We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018)., Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of hedging its bets across the embedding space upon encountering ambiguous inputs., This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.",18,5.706521739130435,10.222222222222221
442,"['Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  ', 'While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  ', 'Despite its success, this approach has notable drawbacks.', 'Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \n', 'We present two new techniques to address these problems.  ', 'First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.', 'Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  ', 'Both the contraction and regression weights are learned end-to-end by backpropagation.', 'By imposing low rank on both, we use significantly fewer parameters.  ', 'Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.04878048226237297, 0.1428571343421936, 0.0, 0.1860465109348297, 0.10810810327529907, 0.23255813121795654, 0.23076923191547394, 0.21052631735801697, 0.05128204822540283, 0.23333333432674408]",S16FPMgRZ,"['We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.', 'This paper proposes new layer architectures of neural networks using a low-rank representation of tensors', 'This paper incorporates tensor decomposition and tensor regression into CNN by using a new tensor regression layer.']","['convolution neural network typically consist many convolutional layer followed several fullyconnected layer ', 'convolutional layer map highorder activation tensor  fullyconnected layer operate flattened activation vector ', 'despite success  approach notable drawback ', 'flattening discard multidimensional structure activation  fullyconnected layer require large number parameter ', 'present two new technique address problem ', 'first  introduce tensor contraction layer replace ordinary fullyconnected layer neural network ', 'second  introduce tensor regression layer  express output neural network lowrank multilinear mapping highorder activation tensor softmax layer ', 'contraction regression weight learned endtoend backpropagation ', 'imposing low rank  use significantly fewer parameter ', 'experiment imagenet dataset show applied popular vgg resnet architecture  method significantly reduce number parameter fully connected layer  65  space saving  negligibly impacting accuracy ']","Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  , While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  , Despite its success, this approach has notable drawbacks., Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. 
, We present two new techniques to address these problems.  , First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network., Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  , Both the contraction and regression weights are learned end-to-end by backpropagation., By imposing low rank on both, we use significantly fewer parameters.  , Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.",18,6.244047619047619,9.333333333333334
443,"['We explore ways of incorporating bilingual dictionaries to enable semi-supervised\n', 'neural machine translation.', 'Conventional back-translation methods have shown\n', 'success in leveraging target side monolingual data.', 'However, since the quality of\n', 'back-translation models is tied to the size of the available parallel corpora, this\n', 'could adversely impact the synthetically generated sentences in a low resource\n', 'setting.', 'We propose a simple data augmentation technique to address both this\n', 'shortcoming.', 'We incorporate widely available bilingual dictionaries that yield\n', 'word-by-word translations to generate synthetic sentences.', 'This automatically\n', 'expands the vocabulary of the model while maintaining high quality content.', 'Our\n', 'method shows an appreciable improvement in performance over strong baselines.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.29999998211860657, 0.4615384638309479, 0.0, 0.11764705181121826, 0.0, 0.0, 0.0, 0.2857142686843872, 0.3333333432674408, 0.0, 0.0, 0.0]",B1ecYsqSuN,"['We use bilingual dictionaries for data augmentation for neural machine translation', 'This paper investigates using bilingual dictionaries to create synthetic sources for target-side monolingual data to improve over NMT models trained with small amounts of parallel data.']","['explore way incorporating bilingual dictionary enable semisupervised', 'neural machine translation ', 'conventional backtranslation method shown', 'success leveraging target side monolingual data ', 'however  since quality', 'backtranslation model tied size available parallel corpus ', 'could adversely impact synthetically generated sentence low resource', 'setting ', 'propose simple data augmentation technique address', 'shortcoming ', 'incorporate widely available bilingual dictionary yield', 'wordbyword translation generate synthetic sentence ', 'automatically', 'expands vocabulary model maintaining high quality content ', '', 'method show appreciable improvement performance strong baseline ']","We explore ways of incorporating bilingual dictionaries to enable semi-supervised
, neural machine translation., Conventional back-translation methods have shown
, success in leveraging target side monolingual data., However, since the quality of
, back-translation models is tied to the size of the available parallel corpora, this
, could adversely impact the synthetically generated sentences in a low resource
, setting., We propose a simple data augmentation technique to address both this
, shortcoming., We incorporate widely available bilingual dictionaries that yield
, word-by-word translations to generate synthetic sentences., This automatically
, expands the vocabulary of the model while maintaining high quality content., Our
, method shows an appreciable improvement in performance over strong baselines.",18,6.571428571428571,5.833333333333333
444,"[""Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity."", 'One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning.', 'In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus.', 'Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward.', 'We propose a new curiosity method which uses episodic memory to form the novelty bonus.', 'To determine the bonus, the current observation is compared with the observations in memory.', 'Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics.', 'This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences.', 'We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo.', 'In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM.', 'In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.', 'The code is available at https://github.com/google-research/episodic-curiosity/.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1395348757505417, 0.1304347813129425, 0.0952380895614624, 0.08695651590824127, 0.44999998807907104, 0.10810810327529907, 0.22641508281230927, 0.4814814627170563, 0.10526315122842789, 0.14999999105930328, 0.1463414579629898, 0.0]",SkeK3s0qKQ,"['We propose a novel model of curiosity based on episodic memory and the ideas of reachability which allows us to overcome the known ""couch-potato"" issues of prior work.', 'Proposes to give exploration bonuses in RL algorithms by giving larger bonuses to observations that are father away in environment steps.', 'The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems and considers many experiments on complex 3D environments']","['reward sparse real world today reinforcement learning algorithm struggle sparsity ', 'one solution problem allow agent create reward  thus making reward dense suitable learning ', 'particular  inspired curious behaviour animal  observing something novel could rewarded bonus ', 'bonus summed real task reward  making possible rl algorithm learn combined reward ', 'propose new curiosity method us episodic memory form novelty bonus ', 'determine bonus  current observation compared observation memory ', 'crucially  comparison done based many environment step take reach current observation memory  incorporates rich information environment dynamic ', 'allows u overcome known  couchpotato  issue prior work  agent find way instantly gratify exploiting action lead hardly predictable consequence ', 'test approach visually rich 3d environment vizdoom  dmlab mujoco ', 'navigational task vizdoom dmlab  agent outperforms stateoftheart curiosity method icm ', 'mujoco  ant equipped curiosity module learns locomotion firstpersonview curiosity ', 'code available http  githubcomgoogleresearchepisodiccuriosity ']","Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity., One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning., In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus., Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward., We propose a new curiosity method which uses episodic memory to form the novelty bonus., To determine the bonus, the current observation is compared with the observations in memory., Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics., This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences., We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo., In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM., In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only., The code is available at https://github.com/google-research/episodic-curiosity/.",19,5.4734513274336285,11.894736842105264
445,"[""We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task."", ""We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''."", 'Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.']","[1, 0, 0]","[1.0, 0.30188679695129395, 0.23728813230991364]",SkZxCk-0Z,"[""We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task."", 'The paper proposes a new model to use deep models for detecting logical entailment as a product of continuous functions over possible worlds.', 'Proposes a new model designed for machine learning with predicting logical entailment.']","['introduce new dataset logical entailment purpose measuring model  ability capture exploit structure logical expression entailment prediction task ', 'use task compare series architecture ubiquitous sequenceprocessing literature  addition new model class  possibleworldnets  computes entailment  convolution possible world  ', 'result show convolutional network present wrong inductive bias class problem relative lstm rnns  treestructured neural network outperform lstm rnns due enhanced ability exploit syntax logic  possibleworldnets outperform benchmark ']","We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task., We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''., Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.",6,6.029411764705882,17.0
446,"['Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  ', 'However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.', 'Also, previously seen training samples may not be available at the time of retraining.', 'We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.', 'Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.', 'An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.', 'We evaluated the proposed scheme on several recognition applications.', 'The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.27586206793785095, 0.0, 0.0, 0.17142856121063232, 0.2142857164144516, 0.1860465109348297, 0.0, 0.21052631735801697]",Hy-lXyDWG,"['The paper is about a new energy-efficient methodology for Incremental learning', 'Proposes procedure for incremental learning as transfer learning.', 'The paper presents a method to train deep convolutional neural networks in an incremental fashion, in which data are available in small batches over a period of time.', 'Presents an approach to class-incremental learning using deep networks by proposing three different learning strategies in the final/best approach.']","['deep convolutional neural network  dcnn  based supervised learning widely practiced approach largescale image classification ', 'however  retraining large network accommodate new  previously unseen data demand high computational time energy requirement ', 'also  previously seen training sample may available time retraining ', 'propose efficient training methodology incrementally growing dcnn allow new class learned sharing part base network ', 'proposed methodology inspired transfer learning technique  although forget previously learned class ', 'updated network learning new set class formed using previously learned convolutional layer  shared initial part base network  addition newly added convolutional kernel included later layer network ', 'evaluated proposed scheme several recognition application ', 'classification accuracy achieved approach comparable regular incremental learning approach  network updated new training sample  without network sharing  ']","Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  , However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements., Also, previously seen training samples may not be available at the time of retraining., We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network., Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes., An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network., We evaluated the proposed scheme on several recognition applications., The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).",13,5.946107784431137,12.846153846153847
447,"['Recurrent neural networks (RNNs) are widely used to model sequential data but\n', 'their non-linear dependencies between sequence elements prevent parallelizing\n', 'training over sequence length.', 'We show the training of RNNs with only linear\n', 'sequential dependencies can be parallelized over the sequence length using the\n', 'parallel scan algorithm, leading to rapid training on long sequences even with\n', 'small minibatch size.', 'We develop a parallel linear recurrence CUDA kernel and\n', 'show that it can be applied to immediately speed up training and inference of\n', 'several state of the art RNN architectures by up to 9x.', ' We abstract recent work\n', 'on linear RNNs into a new framework of linear surrogate RNNs and develop a\n', 'linear surrogate model for the long short-term memory unit, the GILR-LSTM, that\n', 'utilizes parallel linear recurrence.', ' We extend sequence learning to new\n', 'extremely long sequence regimes that were previously out of reach by\n', 'successfully training a GILR-LSTM on a synthetic sequence classification task\n', 'with a one million timestep dependency.\n']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2142857164144516, 0.0, 0.09999999403953552, 0.07999999821186066, 0.07692307233810425, 0.2857142686843872, 0.0, 0.1599999964237213, 0.06666666269302368, 0.07407406717538834, 0.0, 0.14814814925193787, 0.14814814925193787, 0.20000000298023224, 0.09090908616781235, 0.0, 0.07999999821186066, 0.17391303181648254]",HyUNwulC-,"['use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency', 'Proposes accelerating RNN by applying the method from Blelloch.', 'The authors propose a parallel algorithm for Linear Surrogate RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM.']","['recurrent neural network  rnns  widely used model sequential data', 'nonlinear dependency sequence element prevent parallelizing', 'training sequence length ', 'show training rnns linear', 'sequential dependency parallelized sequence length using', 'parallel scan algorithm  leading rapid training long sequence even', 'small minibatch size ', 'develop parallel linear recurrence cuda kernel', 'show applied immediately speed training inference', 'several state art rnn architecture 9x ', 'abstract recent work', 'linear rnns new framework linear surrogate rnns develop', 'linear surrogate model long shortterm memory unit  gilrlstm ', 'utilizes parallel linear recurrence ', 'extend sequence learning new', 'extremely long sequence regime previously reach', 'successfully training gilrlstm synthetic sequence classification task', 'one million timestep dependency ']","Recurrent neural networks (RNNs) are widely used to model sequential data but
, their non-linear dependencies between sequence elements prevent parallelizing
, training over sequence length., We show the training of RNNs with only linear
, sequential dependencies can be parallelized over the sequence length using the
, parallel scan algorithm, leading to rapid training on long sequences even with
, small minibatch size., We develop a parallel linear recurrence CUDA kernel and
, show that it can be applied to immediately speed up training and inference of
, several state of the art RNN architectures by up to 9x.,  We abstract recent work
, on linear RNNs into a new framework of linear surrogate RNNs and develop a
, linear surrogate model for the long short-term memory unit, the GILR-LSTM, that
, utilizes parallel linear recurrence.,  We extend sequence learning to new
, extremely long sequence regimes that were previously out of reach by
, successfully training a GILR-LSTM on a synthetic sequence classification task
, with a one million timestep dependency.
",21,5.4875,7.619047619047619
448,"['Neural text generation models are often autoregressive language models or seq2seq models.', 'Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks.', 'These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality.', 'Language models are typically trained via maximum likelihood and most often with teacher forcing.', 'Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time.', 'We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation.', 'GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them.', 'We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.', 'We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.1111111044883728, 0.1111111044883728, 0.0, 0.0, 0.052631575614213943, 0.10810810327529907, 0.1666666567325592, 0.25, 0.0]",ByOExmWAb,"['Natural language GAN for filling in the blank', 'This paper proposes to generate text using GANs.', 'Generating text samples using GAN and a mechanism to fill in missing words conditional on the surrounding text']","['neural text generation model often autoregressive language model seq2seq model ', 'neural autoregressive seq2seq model generate text sampling word sequentially  word conditioned previous model  stateoftheart several machine translation summarization benchmark ', 'benchmark often defined validation perplexity even though direct measure sample quality ', 'language model typically trained via maximum likelihood often teacher forcing ', 'teacher forcing wellsuited optimizing perplexity result poor sample quality generating text requires conditioning sequence word never observed training time ', 'propose improve sample quality using generative adversarial network  gans   explicitly train generator produce high quality sample shown lot success image generation ', 'gans originally designed output differentiable value  discrete language generation challenging ', 'introduce actorcritic conditional gan fill missing text conditioned surrounding context ', 'show qualitatively quantitatively  evidence produce realistic text sample compared maximum likelihood trained model ']","Neural text generation models are often autoregressive language models or seq2seq models., Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks., These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality., Language models are typically trained via maximum likelihood and most often with teacher forcing., Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time., We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation., GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them., We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context., We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.",14,6.01063829787234,13.428571428571429
449,"['Parametric texture models have been applied successfully to synthesize artificial images.', 'Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.', 'In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.', 'For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.', 'Here, we implemented a human-vision-inspired novelty detection approach.', 'Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.', 'Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.', 'Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.', 'Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.07692307233810425, 0.060606054961681366, 0.19512194395065308, 0.17142856121063232, 0.260869562625885, 0.3478260934352875, 0.3243243098258972, 0.25, 0.2857142686843872]",BJEOOsCqKm,"['Comparison of psychophysical and CNN-encoded  texture representations in a one-class neural network novelty detection application.', 'This paper focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task', 'This paper considers detecting anomalies in textures and proposes original loss function.', 'Proposes training two anomaly detectors from three different models to detect perceptual anomalies in visual textures.']","['parametric texture model applied successfully synthesize artificial image ', 'psychophysical study show defined condition observer unable differentiate modelgenerated original natural texture ', 'industrial application reverse case interest  texture analysis system decide human observer able discriminate reference novel texture ', 'example  case inspecting decorative surface de tection visible texture anomaly without prior knowledge required ', ' implemented humanvisioninspired novelty detection approach ', 'assuming feature used texture synthesis important human texture percep tion  compare psychophysical well learnt texture representation based activation pretrained cnn novelty detection scenario ', 'additionally  introduce novel objective function train oneclass neural network novelty detection compare result standard oneclass svm approach ', 'experiment clearly show difference humanvisioninspired texture representation learnt feature detecting visual anomaly ', 'based dig ital print inspection scenario show psychophysical texture representation able outperform cnnencoded feature ']","Parametric texture models have been applied successfully to synthesize artificial images., Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures., In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture., For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required., Here, we implemented a human-vision-inspired novelty detection approach., Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario., Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches., Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies., Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.",13,6.309782608695652,14.153846153846153
450,"['In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.', 'To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.', 'We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.', 'Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.']","[0, 1, 0, 0]","[0.13333332538604736, 0.3928571343421936, 0.17777776718139648, 0.0952380895614624]",r1kjEuHpZ,"['We propose a new type of regularization approach that encourages non-overlapness in representation learning, for the sake of improving interpretability and reducing overfitting.', 'The paper introduces a matrix regularizer to simultaneously induce both sparsity and approximate orthogonality.', 'The paper studies a regularization method to promote sparsity and reduce the overlap among the supports of the weight vectors in the learned representations to enhance interpretability and avoid overfitting', 'The paper proposed a new regularization approach that simultaneously encourages the weight vectors (W) to be sparse and orthogonal to each other.']","['representation learning  rl   make learned representation easy interpret le overfitted training data two important challenging issue ', 'address problem  study new type regularization approach encourages support weight vector rl model small overlap  simultaneously promoting nearorthogonality among vector sparsity vector ', 'apply proposed regularizer two model  neural network  nns  sparse coding  sc   develop efficient admmbased algorithm regularized sc ', 'experiment various datasets demonstrate weight vector learned regularizer interpretable better generalization performance ']","In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues., To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector., We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC., Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.",8,5.933962264150943,13.25
451,"['Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  ', 'Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\n', 'We prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data.', 'We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\n', 'This result leads to a new way of initializing gate biases in LSTMs and GRUs.', 'Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n']","[0, 0, 0, 0, 1, 0]","[0.1538461446762085, 0.09302324801683426, 0.25641024112701416, 0.12121211737394333, 0.2857142686843872, 0.14999999105930328]",SJcKhk-Ab,"['Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.', 'Paper links recurrent network deisgn and its effect on how the network reacts to time transformations, and uses this to develop a simple bias initialization scheme.']","['successful recurrent model long shortterm memory  lstms  gated recurrent unit  grus  use emph  ad hoc  gating mechanism ', 'empirically model found improve learning medium long term temporal dependency help vanishing gradient issue ', 'prove learnable gate recurrent model formally provide emph  quasiinvariance general time transformation  input data ', 'recover part lstm architecture simple axiomatic approach ', 'result lead new way initializing gate bias lstms grus ', 'experimentally  new emph  chrono initialization  shown greatly improve learning long term dependency  minimal implementation effort ']","Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \emph{ad hoc} gating mechanisms.  , Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.
	
, We prove that learnable gates in a recurrent model formally provide \emph{quasi-invariance to general time transformations} in the input data., We recover part of the LSTM architecture from a simple axiomatic approach.
	
, This result leads to a new way of initializing gate biases in LSTMs and GRUs., Experimentally, this new \emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.

",8,5.798165137614679,13.625
452,"['Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.', 'A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.', 'In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}.', 'In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.', ""We call this approach ``learning to teach''."", 'In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).', 'The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.', 'To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.11764705181121826, 0.25, 0.2631579041481018, 0.17391303181648254, 0.1666666567325592, 0.26923075318336487, 0.1463414579629898, 0.1538461446762085]",HJewuJWCZ,"['We propose and verify the effectiveness of learning to teach, a new framework to automatically guide machine learning process.', 'This paper focuses on ""machine teaching"" and proposes leveraging reinforcement learning by defining the reward as how fast the learner learns and using policy gradient to update the teacher parameters', 'The authors define a deep learning model composed of four components: a student model, a teacher model, a loss function, and a data set. ', 'Suggests a ""learning to teach"" framework, corresponding to choices over the data presented to the learner.']","['teaching play important role society  spreading human knowledge educating next generation ', 'good teacher select appropriate teaching material  impact suitable methodology  set targeted examination  according learning behavior student ', 'field artificial intelligence  however  one fully explored role teaching  pay attention machine emph  learning  ', 'paper  argue equal attention   paid teaching  furthermore  optimization framework  instead heuristic  used obtain good teaching strategy ', 'call approach  learning teach  ', 'approach  two intelligent agent interact  student model  corresponds learner traditional machine learning algorithm   teacher model  determines appropriate data  loss function  hypothesis space facilitate training student model  ', 'teacher model leverage feedback student model optimize teaching strategy mean reinforcement learning  achieve teacherstudent coevolution ', 'demonstrate practical value proposed approach  take training deep neural network  dnn  example  show using learning teach technique  able use much le training data fewer iteration achieve almost accuracy different kind dnn model  eg  multilayer perceptron  convolutional neural network recurrent neural network  various machine learning task  eg  image classification text understanding  ']","Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations., A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students., In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}., In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies., We call this approach ``learning to teach''., In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model)., The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution., To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",31,5.615079365079365,8.129032258064516
453,"['We present DL2, a system for training and querying neural networks with logical constraints.', 'The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints.', 'We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.']","[1, 0, 0]","[0.5600000023841858, 0.27272728085517883, 0.25806450843811035]",H1faSn0qY7,"['A differentiable loss for logic constraints for training and querying neural networks.', 'A framework for turning queries over parameters and input, ouput pairs to neural networks into differentiable loss functions and an associated declarative language for specifying these queries', 'This paper tackles the problem of combining logical approaches with neural networks by translating a logical formula into a non-negative loss function for a neural network.']","['present dl2  system training querying neural network logical constraint ', 'key idea translate constraint differentiable loss desirable mathematical property either train loss iterative manner use loss querying network input subject constraint ', 'empirically demonstrate dl2 effective training querying scenario  across range constraint data set ']","We present DL2, a system for training and querying neural networks with logical constraints., The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints., We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.",5,5.217948717948718,15.6
454,"['Genetic algorithms have been widely used in many practical optimization problems.\n', 'Inspired by natural selection, operators, including mutation, crossover\n', 'and selection, provide effective heuristics for search and black-box optimization.\n', 'However, they have not been shown useful for deep reinforcement learning, possibly\n', 'due to the catastrophic consequence of parameter crossovers of neural networks.\n', 'Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm\n', 'for sample-efficient deep policy optimization.', 'GPO uses imitation learning\n', 'for policy crossover in the state space and applies policy gradient methods for mutation.\n', 'Our experiments on MuJoCo tasks show that GPO as a genetic algorithm\n', 'is able to provide superior performance over the state-of-the-art policy gradient\n', 'methods and achieves comparable or higher sample efficiency.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.1818181723356247, 0.0, 0.09999999403953552, 0.1818181723356247, 0.0952380895614624, 0.0952380895614624, 0.2666666507720947, 0.0, 0.08695651590824127, 0.0, 0.0, 0.0]",ByOnmlWC-,"['Genetic algorithms based approach for optimizing deep neural network policies', 'The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together.', 'This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.']","['genetic algorithm widely used many practical optimization problem ', 'inspired natural selection  operator  including mutation  crossover', 'selection  provide effective heuristic search blackbox optimization ', 'however  shown useful deep reinforcement learning  possibly', 'due catastrophic consequence parameter crossover neural network ', ' present genetic policy optimization  gpo   new genetic algorithm', 'sampleefficient deep policy optimization ', 'gpo us imitation learning', 'policy crossover state space applies policy gradient method mutation ', 'experiment mujoco task show gpo genetic algorithm', 'able provide superior performance stateoftheart policy gradient', 'method achieves comparable higher sample efficiency ']","Genetic algorithms have been widely used in many practical optimization problems.
, Inspired by natural selection, operators, including mutation, crossover
, and selection, provide effective heuristics for search and black-box optimization.
, However, they have not been shown useful for deep reinforcement learning, possibly
, due to the catastrophic consequence of parameter crossovers of neural networks.
, Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm
, for sample-efficient deep policy optimization., GPO uses imitation learning
, for policy crossover in the state space and applies policy gradient methods for mutation.
, Our experiments on MuJoCo tasks show that GPO as a genetic algorithm
, is able to provide superior performance over the state-of-the-art policy gradient
, methods and achieves comparable or higher sample efficiency.",20,6.222222222222222,5.85
455,"['To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights.', 'One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping.', 'Despite its empirical success, little is understood about why the straight-through gradient method works.\n', 'Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterovs dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method.', 'ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness.', 'For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization.', 'We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.']","[0, 1, 0, 0, 0, 0, 0]","[0.04878048226237297, 0.277777761220932, 0.1818181723356247, 0.1538461446762085, 0.1538461446762085, 0.1764705777168274, 0.1249999925494194]",HyzMyhCcK7,"['A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.', 'Proposes ProxQuant method to train neural networks with quantized weights.', 'Proposes solving binary nets and its variants using proximal gradient descent.']","['make deep neural network feasible resourceconstrained environment  mobile device   beneficial quantize model using lowprecision weight ', 'one common technique quantizing neural network straightthrough gradient method  enables backpropagation quantization mapping ', 'despite empirical success  little understood straightthrough gradient method work ', 'building upon novel observation straightthrough gradient method fact identical wellknown nesterov  dualaveraging algorithm quantization constrained optimization problem  propose principled alternative approach  called proxquant  formulates quantized network training regularized learning problem instead optimizes via proxgradient method ', 'proxquant backpropagation underlying fullprecision vector applies efficient proxoperator stochastic gradient step encourage quantizedness ', 'quantizing resnets lstms  proxquant outperforms stateoftheart result binary quantization par stateoftheart multibit quantization ', 'perform theoretical analysis showing proxquant converges stationary point mild smoothness assumption  whereas variant lazy proxgradient method fail converge setting ']","To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights., One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping., Despite its empirical success, little is understood about why the straight-through gradient method works.
, Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterovs dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method., ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness., For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization., We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.",15,6.65934065934066,12.133333333333333
456,"['Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions.', 'However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., near linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\n', 'Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss.', 'We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units.', 'We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.']","[1, 0, 0, 0, 0]","[0.25641024112701416, 0.09677419066429138, 0.11428570747375488, 0.23728813230991364, 0.052631575614213943]",Hkfmn5n6W,"['""Bad"" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before', 'In networks with a single hidden layer, the volume of suboptimal local minima exponentially decreases in comparison to global minima.', ""This paper aims to answer why standard SGD based algorithms on neural network converge to 'good' solutions.""]","['background  statistical mechanic result  dauphin et al   2014   choromanska et al   2015   suggest local minimum high error exponentially rare high dimension ', 'however  prove low error guarantee multilayer neural network  mnns   previous work far required either heavily modified mnn model training method  strong assumption label  eg   near  linear separability   unrealistically wide hidden layer omega  n  unit ', 'result  examine mnn one hidden layer piecewise linear unit  single output  quadratic loss ', 'prove  high probability limit nrightarrowinfty datapoints  volume differentiable region empiric loss containing suboptimal differentiable local minimum exponentially vanishing comparison volume global minimum  given standard normal input dimension d0tilde  omega   sqrt  n    realistic number d1tilde  omega   nd0  hidden unit ', 'demonstrate result numerically  example  0  binary classification training error cifar nd0  16 hidden neuron ']","Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions., However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., near linear separability), or an unrealistically wide hidden layer with \Omega\(N) units. 

, Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss., We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\tilde{\Omega}(\sqrt{N}), and a more realistic number of d_1=\tilde{\Omega}(N/d_0) hidden units., We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.",17,5.932515337423313,8.578947368421053
457,"['Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations.', 'An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications.', 'Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples.', 'In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.', 'It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models.', 'Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples.', 'Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method.', 'Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.19512194395065308, 0.1428571343421936, 0.2800000011920929, 0.1818181723356247, 0.1395348757505417, 0.5789473652839661, 0.0555555522441864, 0.25]",BJzVUj0qtQ,"['We propose an attention-invariant attack method to generate more transferable adversarial examples for black-box attacks, which can fool state-of-the-art defenses with a high success rate.', 'The paper proposes a new way of overcoming state of the art defences against adversarial attacks on CNN.', 'This paper suggests that ""attention shift"" is a key property behind failure of adversarial attacks to transfer and propose an attention-invariant attack method']","['deep neural network vulnerable adversarial example  mislead classifier adding imperceptible perturbation ', 'intriguing property adversarial example good transferability  making blackbox attack feasible realworld application ', 'due threat adversarial attack  many method proposed improve robustness  several stateoftheart defense shown robust transferable adversarial example ', 'paper  identify attention shift phenomenon  may hinder transferability adversarial example defense model ', 'indicates defense rely different discriminative region make prediction compared normally trained model ', 'therefore  propose attentioninvariant attack method generate transferable adversarial example ', 'extensive experiment imagenet dataset validate effectiveness proposed method ', 'best attack fool eight stateoftheart defense 82  success rate average based transferability  demonstrating insecurity defense technique ']","Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations., An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications., Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples., In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models., It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models., Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples., Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method., Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.",16,6.372549019607843,9.5625
458,"['We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\\log{K})$ memory while only requiring $O(K\\log{K} + d\\log{K})$ operation for inference.', 'MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\\log{K})$ memory without any assumption on the relationship between classes.', 'MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.', 'We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.', 'With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\\%, which is the best-reported accuracy on this dataset.', 'Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\\% accuracy.  ', 'In contrast, MACH can achieve 9\\% accuracy with 480x reduction in the model size (of mere 0.6GB).', 'With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.04651162400841713, 0.12903225421905518, 0.2142857164144516, 0.0, 0.307692289352417, 0.05882352590560913, 0.0, 0.2666666507720947]",r1RQdCg0W,"['How to Training 100,000 classes on a single GPU', 'Proposes an efficient hashing method MACH for softmax approximation in the context of large output space, which saves both memory and computation.', 'A method for classification scheme for problems involving a large number of classes in a multi-class setting demonstrated on ODP and Imagenet-21K datasets', 'The paper presents a hashing based scheme for reducing memory and computation time for K-way classification when K is large']","['present mergedaveraged classifier via hashing  mach   k  classification large  k   compared traditional onevsall classifier require   kd   memory inference cost  mach need   dlog  k    memory requiring   klog  k   dlog  k    operation inference ', 'mach first generic  k  classification algorithm  provably theoretical guarantee  requires   log  k    memory without assumption relationship class ', 'mach us universal hashing reduce classification large number class independent classification task small  constant  number class ', 'provide theoretical quantification accuracymemory tradeoff showing first connection extreme classification heavy hitter ', 'mach train odp dataset 100000 class 400000 feature single titan x gpu  12gb   classification accuracy 1928   bestreported accuracy dataset ', 'work  best performing baseline onevsall classifier requires 40 billion parameter  320 gb model size  achieves 9  accuracy ', 'contrast  mach achieve 9  accuracy 480x reduction model size  mere 06gb  ', 'mach  also demonstrate complete training finegrained imagenet dataset  compressed size 104gb   21000 class  single gpu ']","We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\log{K})$ memory while only requiring $O(K\log{K} + d\log{K})$ operation for inference., MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\log{K})$ memory without any assumption on the relationship between classes., MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes., We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters., With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\%, which is the best-reported accuracy on this dataset., Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\% accuracy.  , In contrast, MACH can achieve 9\% accuracy with 480x reduction in the model size (of mere 0.6GB)., With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.",18,6.01,10.526315789473685
459,"['Gradient-based optimization is the foundation of deep learning and reinforcement learning.\n', 'Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy.', 'We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\n', 'These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings.', 'We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic.', 'We also demonstrate this framework for training discrete latent-variable models.']","[0, 0, 1, 0, 0, 0]","[0.2857142686843872, 0.0, 0.52173912525177, 0.09302324801683426, 0.21052631735801697, 0.23529411852359772]",SyzKd1bCW,"['We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ', 'Suggests a new approach to performing gradient descent for blackbox optimization or training discrete latent variable models.']","['gradientbased optimization foundation deep learning reinforcement learning ', 'even mechanism optimized unknown differentiable  optimization using highvariance biased gradient estimate still often best strategy ', 'introduce general framework learning lowvariance  unbiased gradient estimator blackbox function random variable  based gradient learned function ', 'estimator jointly trained model parameter policy  applicable discrete continuous setting ', 'give unbiased  adaptive analog stateoftheart reinforcement learning method advantage actorcritic ', 'also demonstrate framework training discrete latentvariable model ']","Gradient-based optimization is the foundation of deep learning and reinforcement learning.
, Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy., We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.
, These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings., We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic., We also demonstrate this framework for training discrete latent-variable models.",11,6.446601941747573,9.363636363636363
460,"['Do GANS (Generative Adversarial Nets) actually learn the target distribution?', 'The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.', 'A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.', 'It showed that the training objective can approach its optimum value even if the generated distribution has very low support.', 'In other words, the training objective is unable to prevent mode collapse.', 'The current paper makes two contributions.', '(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.', 'Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  ', '(2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.', 'Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.', 'More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.04878048226237297, 0.1111111044883728, 0.07843136787414551, 0.1599999964237213, 0.1395348757505417, 0.0, 0.2083333283662796, 0.2083333283662796, 0.20338982343673706, 0.2641509473323822, 0.08888888359069824]",BJehNfW0-,"[""We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well."", 'The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally. ', 'This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated sample, with experiment results interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.', 'The paper uses birthday paradox to show that some GAN architectures generate distributions with fairly low support.']","['gans  generative adversarial net  actually learn target distribution ', 'foundational paper goodfellow et al   2014  suggested  given sufficiently large deep net  sample size  computation time ', 'recent theoretical analysis arora et al   2017  raised doubt whether hold discriminator bounded size ', 'showed training objective approach optimum value even generated distribution low support ', 'word  training objective unable prevent mode collapse ', 'current paper make two contribution ', ' 1  proposes novel test estimating support size using birthday paradox discrete probability ', 'using evidence presented wellknown gans approach learn distribution fairly low support ', ' 2  theoretically study encoderdecoder gans architecture  eg  biganali   proposed learn meaningful feature via gans  consequently also solve modecollapse issue ', 'result show encoderdecoder training objective also guarantee learning full distribution prevent serious mode collapse ', 'seriously  prevent learning meaningless code data  contrary usual intuition ']","Do GANS (Generative Adversarial Nets) actually learn the target distribution?, The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time., A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size., It showed that the training objective can approach its optimum value even if the generated distribution has very low support., In other words, the training objective is unable to prevent mode collapse., The current paper makes two contributions., (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability., Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  , (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue., Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse., More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.",20,5.8201058201058204,8.590909090909092
461,"['Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed.', 'Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail.', 'However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language.', 'In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks.', 'We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation.', 'We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.']","[0, 0, 0, 0, 1, 0]","[0.1249999925494194, 0.10526315122842789, 0.09302324801683426, 0.37288135290145874, 0.38461539149284363, 0.35555556416511536]",H1BLjgZCb,"['We propose a framework to generate natural adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.', 'Suggests a method for creation of semantical adversary examples.', 'Proposes a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation ']","['due complex nature  hard characterize way machine learning model misbehave exploited deployed ', 'recent work adversarial example  ie  input minor perturbation result substantially different model prediction  helpful evaluating robustness model exposing adversarial scenario fail ', 'however  malicious perturbation often unnatural  semantically meaningful  applicable complicated domain language ', 'paper  propose framework generate natural legible adversarial example lie data manifold  searching semantic space dense continuous data representation  utilizing recent advance generative adversarial network ', 'present generated adversary demonstrate potential proposed approach blackbox classifier wide range application image classification  textual entailment  machine translation ', 'include experiment show generated adversary natural  legible human  useful evaluating analyzing blackbox classifier ']","Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed., Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail., However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language., In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks., We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation., We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.",19,5.9226190476190474,8.4
462,"['Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  ', 'It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.', 'The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.', 'In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.', 'This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.', 'We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.']","[0, 0, 0, 1, 0, 0]","[0.12765957415103912, 0.20512820780277252, 0.19512194395065308, 0.4375, 0.13333332538604736, 0.09999999403953552]",HyMTkQZAb,"['We extend the K-FAC method to RNNs by developing a new family of Fisher approximations.', 'The authors extends the K-FAC method to RNNs and presents 3 ways of approximating F, showing optimization results on 3 datasets, which outperforms ADAM in both number of updates and computation time.', 'Proposes to extend the Kronecker-factor Appropriate Curvature optimization method to the setting of recurrent neural networks.', 'The authors present a second-order method that is specifically designed for RNNs']","['kroneckerfactor approximate curvature  marten  grosse  2015   kfac  2ndorder optimization method shown give stateoftheart performance largescale neural network optimization task  ba et al  2017  ', 'based approximation fisher information matrix  fim  make assumption particular structure network way parameterized ', 'original kfac method applicable fullyconnected network  although recently extended grosse  marten  2016  handle convolutional network well ', 'work extend method handle rnns introducing novel approximation fim rnns ', 'approximation work modelling covariance structure gradient contribution different timesteps using chainstructured linear gaussian graphical model  summing various crosscovariances  computing inverse closed form ', 'demonstrate experiment method significantly outperforms general purpose stateoftheart optimizers like sgd momentum adam several challenging rnn training task ']","Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  , It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized., The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well., In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs., This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form., We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.",11,5.969512195121951,14.909090909090908
463,"['Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.', 'In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.', 'We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.', 'In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.\n']","[0, 1, 0, 0]","[0.1818181723356247, 0.37037035822868347, 0.19512194395065308, 0.2083333283662796]",ByGuynAct7,"['The generative model for kernels of convolutional neural networks, that acts as a prior distribution while training on new datasets.', 'A method for modeling convolutional neural networks using a Bayes method.', ""Proposes the 'deep weight prior': the idea is to elicit a prior on an auxilary dataset and then use that prior over the CNN filters to jump start inference for a data set of interest."", 'This paper explores learning informative priors for convolutional neural network models with similar problem domains by using autoencoders to obtain an expressive prior on the filtered weights of the trained networks.']","['bayesian inference known provide general framework incorporating prior knowledge specific property machine learning model via carefully choosing prior distribution ', 'work  propose new type prior distribution convolutional neural network  deep weight prior  dwp   exploit generative model encourage specific structure trained convolutional filter eg  spatial correlation weight ', 'define dwp form implicit distribution propose method variational inference type implicit prior ', 'experiment  show dwp improves performance bayesian neural network training data limited  initialization weight sample dwp accelerates training conventional convolutional neural network ']","Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution., In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights., We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors., In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.
",10,5.840336134453781,11.9
464,"['The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  ', 'Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.', 'PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  ', 'We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.', 'Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.']","[0, 0, 0, 1, 0]","[0.13793103396892548, 0.0, 0.0, 0.375, 0.1538461446762085]",HkanP0lRW,"['We applied deep learning techniques to hyperspectral image segmentation and iterative feature sampling.', 'Proposes a greedy scheme to select a subset of highly correlated spectral features in a classification task.', 'The paper explores the use of neural networks for classification and segmentation of hypersepctral imaging (HSI) of cells.', 'Classifying cells and implementing cell segmentation based on deep learning techniques with reduction of input features']","['high dimensionality hyperspectral imaging force unique challenge scope  size processing requirement ', 'motivated potential inthefield cell sorting detector  examine synechocystis sp ', 'pcc 6803 dataset wherein cell grown alternatively nitrogen rich deplete culture ', 'use deep learning technique successfully classify cell generate mask segmenting cellscondition background ', ' use classification accuracy guide datadriven  iterative feature selection method  allowing design neural network requiring 90  fewer input feature little accuracy degradation ']","The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  , Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp., PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  , We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background., Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.",10,6.130434782608695,9.2
465,"['Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \\textit{etc}.', 'Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \\{plot, question, answer\\}.', 'Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited', '(i) $x-y$ axes variables', '(ii) question templates and', '(iii) answer vocabulary and hence do not adequately capture the challenges posed by this task.', 'To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators.', 'First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \\textit{etc.', '} extracted from  real word data sources such as World Bank, government sites, \\textit{etc}.', 'Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots.', 'Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary.', 'As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task.', 'We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to', '(i) extract relevant data from the plot and convert it to a semi-structured table', '(ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived.', 'We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.09090908616781235, 0.04651162400841713, 0.17142856121063232, 0.0, 0.09999999403953552, 0.12903225421905518, 0.2380952388048172, 0.21621620655059814, 0.06666666269302368, 0.1860465109348297, 0.20512819290161133, 0.10526315122842789, 0.2631579041481018, 0.2666666507720947, 0.14999999105930328, 0.12765957415103912]",SygeznA9YX,"['We created a new dataset for data interpretation over plots and also propose a baseline for the same.', 'The authors propose a pipeline to solve the DIP problem involving learning from datasets containing triplets of the form {plot, question, answer}', 'Proposes an algorithm that can interpret data shown in scientific plots.']","['data interpretation important part quantitative aptitude exam requires individual answer question grounded plot bar chart  line graph  scatter plot  textit  etc  ', 'recently  increasing interest building model perform task learning datasets containing triplet form   plot  question  answer  ', 'two datasets proposed recent past contain plot generated synthetic data limited', '   xy  ax variable', ' ii  question template', ' iii  answer vocabulary hence adequately capture challenge posed task ', 'overcome limitation existing datasets  introduce new dataset containing  97  million questionanswer pair grounded  270000  plot three main differentiator ', 'first  plot dataset contain wide variety realistic  x     variable co2 emission  fertility rate  textit  etc ', ' extracted real word data source world bank  government site  textit  etc  ', 'second  question dataset complex based template extracted interesting question asked crowd worker using fraction plot ', 'lastly  answer dataset restricted small vocabulary large fraction answer seen test time present training vocabulary ', 'result  existing model visual question answering largely use endtoend model multiclass classification framework used task ', 'establish initial result dataset emphasize complexity task using multistaged modular pipeline various subcomponents', '  extract relevant data plot convert semistructured table', ' ii  combine question table use compositional semantic parsing arrive logical form answer derived ', 'believe modular framework best way go forward would enable research community independently make progress subtasks involved plot question answering ']","Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \textit{etc}., Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \{plot, question, answer\}., Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited, (i) $x-y$ axes variables, (ii) question templates and, (iii) answer vocabulary and hence do not adequately capture the challenges posed by this task., To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators., First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \textit{etc., } extracted from  real word data sources such as World Bank, government sites, \textit{etc}., Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots., Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary., As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task., We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to, (i) extract relevant data from the plot and convert it to a semi-structured table, (ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived., We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.",31,5.334310850439882,11.0
466,"['Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing.', 'Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model.', 'PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule.', 'Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well.', 'Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train.', 'Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation.', 'Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting.', 'In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs.', 'In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.']","[0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.25641024112701416, 0.0, 0.05714285373687744, 0.0, 0.1621621549129486, 0.0, 0.052631575614213943, 0.0]",HJJ23bW0b,"['Improving Predictive State Recurrent Neural Networks via Orthogonal Random Features', 'Proposes improving the performances of Predicitve State Recurrent Neural Networks by considering Orthogonal Random Features.', 'The paper tackles the problem of training predictive state recurrent neural networks and makes two contributions.']","['learning predict complex timeseries data fundamental challenge range discipline including machine learning  robotics  natural language processing ', 'predictive state recurrent neural network  psrnns   downey et al   stateoftheart approach modeling timeseries data combine benefit probabilistic filter recurrent neural network single model ', 'psrnns leverage concept hilbert space embeddings distribution  smola et al   embed predictive state reproducing kernel hilbert space  estimate  predict  update embedded state using kernel bayes rule ', 'practical implementation psrnns made possible machinery random feature  input feature mapped new space dot product approximate kernel well ', 'unfortunately psrnns often require large number rf obtain good result  resulting large model slow execute slow train ', 'orthogonal random feature  orfs   choromanski et al   improvement rf shown decrease number rf required pointwise kernel approximation ', 'unfortunately  clear orfs applied psrnns  psrnns rely kernel ridge regression core component learning algorithm  theoretical guarantee orf apply setting ', 'paper  extend theory orfs kernel ridge regression show orfs used obtain orthogonal psrnns  opsrnns   smaller faster psrnns ', 'particular  show opsrnn model clearly outperform lstms furthermore  achieve accuracy similar psrnns order magnitude smaller number feature needed ']","Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing., Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model., PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule., Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well., Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train., Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation., Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting., In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs., In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.",23,5.405904059040591,10.423076923076923
467,"['Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.', 'This creates a fundamental quality- versus-quantity trade-off in the learning process.', 'Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?', 'We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.', 'To this end, we propose fidelity-weighted learning (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.', 'FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).', 'Both student and teacher are learned from the data.', 'We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.1111111044883728, 0.07692307233810425, 0.13333332538604736, 0.10256409645080566, 0.6111111044883728, 0.043478257954120636, 0.0833333283662796, 0.15686273574829102]",B1X0mzZCW,"['We propose Fidelity-weighted Learning, a semi-supervised teacher-student approach for training neural networks using weakly-labeled data.', 'This paper suggests an approach for learning with weak supervision by using a clean and a noisy dataset and assuming a teacher and student networks', 'The paper attemps to train deep neural network models with few labelled training samples.', 'The authors propose an approach for training deep learning models for situations where there is not enough reliable annotated data.']","['training deep neural network requires many training sample  practice training label expensive obtain may varying quality  may trusted expert labelers others might heuristic source weak supervision crowdsourcing ', 'creates fundamental quality versusquantity tradeoff learning process ', 'learn small amount highquality data potentially large amount weaklylabeled data ', 'argue learner could somehow know take labelquality account learning data representation  could get best world ', 'end  propose  fidelityweighted learning   fwl   semisupervised student teacher approach training deep neural network using weaklylabeled data ', 'fwl modulates parameter update student network  trained task care  persample basis according posterior confidence labelquality estimated teacher  access highquality label  ', 'student teacher learned data ', 'evaluate fwl two task information retrieval natural language processing outperform stateoftheart alternative semisupervised method  indicating approach make better use strong weak label  lead better taskdependent data representation ']","Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing., This creates a fundamental quality- versus-quantity trade-off in the learning process., Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?, We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds., To this end, we propose fidelity-weighted learning (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data., FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels)., Both student and teacher are learned from the data., We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.",15,5.62085308056872,14.066666666666666
468,"[' Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving.', 'When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role.', 'The purpose of this paper is to propose new online learning approaches for supervised dimension reduction.', 'Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner.', 'The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in.', 'We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm.', 'We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.']","[0, 0, 0, 0, 0, 1, 0]","[0.1904761791229248, 0.2916666567325592, 0.3589743673801422, 0.4399999976158142, 0.3461538553237915, 0.44999998807907104, 0.1621621549129486]",B1MUroRct7,"['We proposed two new approaches,  the incremental sliced inverse regression and incremental overlapping sliced inverse regression, to implement supervised dimension reduction in an online learning manner.', 'Studies sufficient dimension reduction problem and proposes an incremental sliced inverse regression algorithm.', 'This paper proposes an online learning algorithm for supervised dimension reduction, called incremental sliced inverse regression']","['online learning attracted great attention due increasing demand system ability learning evolving ', 'data processed also high dimensional dimension reduction necessary visualization prediction enhancement  online dimension reduction play essential role ', 'purpose paper propose new online learning approach supervised dimension reduction ', 'first algorithm motivated adapting sliced inverse regression  sir   pioneer effective algorithm supervised dimension reduction  making implementable incremental manner ', 'new algorithm  called incremental sliced inverse regression  isir   able update subspace significant factor intrinsic lower dimensionality fast efficiently new observation come ', 'also refine algorithm using overlapping technique develop incremental overlapping sliced inverse regression  iosir  algorithm ', 'verify effectiveness efficiency algorithm simulation real data application ']"," Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving., When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role., The purpose of this paper is to propose new online learning approaches for supervised dimension reduction., Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner., The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in., We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm., We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.",12,5.930817610062893,13.25
469,"['This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes.', 'The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training.', 'This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy.', 'Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training.', 'We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks.', 'For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight.', 'With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model.', 'To achieve the similar classification error, the conventional binary neural network model requires 4 more data storage for weights than our proposed model.', 'For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6 storage savings to achieve the similar accuracy.\n']","[0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.1702127605676651, 0.1666666567325592, 0.13636362552642822, 0.0, 0.15789473056793213, 0.17391303181648254, 0.1304347813129425, 0.09999999403953552, 0.17777776718139648]",rkONG0xAW,"['We propose a learning model enabling DNN to learn with only 2 bit/weight, which is especially useful for on-device learning', 'Proposes a method to discretisize a NN incrementally to improve memory and performance.']","['paper present storageefficient learning model titled recursive binary neural network embedded mobile device limited amount onchip data storage hundred kilobyte ', 'main idea proposed model recursively recycle data storage weight  parameter  training ', 'enables device given storage constraint train instantiate neural network classifier larger number weight chip  achieving better classification accuracy ', 'efficient use onchip storage reduces offchip storage access  improving energyefficiency speed training ', 'verified proposed training model deep convolutional neural network classifier mnist voice activity detection benchmark ', 'deep neural network  model achieves data storage requirement low 2 bitsweight  whereas conventional binary neural network learning model require data storage 8 32 bitsweight ', 'amount data storage  model train bigger network weight  achieving 1  le test error conventional binary neural network learning model ', 'achieve similar classification error  conventional binary neural network model requires 4 data storage weight proposed model ', 'convolution neural network classifier  proposed model achieves 24  le test error onchip storage 6 storage saving achieve similar accuracy ']","This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes., The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training., This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy., Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training., We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks., For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight., With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model., To achieve the similar classification error, the conventional binary neural network model requires 4 more data storage for weights than our proposed model., For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6 storage savings to achieve the similar accuracy.
",17,5.598253275109171,13.470588235294118
470,"['Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.).', 'We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks.', 'In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error.', 'Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  ', 'The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.', 'The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space.', 'These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification.', 'The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  ', 'These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification.', 'We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.']","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.08888888359069824, 0.2666666507720947, 0.12244897335767746, 0.1904761791229248, 0.1428571343421936, 0.1904761791229248, 0.24242423474788666, 0.1538461446762085, 0.2380952388048172, 0.17142856121063232]",rJL6pz-CZ,"['Learning transport operators on manifolds forms a valuable representation for doing tasks like transfer learning.', 'Uses a dictionary learning framework to learn manifold transport operators on augmented USPS digits.', 'The paper considers the framework of manifold transport operator learning of Culpepper and Olshausen (2009), and interprets it as obtaining a MAP estimate under a probabilistic generative model.']","['withinclass variation highdimensional dataset modeled lowdimensional manifold due constraint physical process producing variation  eg  translation  illumination  etc   ', 'desire method learning representation manifold induced identitypreserving transformation used increase robustness  reduce training burden  encourage interpretability machine learning task ', 'particular  needed representation transformation manifold robustly capture shape manifold input data  generate new point manifold  extend transformation outside training domain without significantly increasing error ', 'previous work proposed algorithm efficiently learn analytic operator  called transport operator  define process transporting one data point manifold another ', 'main contribution paper define two transfer learning method use generative manifold representation learn natural transformation incorporate new data ', 'first method us representation novel randomized approach transfer learning employ learned generative model map unseen region data space ', 'result shown demonstration transfer learning data augmentation task fewshot image classification ', 'second method use transport operator injecting specific transformation new data example allows realistic image animation informed data augmentation ', 'result shown stylized construction using classic swiss roll data structure demonstration transfer learning data augmentation task fewshot image classification ', 'also propose use transport operator injecting transformation new data example allows realistic image animation ']","Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.)., We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks., In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error., Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  , The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data., The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space., These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification., The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  , These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification., We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.",18,5.912280701754386,15.052631578947368
471,"['The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.', 'We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.', 'If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.', 'This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.', 'SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.', 'Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.', 'Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.', 'It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.', 'Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.']","[0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.05882352590560913, 0.12121211737394333, 0.12903225421905518, 0.2666666507720947, 0.12903225421905518, 0.052631575614213943, 0.060606054961681366, 0.1875, 0.1111111044883728]",rkN2Il-RZ,"['We present a neural variational model for learning language-guided compositional visual concepts.', 'Proposes a novel neural net architecture that learns object concepts by combining a beta-VAE and SCAN.', 'This paper introduces a VAE-based model for translating between images and text, with their latent representation well-suited to applying symbolic operations, giving them a more expressive language for sampling images from text. ', 'This paper proposes a new model called SCAN (Symbol-Concept Association Network) for hierarchical concept learning and allows for generalization to new concepts composed from existing concepts using logical operators.']","['seemingly infinite diversity natural world arises relatively small set coherent rule  law physic chemistry ', 'conjecture rule give rise regularity discovered primarily unsupervised experience represented abstract concept ', 'representation compositional hierarchical  recombined exponentially large set new concept ', 'paper describes scan  symbolconcept association network   new framework learning abstraction visual domain ', 'scan learns concept fast symbol association  grounding disentangled visual primitive discovered unsupervised manner ', 'unlike state art multimodal generative model baseline  approach requires pairing symbol image make assumption form symbol representation ', 'trained  scan capable multimodal bidirectional inference  generating diverse set image sample symbolic description vice versa ', 'also allows traversal manipulation implicit hierarchy visual concept symbolic instruction learnt logical recombination operation ', 'manipulation enable scan break away training data distribution imagine novel visual concept symbolically instructed recombination previously learnt concept ']","The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry., We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts., If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts., This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain., SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner., Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations., Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa., It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations., Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.",16,6.074257425742574,12.625
472,"['Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.', 'In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.', 'The neural topic component encodes information from the source sentence to build a global topic distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.', 'The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.', 'In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n']","[0, 1, 0, 0, 0]","[0.1702127605676651, 0.4285714328289032, 0.2448979616165161, 0.29999998211860657, 0.10526315122842789]",S1GUgxgCW,"['Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.', 'This paper proposed the combination of topic model and seq2seq conversational model', 'Proposes a conversational model with topical information by combining seq2seq model with neural topic models and shows the proposed model outperforms some the baseline model seq2seq and other latent variable model variant of seq2seq.', 'The paper addresses the issue of enduring topicality in conversation models and proposes a model which is a combination of a neural topic model and a seq2seq-based dialog system. ']","['despite much success many largescale language task  sequencetosequence  seq2seq  model ideal choice conversational modeling tend generate generic repetitive response ', 'paper  propose latent topic conversational model  ltcm  augments seq2seq model neural topic component better model humanhuman conversation ', 'neural topic component encodes information source sentence build global  topic  distribution word  consulted seq2seq model improve generation time step ', 'experimental result show proposed ltcm generate diverse interesting response sampling learnt latent representation ', 'subjective human evaluation  judge also confirm ltcm preferred option comparing competitive baseline model ']","Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses., In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations., The neural topic component encodes information from the source sentence to build a global topic distribution over words, which is then consulted by the seq2seq model to improve generation at each time step., The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations., In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.
",9,5.7846153846153845,14.444444444444445
473,"['Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs.', 'Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function.', 'In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph.', 'Accordingly, message passing may not propagate helpful information throughout the graph.', 'We show that this conventional approach can fail to learn to perform even simple graph classification tasks.', 'We alleviate this serious shortcoming of the GNNs by making them a two step method.', 'In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph.', 'The embedding algorithm represents the graph as a point-cloud in the embedding space.', 'In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method.', 'The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors.', 'In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed.', 'Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm.', 'We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors.', 'We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets.', 'For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.06666666269302368, 0.060606054961681366, 0.09090908616781235, 0.07407406717538834, 0.07692307233810425, 0.19354838132858276, 0.27272728085517883, 0.14814814925193787, 0.12903225421905518, 0.2857142686843872, 0.25, 0.21621620655059814, 0.0624999962747097, 0.0]",Hkes0iR9KX,"['The graph analysis problem is transformed into a point cloud analysis problem. ', 'Proposes a deep GNN network for graph classification problems using their adaptive graph pooling layer.', 'The authors propose a method for learning representations for graphs']","['existing graph neural network  gnns  mere extension convolutional neural network  cnns  graph ', 'generally  consist several step message passing node followed global indiscriminate feature pooling function ', 'many datasets  however  node unlabeled label provide information similarity node location node graph ', 'accordingly  message passing may propagate helpful information throughout graph ', 'show conventional approach fail learn perform even simple graph classification task ', 'alleviate serious shortcoming gnns making two step method ', 'first proposed approach  graph embedding algorithm utilized obtain continuous feature vector node graph ', 'embedding algorithm represents graph pointcloud embedding space ', 'second step  gnn applied pointcloud representation graph provided embedding method ', 'gnn learns perform given task inferring topological structure graph encoded spatial distribution embedded vector ', 'addition  extend proposed approach graph clustering problem new architecture graph clustering proposed ', 'moreover  spatial representation graph utilized design graph pooling algorithm ', 'turn problem graph downsampling column sampling problem  ie  sampling algorithm selects subset node whose feature vector preserve spatial distribution feature vector ', 'apply proposed approach several popular benchmark datasets shown proposed geometrical approach strongly improves stateoftheart result several datasets ', 'instance  ptc dataset  improve stateoftheart result 22  ']","Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs., Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function., In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph., Accordingly, message passing may not propagate helpful information throughout the graph., We show that this conventional approach can fail to learn to perform even simple graph classification tasks., We alleviate this serious shortcoming of the GNNs by making them a two step method., In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph., The embedding algorithm represents the graph as a point-cloud in the embedding space., In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method., The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors., In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed., Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm., We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors., We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets., For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.",27,5.357142857142857,11.407407407407407
474,"['Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs.', 'Such adversarial examples can mislead DNNs to produce adversary-selected results.\n', 'Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. \n', 'In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. \n', 'For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  \n', 'We apply AdvGAN in both semi-whitebox and black-box attack settings.', 'In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks.', 'In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.\n', 'Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks.', 'Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1764705777168274, 0.14814814925193787, 0.19512194395065308, 0.3414634168148041, 0.14999999105930328, 0.4615384638309479, 0.1621621549129486, 0.1875, 0.10810810327529907, 0.1875]",HknbyQbC-,"['We propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings.', 'Describes AdvGAN, a conditional GAN plus adversarial loss, and evaluates AdvGAN on semi-white box and black box setting, reporting state-of-art results.', ""This paper proposes a way of generating adversarial examples that fool classification systems and wins MadryLab's mnist challenge.""]","['deep neural network  dnns  found vulnerable adversarial example resulting adding smallmagnitude perturbation input ', 'adversarial example mislead dnns produce adversaryselected result ', 'different attack strategy proposed generate adversarial example  produce high perceptual quality efficiently requires research effort ', 'paper  propose advgan generate adversarial example generative adversarial network  gans   learn approximate distribution original instance ', 'advgan  generator trained  generate adversarial perturbation efficiently instance  potentially accelerate adversarial training defense ', 'apply advgan semiwhitebox blackbox attack setting ', 'semiwhitebox attack  need access original target model generator trained  contrast traditional whitebox attack ', 'blackbox attack  dynamically train distilled model blackbox model optimize generator accordingly ', 'adversarial example generated advgan different target model high attack success rate stateoftheart defense compared attack ', 'attack placed first 9276  accuracy public mnist blackbox attack challenge ']","Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs., Such adversarial examples can mislead DNNs to produce adversary-selected results.
, Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. 
, In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. 
, For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  
, We apply AdvGAN in both semi-whitebox and black-box attack settings., In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks., In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.
, Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks., Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.",19,6.0717948717948715,10.263157894736842
475,"['This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.', 'Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.', 'We empirically demonstrate that:', 'a) deep autoencoder models generalize much better than the shallow ones,', 'b) non-linear activation functions with negative parts are crucial for training deep models, and', 'c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.', 'We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.', 'The new algorithm significantly speeds up training and improves model performance.', 'Our code is publicly available.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3255814015865326, 0.1764705777168274, 0.0, 0.0714285671710968, 0.06451612710952759, 0.06451612710952759, 0.1111111044883728, 0.0, 0.0]",SkNQeiRpb,"['This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.', 'This paper presents a deep autoencoder model for rating prediction that outperforms other state-of-the-art approahces on the Netflix prize dataset. ', 'Proposes to use a deep AE to do rating prediction tasks in recommender systems.', 'The authors present a model for more accurate Netflix recommendations demonstrating that a deep autoencoder can out-perform more complex RNN-based models that have temporal information. ']","['paper proposes new model rating prediction task recommender system significantly outperforms previous stateofthe art model timesplit netflix data set ', 'model based deep autoencoder 6 layer trained endtoend without layerwise pretraining ', 'empirically demonstrate ', ' deep autoencoder model generalize much better shallow one ', 'b  nonlinear activation function negative part crucial training deep model ', 'c  heavy use regularization technique dropout necessary prevent overfitting ', 'also propose new training algorithm based iterative output refeeding overcome natural sparseness collaborate filtering ', 'new algorithm significantly speed training improves model performance ', 'code publicly available ']","This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set., Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training., We empirically demonstrate that:, a) deep autoencoder models generalize much better than the shallow ones,, b) non-linear activation functions with negative parts are crucial for training deep models, and, c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting., We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering., The new algorithm significantly speeds up training and improves model performance., Our code is publicly available.",10,5.902439024390244,12.3
476,"['Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.', 'However, their inherently sequential computation makes them slow to train.', 'Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.', 'Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.', 'We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.', 'UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.', 'We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.', 'In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete.', 'Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.11538460850715637, 0.0, 0.1875, 0.12121211737394333, 0.4583333134651184, 0.22727271914482117, 0.2380952388048172, 0.10256409645080566, 0.40625]",HyzdRiR9Y7,"['We introduce the Universal Transformer, a self-attentive parallel-in-time recurrent sequence model that outperforms Transformers and LSTMs on a wide range of sequence-to-sequence tasks, including machine translation.', 'Proposes a new model UT, based on the Transformer model, with added recurrence and dynamic halting of the recurrence.', 'This paper extends Transformer by recursively applying a multi-head self-attention block, rather than stacking multiple blocks in the vanilla Transformer.']","['recurrent neural network  rnns  sequentially process data updating state new data point  long de facto choice sequence modeling task ', 'however  inherently sequential computation make slow train ', 'feedforward convolutional architecture recently shown achieve superior result sequence modeling task machine translation  added advantage concurrently process input sequence  leading easy parallelization faster training time ', 'despite success  however  popular feedforward sequence model like transformer fail generalize many simple task recurrent model handle ease  eg  copying string even simple logical inference string formula length exceed observed training time ', 'propose universal transformer  ut   parallelintime selfattentive recurrent sequence model cast generalization transformer model address issue ', 'ut combine parallelizability global receptive field feedforward sequence model like transformer recurrent inductive bias rnns ', 'also add dynamic perposition halting mechanism find improves accuracy several task ', 'contrast standard transformer  certain assumption ut shown turingcomplete ', 'experiment show ut outperform standard transformer wide range algorithmic language understanding task  including challenging lambada language modeling task ut achieve new state art  machine translation ut achieve 09 bleu improvement transformer wmt14 ende dataset ']","Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks., However, their inherently sequential computation makes them slow to train., Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times., Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time., We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues., UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs., We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks., In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete., Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",20,5.786821705426356,12.285714285714286
477,"['We present a framework for interpretable continual learning (ICL).', 'We show that explanations of previously performed tasks can be used to improve performance on future tasks.', 'ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task.', 'The ICL idea is general and may be applied to many continual learning approaches.', 'Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting.', 'We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality.', 'Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.']","[0, 0, 0, 0, 1, 0, 0]","[0.20512820780277252, 0.260869562625885, 0.23076923191547394, 0.27272728085517883, 0.2745097875595093, 0.25, 0.2539682388305664]",S1g9N2A5FX,"['The paper develops an interpretable continual learning framework where explanations of the finished tasks are used to enhance the attention of the learner during the future tasks, and where an explanation metric is proposed too. ', 'The authors propose a framework for continual learning based on explanations for performed classifications of previously learned tasks', 'This paper proposes an extension to the continual learning framework using existing variational continual learning as the base method with weight of evidence.']","['present framework interpretable continual learning  icl  ', 'show explanation previously performed task used improve performance future task ', 'icl generates good explanation finished task  us focus attention important facing new task ', 'icl idea general may applied many continual learning approach ', 'focus variational continual learning framework take advantage flexibility efficacy overcoming catastrophic forgetting ', 'use saliency map provide explanation performed task propose new metric ass quality ', 'experiment show icl achieves stateoftheart result term overall continual learning performance measured average classification accuracy  also term explanation  assessed qualitatively quantitatively using proposed metric ']","We present a framework for interpretable continual learning (ICL)., We show that explanations of previously performed tasks can be used to improve performance on future tasks., ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task., The ICL idea is general and may be applied to many continual learning approaches., Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting., We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality., Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.",10,5.581560283687943,14.1
478,"['The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017).', 'On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10).', 'In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.', 'In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations.', 'The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator.', 'We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput.', 'To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision']","[0, 0, 0, 0, 0, 0, 1]","[0.11538460850715637, 0.0317460261285305, 0.1304347813129425, 0.060606054961681366, 0.0, 0.1875, 0.3396226465702057]",H135uzZ0-,"['Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training;', 'This paper shows that a careful implementation of mixed-precision dynamic fixed point computation can achieve state of the art accuracy using a reduced precision deep learning model with a 16 bit integer representation', 'Proposes a ""dynamic fixed point"" scheme that shares the exponent part for a tensor and develops procedures to do NN computing with this format and demonstrates this for limited precision training.']","['stateoftheart  sota  mixed precision training dominated variant low precision floating point operation  particular  fp16 accumulating fp32 micikevicius et al   2017  ', 'hand  lot research also happened domain low mixedprecision integer training  work either present result nonsota network  instance alexnet imagenet1k   relatively small datasets  like cifar10  ', 'work  train stateoftheart visual understanding neural network imagenet1k dataset  integer operation general purpose  gp  hardware ', 'particular  focus integer fusedmultiplyandaccumulate  fma  operation take two pair int16 operand accumulate result int32 outputwe propose shared exponent representation tensor develop dynamic fixed point  dfp  scheme suitable common neural network operation ', 'nuance developing efficient integer convolution kernel examined  including method handle overflow int32 accumulator ', 'implement cnn training resnet50  googlenetv1  vgg16 alexnet  network achieve exceed sota accuracy within number iteration fp32 counterpart without change hyperparameters 18x improvement endtoend training throughput ', 'best knowledge result represent first int16 training result gp hardware imagenet1k dataset using sota cnns achieve highest reported accuracy using half precision']","The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017)., On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10)., In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware., In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations., The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator., We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput., To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision",18,5.881578947368421,12.0
479,"['In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model.', 'The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed.', 'Key ingredients for the acoustic model are Gated Linear Units and high dropout.', 'We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n']","[1, 0, 0, 0]","[0.5333333015441895, 0.1538461446762085, 0.2222222238779068, 0.0555555522441864]",Hyig0zb0Z,"['A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.', 'This paper applies gated convolutional neural networks to speech recognition, using the training criterion ASG.']","['paper introduce new speech recognition system  leveraging simple letterbased convnet acoustic model ', 'acoustic model requires audio transcription training  alignment annotation  forced alignment step needed  inference  decoder take word list language model  fed letter score acoustic model  phonetic word lexicon needed ', 'key ingredient acoustic model gated linear unit high dropout ', 'show near stateoftheart result word error rate librispeech corpus mfsc feature  clean configuration ']","In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model., The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed., Key ingredients for the acoustic model are Gated Linear Units and high dropout., We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.
",9,5.203883495145631,10.3
480,"['Generative adversarial networks (GANs) are a powerful framework for generative tasks.', 'However, they are difficult to train and tend to miss modes of the true data generation process.', 'Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space.', 'We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations.', 'Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes.', 'We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.']","[0, 0, 0, 1, 0, 0]","[0.07692307233810425, 0.12903225421905518, 0.19999998807907104, 0.21276594698429108, 0.06451612710952759, 0.1538461446762085]",ry0WOxbRZ,"['A noval GAN framework that utilizes transformation-invariant features to learn rich representations and strong generators.', 'Proposes a modified GAN objective consisting of a classic GAN term and an invariant encoding term.', 'This paper presents the IVE-GAN, a model that introduces en encoder to the Generative Adversarial Network framework.']","['generative adversarial network  gans  powerful framework generative task ', 'however  difficult train tend miss mode true data generation process ', 'although gans learn rich representation covered mode data latent space  framework miss inverse mapping data latent space ', 'propose invariant encoding generative adversarial network  ivegans   novel gan framework introduces mapping individual sample data utilizing feature data invariant certain transformation ', 'since model map individual sample latent space  naturally encourages generator cover mode ', 'demonstrate effectiveness approach term generative performance learning rich representation several datasets including common benchmark image generation task ']","Generative adversarial networks (GANs) are a powerful framework for generative tasks., However, they are difficult to train and tend to miss modes of the true data generation process., Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space., We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations., Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes., We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.",10,5.62043795620438,13.7
481,"['We propose a method for learning the dependency structure between latent variables in deep latent variable models.  ', 'Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.', 'In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  ', 'The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  ', 'Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  ', 'We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.', 'Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.']","[1, 0, 0, 0, 0, 0, 0]","[0.6896551847457886, 0.0, 0.34285715222358704, 0.19999998807907104, 0.12121211737394333, 0.1666666567325592, 0.1428571343421936]",SJgsCjCqt7,"['We propose a method for learning latent dependency structure in variational autoencoders.', 'Uses a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model.', 'This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training.', 'The authors propose to augment the latent space of a VAE with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior']","['propose method learning dependency structure latent variable deep latent variable model ', 'general modeling inference framework combine complementary strength deep generative model probabilistic graphical model ', 'particular  express latent variable space variational autoencoder  vae  term bayesian network learned  flexible dependency structure ', 'network parameter  variational parameter well latent topology optimized simultaneously single objective ', 'inference formulated via sampling procedure produce expectation latent variable structure incorporates topdown bottomup reasoning latent variable value ', 'validate framework extensive experiment mnist  omniglot  cifar10 ', 'comparison stateoftheart structured variational autoencoder baseline show improvement term expressiveness learned model ']","We propose a method for learning the dependency structure between latent variables in deep latent variable models.  , Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models., In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  , The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  , Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  , We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10., Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.",12,6.356060606060606,11.0
482,"[""Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change."", 'Detecting changes is important as they may indicate critical events.', 'However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales.', 'To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales.', 'Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing.', 'Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.']","[0, 0, 0, 1, 0, 0]","[0.11428570747375488, 0.0, 0.13636362552642822, 0.25531914830207825, 0.10810810327529907, 0.03999999538064003]",HkGTwjCctm,"['We introduce a scale-invariant neural network architecture for changepoint detection in multivariate time series.', 'The paper leverages the concept of wavelet transform within a deep architecture to solve change point detection.', 'This paper proposes a pyramid based neural net and applies it to 1D signals with underlying processes occurring at different time scales where the task is change point detection']","['many realworld time series  activity recognition  finance  climate science  changepoints system structure parameter change ', 'detecting change important may indicate critical event ', 'however  existing method changepoint detection face challenge  1  pattern change modeled using simple predefined metric   2  change occur gradually  multiple timescales ', 'address  show changepoint detection treated supervised learning problem  propose new deep neural network architecture efficiently identify abrupt gradual change multiple scale ', 'proposed method  pyramid recurrent neural network  prnn   designed scaleinvariant  incorporating wavelet pyramid analysis technique multiscale signal processing ', 'experiment synthetic realworld datasets  show prnn detect abrupt gradual change higher accuracy state art extrapolate detect changepoints novel timescales seen training ']","Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change., Detecting changes is important as they may indicate critical events., However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales., To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales., Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing., Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.",19,5.775757575757575,8.68421052631579
483,"['We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.', 'We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables.', 'The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way.', 'For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.']","[1, 0, 0, 0]","[0.43478259444236755, 0.1249999925494194, 0.0, 0.06666666269302368]",HkeyZhC9F7,"['RL finds better heuristics for automated reasoning algorithms.', 'Aims to learn a heuristic for a backtracking search algorithm utilizing Reinforcement learning and proposes a model that makes use of Graphical Neural Networks to produce literal and clauses embedding, and use them to predict the quality of each literal to decide the probability of each action.', 'The paper proposes an approach to automatically learning variable selection heuristics for QBF using deep learning']","['demonstrate learn efficient heuristic automated reasoning algorithm deep reinforcement learning ', 'focus backtracking search algorithm quantified boolean logic  already solve formula impressive size  100 thousand variable ', 'main challenge find representation formula lends making prediction scalable way ', 'challenging problem  heuristic learned approach reduces execution time factor 10 compared existing handwritten heuristic ']","We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning., We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables., The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way., For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.",6,5.682352941176471,14.166666666666666
484,"['We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data.', 'We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.', 'Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution.', 'We use this procedure to assess the performance of several modern generative adversarial network architectures.', 'We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  ', 'We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.']","[1, 0, 0, 0, 0, 0]","[0.29999998211860657, 0.10526315122842789, 0.10526315122842789, 0.06451612710952759, 0.14999999105930328, 0.1395348757505417]",ByuI-mW0W,"['Assess whether or not your GAN is actually doing something other than memorizing the training data.', 'Aims to provide a quality measure/test for GANs and proposes to evaluate the current approximation of a distribution learnt by a GAN by using Wasserstein distance between two distributions made of a sum of Diracs as a baseline performance. ', 'This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation, using the procedure to test and improve the current GANs']","['consider question ass generative adversarial network  particular respect whether generalise beyond memorising training data ', 'propose simple procedure assessing generative adversarial network performance based principled consideration actual goal generalisation ', 'approach involves using test set estimate wasserstein distance generative distribution produced procedure  underlying data distribution ', 'use procedure ass performance several modern generative adversarial network architecture ', 'find procedure sensitive choice ground metric underlying data space  suggest choice ground metric substantially improves performance ', 'finally suggest attending ground metric used wasserstein generative adversarial network training may fruitful  outline concrete pathway towards ']","We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data., We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is., Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution., We use this procedure to assess the performance of several modern generative adversarial network architectures., We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  , We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.",10,5.773972602739726,14.6
485,"['The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.', 'Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).', 'Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.', 'In this work, we propose to leverage automatic search techniques to discover new activation functions.', 'Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.', 'We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.', 'Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.', 'For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.', 'The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.20408162474632263, 0.09302324801683426, 0.08695651590824127, 0.22727271914482117, 0.2666666507720947, 0.21739129722118378, 0.53125, 0.11320754140615463, 0.1538461446762085]",SkBYYyZRZ,"['We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.', 'Proposes a reinforcement learning based approach for finding non-linearity by searching through combinations from a set of unary and binary operators.', 'This paper utilizes reinforcement learning to search the combination of a set of unary and binary functions resulting in a new activation function', 'The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates. ']","['choice activation function deep network significant effect training dynamic task performance ', 'currently  successful widelyused activation function rectified linear unit  relu  ', 'although various handdesigned alternative relu proposed  none managed replace due inconsistent gain ', 'work  propose leverage automatic search technique discover new activation function ', 'using combination exhaustive reinforcement learningbased search  discover multiple novel activation function ', 'verify effectiveness search conducting empirical evaluation best discovered activation function ', 'experiment show best discovered activation function  f  x   x  sigmoid  beta  x   name swish  tends work better relu deeper model across number challenging datasets ', 'example  simply replacing relus swish unit improves top1 classification accuracy imagenet 09  mobile nasneta 06  inceptionresnetv2 ', 'simplicity swish similarity relu make easy practitioner replace relus swish unit neural network ']","The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance., Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU)., Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains., In this work, we propose to leverage automatic search techniques to discover new activation functions., Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions., We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function., Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets., For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2., The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.",17,5.6923076923076925,10.705882352941176
486,"['Successful training of convolutional neural networks is often associated with suffi-\n', 'ciently deep architectures composed of high amounts of features.', 'These networks\n', 'typically rely on a variety of regularization and pruning techniques to converge\n', 'to less redundant states.', 'We introduce a novel bottom-up approach to expand\n', 'representations in fixed-depth architectures.', 'These architectures start from just a\n', 'single feature per layer and greedily increase width of individual layers to attain\n', 'effective representational capacities needed for a specific task.', 'While network\n', 'growth can rely on a family of metrics, we propose a computationally efficient\n', 'version based on feature time evolution and demonstrate its potency in determin-\n', 'ing feature importance and a networks effective capacity.', 'We demonstrate how\n', 'automatically expanded architectures converge to similar topologies that benefit\n', 'from lesser amount of parameters or improved accuracy and exhibit systematic\n', 'correspondence in representational complexity with the specified task.', 'In contrast\n', 'to conventional design patterns with a typical monotonic increase in the amount of\n', 'features with increased depth, we observe that CNNs perform better when there is\n', 'more learnable parameters in intermediate, with falloffs to earlier and later layers.']","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0714285671710968, 0.07999999821186066, 0.06896550953388214, 0.0952380895614624, 0.1599999964237213, 0.0952380895614624, 0.08695651590824127, 0.2666666507720947, 0.07999999821186066, 0.0, 0.06896550953388214, 0.1599999964237213, 0.0, 0.23076923191547394, 0.0, 0.1599999964237213, 0.13333332538604736, 0.19999998807907104, 0.13793103396892548]",SkffVjUaW,"['A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity.', 'Proposes to dynamically adjust the feature map depth of a fully convolutional neural network, formulating a measure of self-resemblance and boosting performance.', 'Introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity.', 'Aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network.']","['successful training convolutional neural network often associated suffi', 'ciently deep architecture composed high amount feature ', 'network', 'typically rely variety regularization pruning technique converge', 'le redundant state ', 'introduce novel bottomup approach expand', 'representation fixeddepth architecture ', 'architecture start', 'single feature per layer greedily increase width individual layer attain', 'effective representational capacity needed specific task ', 'network', 'growth rely family metric  propose computationally efficient', 'version based feature time evolution demonstrate potency determin', 'ing feature importance network  effective capacity ', 'demonstrate', 'automatically expanded architecture converge similar topology benefit', 'lesser amount parameter improved accuracy exhibit systematic', 'correspondence representational complexity specified task ', 'contrast', 'conventional design pattern typical monotonic increase amount', 'feature increased depth  observe cnns perform better', 'learnable parameter intermediate  falloff earlier later layer ']","Successful training of convolutional neural networks is often associated with suffi-
, ciently deep architectures composed of high amounts of features., These networks
, typically rely on a variety of regularization and pruning techniques to converge
, to less redundant states., We introduce a novel bottom-up approach to expand
, representations in fixed-depth architectures., These architectures start from just a
, single feature per layer and greedily increase width of individual layers to attain
, effective representational capacities needed for a specific task., While network
, growth can rely on a family of metrics, we propose a computationally efficient
, version based on feature time evolution and demonstrate its potency in determin-
, ing feature importance and a networks effective capacity., We demonstrate how
, automatically expanded architectures converge to similar topologies that benefit
, from lesser amount of parameters or improved accuracy and exhibit systematic
, correspondence in representational complexity with the specified task., In contrast
, to conventional design patterns with a typical monotonic increase in the amount of
, features with increased depth, we observe that CNNs perform better when there is
, more learnable parameters in intermediate, with falloffs to earlier and later layers.",25,6.081967213114754,7.32
487,"['Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation).', 'Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way.', 'In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient.', 'Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network.', 'In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation.', 'This feed-forward network learns to approximate the state of the fixed-point using a local learning rule.', 'After training, we can simply use this initializing network for inference, resulting in a learned feedforward network.', 'Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation.', 'This shows how we might go about training deep networks without using backpropagation.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.06451612710952759, 0.0476190410554409, 0.1428571343421936, 0.15686273574829102, 0.21052631735801697, 0.32258063554763794, 0.1875, 0.1111111044883728, 0.13793103396892548]",B1GMDsR5tm,"['We train a feedforward network without backprop by using an energy-based model to provide local targets', 'This paper aims at quickening the iterative inference procedure in energy-based models trained with Equilibrium Propagation (EP), by proposing to train a feedforward network to predict a fixed point of the ""equilibrating network"". ', 'Training a separate network to initialize recurrent networks trained using equilibrium propagation ']","['deep neural network almost universally trained reversemode automatic differentiation  aka  backpropagation  ', 'biological network  hand  appear lack mechanism sending gradient back input neuron  thus learning way ', 'response  scellier  bengio  2017  proposed equilibrium propagation  method gradientbased train ing neural network us local learning rule  crucially  rely neuron mechanism backpropagating error gradient ', 'equilibrium propagation  however  major practical limitation  inference involves iterative optimization neural activation find fixedpoint  number step required closely approximate fixed point scale poorly depth network ', 'response problem  propose initialized equilibrium propagation  train feedforward network initialize iterative inference procedure equilibrium propagation ', 'feedforward network learns approximate state fixedpoint using local learning rule ', 'training  simply use initializing network inference  resulting learned feedforward network ', 'experiment show network appears work well better original version equilibrium propagation ', 'show might go training deep network without using backpropagation ']","Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation)., Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way., In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient., Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network., In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation., This feed-forward network learns to approximate the state of the fixed-point using a local learning rule., After training, we can simply use this initializing network for inference, resulting in a learned feedforward network., Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation., This shows how we might go about training deep networks without using backpropagation.",22,5.835680751173709,9.26086956521739
488,"['We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.', 'A single object may have many attributes which when altered do not change the identity of the object itself.', 'Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses.', 'The attribute of wearing glasses can be changed without changing the identity of the person.', 'However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task.', ""Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses')."", 'We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute.', 'We refer to this specific synthesis process as image attribute manipulation.', 'We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.5454545617103577, 0.07407406717538834, 0.06666666269302368, 0.08695651590824127, 0.0714285671710968, 0.1764705777168274, 0.1249999925494194, 0.0952380895614624, 0.20000000298023224]",BJfRpoA9YX,"['Learn representations for images that factor out a single attribute.', 'This paper builds on Conditional VAE GANs to allow attribute manipulation in the synthesis process.', 'This paper proposes a generative model to learn the representation which can separate the identity of an object from an attribute, and extends the autoencoder adversarial by adding an auxiliary network.']","['propose novel generative model architecture designed learn representation image factor single attribute rest representation ', 'single object may many attribute altered change identity object ', 'consider human face  identity particular person independent whether happen wearing glass ', 'attribute wearing glass changed without changing identity person ', 'however  ability manipulate alter image attribute without altering object identity trivial task ', ' interested learning representation image separate identity object  human face  attribute  wearing glass   ', 'demonstrate success factorization approach using learned representation synthesize face without chosen attribute ', 'refer specific synthesis process image attribute manipulation ', 'demonstrate model achieves competitive score  state art  facial attribute classification task ']","We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation., A single object may have many attributes which when altered do not change the identity of the object itself., Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses., The attribute of wearing glasses can be changed without changing the identity of the person., However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task., Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses')., We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute., We refer to this specific synthesis process as image attribute manipulation., We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.",13,5.27027027027027,14.23076923076923
489,"['Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames.', 'These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive.', 'We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points.', 'For example, our model can ""jump"" and directly sample frames at the end of the video, without sampling intermediate frames.', 'Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity.', 'We also apply our framework to a 3D scene reconstruction dataset.', 'Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain.', 'Reconstructions and videos are available at https://bit.ly/2O4Pc4R.\n']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.39024388790130615, 0.21739129722118378, 0.25, 0.2790697515010834, 0.21052631735801697, 0.2222222238779068, 0.23728813230991364, 0.05882352590560913]",S1gQ5sRcFm,"['We present a model for consistent 3D reconstruction and jumpy video prediction e.g. producing image frames multiple time-steps in the future without generating intermediate frames.', 'This paper proposes a general method for indexed data modeling by encoding index information together with observation into a neural network, and then decode the observation condition on the target index.', 'Proposes using a VAE that encodes input video in a permuation invariant way to predict future frames of a video.']","['stochastic video prediction model take sequence image frame  generate sequence consecutive future image frame ', 'model typically generate future frame autoregressive fashion  slow requires input output frame consecutive ', 'introduce model overcomes drawback generating latent representation arbitrary set frame used simultaneously efficiently sample temporally consistent frame arbitrary timepoints ', 'example  model  jump  directly sample frame end video  without sampling intermediate frame ', 'synthetic video evaluation confirm substantial gain speed functionality without loss fidelity ', 'also apply framework 3d scene reconstruction dataset ', ' model conditioned camera location sample consistent set image occluded region 3d scene might look like  even multiple possibility region might contain ', 'reconstruction video available http  bitly2o4pc4r ']","Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames., These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive., We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points., For example, our model can ""jump"" and directly sample frames at the end of the video, without sampling intermediate frames., Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity., We also apply our framework to a 3D scene reconstruction dataset., Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain., Reconstructions and videos are available at https://bit.ly/2O4Pc4R.
",14,5.550295857988166,12.071428571428571
490,"['The ADAM optimizer is exceedingly popular in the deep learning community.', 'Often it works very well, sometimes it doesnt.', 'Why?', 'We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.', 'We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM s inner workings.', 'Transferring the ""variance adaptation to momentum- SGD gives rise to a novel method, completing the practitioners toolbox for problems where ADAM fails.']","[1, 0, 0, 0, 0, 0]","[0.375, 0.0, 0.05882352590560913, 0.0, 0.07999999821186066]",S1EwLkW0W,"['Analyzing the popular Adam optimizer', 'The paper trys to improve Adam based on variance adaption with momentum by proposing two algorithms', ""This paper analyzes the scale-invariance and the particular shape of the learning rate used in Adam, arguing that Adam's update is a combination of a sign-update and a variance-based learning rate."", 'The paper splits ADAM algorithm into two components: stochastic direction in sign of gradient and adaptive stepwise with relative variance, and two algorithms are proposed to test each of them.']","['adam optimizer exceedingly popular deep learning community ', 'often work well  sometimes  ', '', 'interpret adam combination two aspect  weight  update direction determined sign stochastic gradient  whereas update magnitude solely determined estimate relative variance ', 'disentangle two aspect analyze isolation  shedding light adam  inner working ', 'transferring  variance adaptation  momentum sgd give rise novel method  completing practitioner  toolbox problem adam fails ']","The ADAM optimizer is exceedingly popular in the deep learning community., Often it works very well, sometimes it doesnt., Why?, We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance., We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM s inner workings., Transferring the ""variance adaptation to momentum- SGD gives rise to a novel method, completing the practitioners toolbox for problems where ADAM fails.",11,5.360824742268041,8.818181818181818
491,"['We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.', 'The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation.', 'However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space.', 'In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices.', 'Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power.', 'Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm.', 'We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.']","[1, 0, 0, 0, 0, 0, 0]","[1.0, 0.1875, 0.20512819290161133, 0.3255814015865326, 0.19999998807907104, 0.1428571343421936, 0.2545454502105713]",S1uxsye0Z,"['We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.', 'The authors connect dropout parameters to a bound of the Rademacher complexity of the network', ""Relates complexity of networks' learnability to dropout rates in backpropagation.""]","['propose novel framework adaptively adjust dropout rate deep neural network based rademacher complexity bound ', 'stateoftheart deep learning algorithm impose dropout strategy prevent feature coadaptation ', 'however  choosing dropout rate remains art heuristic relies empirical gridsearch hyperparameter space ', 'work  show network rademacher complexity bounded function related dropout rate vector weight coefficient matrix ', 'subsequently  impose bound regularizer provide theoretical justified way tradeoff model complexity representation power ', 'therefore  dropout rate empirical loss unified objective function  optimized using block coordinate descent algorithm ', 'discover adaptively adjusted dropout rate converge interesting distribution reveal meaningful patternsexperiments task image document classification also show method achieves better performance compared stateof theart dropout algorithm ']","We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound., The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation., However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space., In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices., Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power., Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm., We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.",12,5.939024390243903,13.666666666666666
492,"['Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control.', 'Deep neural networks have been adopted for sensor fusion in a body of recent studies.', 'Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN).', 'In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights.', 'Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.\n']","[0, 1, 0, 0, 0]","[0.1621621549129486, 0.23999999463558197, 0.0, 0.1395348757505417, 0.19512194395065308]",Syeil309tX,"['Optimized gated deep learning architectures for sensor fusion is proposed.', 'The authors improve upon several limitations of the baseline negated architecture by proposing a coarser-grained gated fusion architecture and a two-stage gated fusion architecture', 'Proposes two gated deep learning architectures for sensor fusion and by having the grouped features, demonstrates improved performance, especially in the presence of random sensor noise and failures.']","['sensor fusion key technology integrates various sensory input allow robust decision making many application autonomous driving robot control ', 'deep neural network adopted sensor fusion body recent study ', 'among  socalled netgated architecture proposed  demonstrated improved performance conventional convolu tional neural network  cnn  ', 'paper  address several limitation baseline negated architecture proposing two optimized architecture  coarsergrained gated architecture employing  feature  grouplevel fusion weight twostage gated architecture leveraging grouplevel feature level fusion weight ', 'using driving mode prediction human activity recogni tion datasets  demonstrate significant performance improvement brought proposed gated architecture also robustness presence sensor noise failure ']","Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control., Deep neural networks have been adopted for sensor fusion in a body of recent studies., Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN)., In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights., Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.
",9,6.188405797101449,15.333333333333334
493,"['We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.', 'In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization.', 'Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function.', 'Indeed, batch normalization itself is the cause of gradient explosion.', 'As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations.', 'While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections.', 'Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range.', 'Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.3478260934352875, 0.13793103396892548, 0.1621621549129486, 0.10526315122842789, 0.10810810327529907, 0.0555555522441864, 0.15789473056793213, 0.0]",SyMDXnCcF7,"['Batch normalization causes exploding gradients in vanilla feedforward networks.', 'Develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights.', 'Provides a dynamic perspective on deep neural network using the evolution of the covariance matrix along with the layers.']","['develop mean field theory batch normalization fullyconnected feedforward neural network ', ' provide precise characterization signal propagation gradient backpropagation wide batchnormalized network initialization ', 'theory show gradient signal grow exponentially depth exploding gradient eliminated tuning initial weight variance adjusting nonlinear activation function ', 'indeed  batch normalization cause gradient explosion ', 'result  vanilla batchnormalized network without skip connection trainable large depth common initialization scheme  prediction verify variety empirical simulation ', 'gradient explosion eliminated  reduced tuning network close linear regime  improves trainability deep batchnormalized network without residual connection ', 'finally  investigate learning dynamic batchnormalized network observe single step optimization network achieve relatively stable equilibrium gradient dramatically smaller dynamic range ', 'theory leverage laplace  fourier  gegenbauer transforms derive new identity may independent interest ']","We develop a mean field theory for batch normalization in fully-connected feedforward neural networks., In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization., Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function., Indeed, batch normalization itself is the cause of gradient explosion., As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations., While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections., Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range., Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.",17,6.090909090909091,11.0
494,"['We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  ', 'Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.', 'Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.']","[1, 0, 0]","[0.3265306055545807, 0.178571417927742, 0.1875]",HJMC_iA5tm,"['We train a graph network to predict boolean satisfiability and show that it learns to search for solutions, and that the solutions it finds can be decoded from its activations.', 'The paper describes a general neural network architecture for predicting satisfiability', 'This paper presents the NeuroSAT architecture which uses a deep message passing neural net for predicting the satisfiability of CNF instances']","['present neurosat  message passing neural network learns solve sat problem trained classifier predict satisfiability ', 'although competitive stateoftheart sat solver  neurosat solve problem substantially larger difficult ever saw training simply running iteration ', 'moreover  neurosat generalizes novel distribution  training random sat problem  test time solve sat problem encoding graph coloring  clique detection  dominating set  vertex cover problem  range distribution small random graph ']","We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  , Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations., Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.",11,5.581632653061225,8.909090909090908
495,"['Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain.', 'Traffic forecasting is one canonical example of such learning task.', 'The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting.', 'To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.', 'Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling.', 'We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines']","[0, 0, 0, 1, 0, 0]","[0.0, 0.0, 0.10256409645080566, 0.2800000011920929, 0.06451612710952759, 0.05714285373687744]",SJiHXGWAZ,"['A neural sequence model that learns to forecast on a directed graph.', 'The paper proposes the Diffusion Convolutional Recurrent Neural Network architecture for the spatiotemporal traffic forecasting problem', 'Proposes to build a traffic forecasting model using a diffusion process for convolutional recurrent neural networks to address saptio-temporal autocorrelation.']","['spatiotemporal forecasting various application neuroscience  climate transportation domain ', 'traffic forecasting one canonical example learning task ', 'task challenging due  1  complex spatial dependency road network   2  nonlinear temporal dynamic changing road condition  3  inherent difficulty longterm forecasting ', 'address challenge  propose model traffic flow diffusion process directed graph introduce diffusion convolutional recurrent neural network  dcrnn   deep learning framework traffic forecasting incorporates spatial temporal dependency traffic flow ', 'specifically  dcrnn capture spatial dependency using bidirectional random walk graph  temporal dependency using encoderdecoder architecture scheduled sampling ', 'evaluate framework two realworld largescale road network traffic datasets observe consistent improvement 12   15  stateoftheart baseline']","Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain., Traffic forecasting is one canonical example of such learning task., The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting., To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow., Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling., We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines",12,6.283687943262412,11.75
496,"['Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge.', 'Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior.', 'In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution.', 'We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.', 'The key idea is to train the model to output high uncertainty for data points outside of the training distribution.', 'NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs.', 'NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training.', 'Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning.', 'We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.19354838132858276, 0.1666666567325592, 0.3333333432674408, 0.13793103396892548, 0.3888888955116272, 0.23255813121795654, 0.10526315122842789, 0.25, 0.21052631735801697]",HkgxasA5Ym,"['We train neural networks to be uncertain on noisy inputs to avoid overconfident predictions outside of the training distribution.', 'Presents an approach to obtain uncertainty estimates for neural network predictions that has good performance when quantifying predictive uncertainty at points that are outside of the training distribution.', 'The paper considers the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior']","['obtaining reliable uncertainty estimate neural network prediction long standing challenge ', 'bayesian neural network proposed solution  remains open specify prior ', 'particular  common practice standard normal prior weight space imposes weak regularity  causing function posterior possibly generalize unforeseen way input outside training distribution ', 'propose noise contrastive prior  ncps  obtain reliable uncertainty estimate ', 'key idea train model output high uncertainty data point outside training distribution ', 'ncps using input prior  add noise input current mini batch  output prior  wide distribution given input ', 'ncps compatible model output uncertainty estimate  easy scale  yield reliable uncertainty estimate throughout training ', 'empirically  show ncps prevent overfitting outside training distribution result uncertainty estimate useful active learning ', 'demonstrate scalability method flight delay data set  significantly improve upon previously published result ']","Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge., Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior., In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution., We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates., The key idea is to train the model to output high uncertainty for data points outside of the training distribution., NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs., NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training., Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning., We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.",19,5.505208333333333,10.105263157894736
497,"['Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition.', 'This has lead to the speculation that both systems use similar mechanisms to perform recognition.', 'In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.', 'We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask.', 'When trained on these modified set of images, the model did not show any bias towards selecting shapes as features.', 'Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels.', 'We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.']","[0, 0, 1, 0, 0, 0, 0]","[0.260869562625885, 0.09999999403953552, 0.7213114500045776, 0.13793103396892548, 0.08695651590824127, 0.07547169178724289, 0.11320754140615463]",ByePUo05K7,"['This study highlights a key difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.', 'Seeks to establish via a series of well-designed experiments that CNNs trained for image classification dont encode shape-bias like human vision.', 'This paper highlights the fact that CNNs will not necessarily learn to recognize objects based on their shape and shows they will overfeat to noise based features.']","['convolutional neural network  cnns  inspired human vision  setting  achieve performance comparable human object recognition ', 'lead speculation system use similar mechanism perform recognition ', 'study  conducted series simulation indicate fundamental difference human vision cnns  object recognition human relies analysing shape  cnns shapebias ', 'teased apart type feature selected model modifying cifar10 dataset  addition containing object shape  image concurrently contained nonshape feature  noiselike mask ', 'trained modified set image  model show bias towards selecting shape feature ', 'instead relied whichever feature allowed perform best prediction  even feature noiselike mask single predictive pixel amongst 50176 pixel ', 'also found regularisation method  batch normalisation dropout  change behaviour neither past concurrent experience image datasets ']","Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition., This has lead to the speculation that both systems use similar mechanisms to perform recognition., In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias., We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask., When trained on these modified set of images, the model did not show any bias towards selecting shapes as features., Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels., We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.",17,5.301587301587301,11.117647058823529
498,"['The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.', 'Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.', 'We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.', 'We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.', 'Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.', 'This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.', ""Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content."", 'Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.', 'Our model can be used on problems with a huge, potentially infinite, number of categories.', 'We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.']","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.21276594698429108, 0.1304347813129425, 0.2666666507720947, 0.2181818187236786, 0.19999998807907104, 0.2857142686843872, 0.2926829159259796, 0.23999999463558197, 0.29999998211860657, 0.22727271914482117]",ryRh0bb0Z,"['We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.', 'This paper presents a GAN-based method for image generation that attempts to separate latent variables describing image content from those describing properties of view.', 'This paper proposes a GAN architecture that aims at decomposing the underlying distribution of a particular class into ""content"" and ""view"".', 'Proposes a new generative model based on the Generative Adversarial Network (GAN) that disentangles the content and the view of objects without view supervision and extends GMV into a conditional generative model that takes an input image and generates different views of the object in the input image. ']","['development highdimensional generative model recently gained great surge interest introduction variational autoencoders generative adversarial neural network ', 'different variant proposed underlying latent space structured  example  based attribute describing data generate ', 'focus particular problem one aim generating sample corresponding number object various view ', 'assume distribution data driven two independent latent factor  content  represents intrinsic feature object  view  stand setting particular observation object ', 'therefore  propose generative model conditional variant built disentangled latent space ', 'approach allows u generate realistic sample corresponding various object high variety view ', 'unlike many multiview approach  model nt need supervision view content ', 'compared conditional generation approach mostly based binary categorical attribute  make assumption factor variation ', 'model used problem huge  potentially infinite  number category ', 'experiment four image datasets demonstrate effectiveness model ability generalize ']","The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks., Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate., We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views., We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object., Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space., This approach allows us to generate realistic samples corresponding to various objects in a high variety of views., Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content., Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations., Our model can be used on problems with a huge, potentially infinite, number of categories., We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.",20,5.468468468468468,11.1
499,"['The huge size of deep networks hinders their use in small computing devices.', 'In this paper, we consider compressing the network by weight quantization.', 'We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.', 'Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.']","[0, 0, 0, 1]","[0.0, 0.23076923191547394, 0.23255813121795654, 0.3333333432674408]",BkrSv0lA-,"['A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.', 'Proposes a method of compressing network by means of weight ternarization. ', 'The paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values.', 'The paper extends the loss-aware weight binarization scheme to terarization and arbitrary m-bit quantization and demonstrate its promising performance.']","['huge size deep network hinders use small computing device ', 'paper  consider compressing network weight quantization ', 'extend recently proposed lossaware weight binarization scheme ternarization  possibly different scaling parameter positive negative weight  mbit   2  quantization ', 'experiment feedforward recurrent neural network show proposed scheme outperforms stateoftheart weight quantization algorithm  accurate  even accurate  fullprecision network ']","The huge size of deep networks hinders their use in small computing devices., In this paper, we consider compressing the network by weight quantization., We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization., Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.",8,6.048780487804878,10.25
500,"['In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments.', 'The options framework provides formalism for such abstraction over sequences of decisions.  ', 'However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable.', 'Indeed, it is preferable to learn options directly from interaction with the environment.', 'Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  ', 'In this work we develop a novel policy gradient method for the automatic learning of policies with options.  ', 'This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels.', 'Experimental results show that the options learned can be interpreted.', 'Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.19512194395065308, 0.1875, 0.10256409645080566, 0.1875, 0.17543859779834747, 0.7368420958518982, 0.17391303181648254, 0.13793103396892548, 0.3181818127632141]",rJIgf7bAZ,"['We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.', 'The paper presents a new policy gradient technique for learning options, where a single sample can be used to update all options.', 'Proposes an off-policy method for learning options in complex continuous problems.']","['pursuit increasingly intelligent learning system  abstraction play vital role enabling sophisticated decision made complex environment ', 'option framework provides formalism abstraction sequence decision ', 'however model require option given priori  presumably specified hand  neither efficient  scalable ', 'indeed  preferable learn option directly interaction environment ', 'despite several effort  remains difficult problem  many approach require access model environmental dynamic  inferred option often interpretable  limit ability explain system behavior verification debugging purpose ', 'work develop novel policy gradient method automatic learning policy option ', 'algorithm us inference method simultaneously improve option available agent  thus employed offpolicy manner  without observing option label ', 'experimental result show option learned interpreted ', ' find method presented sample efficient existing method  leading faster stable learning policy option ']","In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments., The options framework provides formalism for such abstraction over sequences of decisions.  , However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable., Indeed, it is preferable to learn options directly from interaction with the environment., Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  , In this work we develop a novel policy gradient method for the automatic learning of policies with options.  , This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels., Experimental results show that the options learned can be interpreted., Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.",21,5.680628272251309,9.095238095238095
501,"['The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.', 'The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000).', 'The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.']","[1, 0, 0]","[0.27586206793785095, 0.1538461446762085, 0.1428571343421936]",ByxF-nAqYX,"['Unsupervised feature selection through capturing the local linear structure of the data', 'Proposes locally linear unsupervised feature selection.', 'The paper proposes the LLUFS method for feature selection.']","['paper  interested unsupervised feature selection  aim retain feature best accounting local pattern data ', 'proposed approach  called locally linear unsupervised feature selection  relies dimensionality reduction method characterize pattern  feature thereafter assessed according compliance wrt  local pattern  taking inspiration locally linear embedding  roweis saul  2000  ', 'experimental validation approach scikitfeature benchmark suite demonstrates effectiveness compared state art ']","The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data., The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000)., The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.",9,6.083333333333333,8.4
502,"['Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.', 'Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax.""', 'In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.', 'We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.', 'We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task.', 'However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly.', 'We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.']","[0, 0, 1, 0, 0, 0, 0]","[0.06666666269302368, 0.15789473056793213, 0.37837836146354675, 0.3333333432674408, 0.04878048226237297, 0.1249999925494194, 0.17777776718139648]",H18WqugAb,"['Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks.', '\nThis paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs and  exposes the short-comings of current seq2seq RNN architectures.', 'The paper analyzes the composition abilities of RNNs, specifically, the generalization ability of RNNs on random subset of SCAN commands, on longer SCAN commands, and of composition over primitive commands. ', 'The authors introduce a new dataset that facilitates the analysis of a Seq2Seq learning case']","['human understand produce new utterance effortlessly  thanks systematic compositional skill ', 'person learns meaning new verb  dax   immediately understand meaning  dax twice   sing dax  ', 'paper  introduce scan domain  consisting set simple compositional navigation command paired corresponding action sequence ', 'test zeroshot generalization capability variety recurrent neural network  rnns  trained scan sequencetosequence method ', 'find rnns generalize well difference training test command small  apply  mixandmatch  strategy solve task ', 'however  generalization requires systematic compositional skill   dax  example   rnns fail spectacularly ', 'conclude proofofconcept experiment neural machine translation  supporting conjecture lack systematicity important factor explaining neural network need large training set ']","Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills., Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax."", In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences., We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods., We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task., However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly., We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.",14,5.784810126582278,11.285714285714286
503,"['This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.', 'First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.', 'Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.', 'We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.', 'The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.']","[1, 0, 0, 0, 0]","[0.30434781312942505, 0.23188404738903046, 0.2950819730758667, 0.28070175647735596, 0.21875]",S1xiOjC9F7,"['We tackle the problem of similarity learning for structured objects with applications in particular in computer security, and propose a new model graph matching networks that excels on this task.', 'Authors introduce a Graph Matching Network for retrival and matching of graph structured objects.', 'The authors attack the problem of graph matching by proposing an extension of graph embedding networks', 'The authors present two methods for learning a similarity score between pairs of graphs and show the beneficiality of introducing idesa from graph matching to graph neural networks.']","['paper address challenging problem retrieval matching graph structured object  make two key contribution ', 'first  demonstrate graph neural network  gnn   emerged effective model various supervised prediction problem defined structured data  trained produce embedding graph vector space enables efficient similarity reasoning ', 'second  propose novel graph matching network model  given pair graph input  computes similarity score jointly reasoning pair new crossgraph attentionbased matching mechanism ', 'demonstrate effectiveness model different domain including challenging problem controlflowgraph based function similarity search play important role detection vulnerability software system ', 'experimental analysis demonstrates model able exploit structure context similarity learning also outperform domainspecific baseline system carefully handengineered problem ']","This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions., First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning., Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism., We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems., The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.",12,5.890243902439025,13.666666666666666
504,"['Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.', 'In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.', 'As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  ', 'We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.', 'Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.', 'We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.']","[0, 1, 0, 0, 0, 0]","[0.11764705181121826, 0.5185185074806213, 0.11428570747375488, 0.13333332538604736, 0.05714285373687744, 0.25806450843811035]",Bk7wvW-C-,"['We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.', 'Modifications to the skip-thought framework for learning sentence embeddings.', 'This paper presents a new RNN encoderCNN decoder hybrid design for use in pretraining, which does not require an autoregressive decoder when pretraining encoders.', 'The authors extend Skip-thought by decoding only one target sentence using a CNN decoder.']","['context information play important role human language understanding  also useful machine learn vector representation language ', 'paper  explore asymmetric encoderdecoder structure unsupervised contextbased sentence representation learning ', 'result  build encoderdecoder architecture rnn encoder cnn decoder  show neither autoregressive decoder rnn decoder required ', 'combine suite effective design significantly improve model efficiency also achieving better performance ', 'model trained two different large unlabeled corpus  case transferability evaluated set downstream language understanding task ', 'empirically show model simple fast producing rich sentence representation excel downstream task ']","Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language., In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning., As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  , We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance., Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks., We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.",11,5.755725190839694,11.909090909090908
505,"['Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs).', 'VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function.', 'GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data.', 'The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems.', 'In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on.', ""In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework."", 'Our numerical results on several datasets demonstrate consistent trends with the proposed theory.']","[0, 0, 0, 1, 0, 0, 0]","[0.21621620655059814, 0.0624999962747097, 0.060606054961681366, 0.25, 0.0952380895614624, 0.12121211737394333, 0.0]",BygMAiRqK7,"['A statistical approach to compute sample likelihoods in Generative Adversarial Networks', 'Show that WGAN with entropic regularization maximizes a lower bound on the likelihood of the observed data distribution.', ""Authors claim it is possible to leverage the upper bound from an entropy regularized optimal transport to come up with a measure of 'sample likelihood'.""]","['building success deep learning  two modern approach learn probability model observed data generative adversarial network  gans  variational autoencoders  vaes  ', 'vaes consider explicit probability model data compute generative distribution maximizing variational lowerbound loglikelihood function ', 'gans  however  compute generative model minimizing distance observed generated probability distribution without considering explicit model observed data ', 'lack explicit probability model gans prohibits computation sample likelihood framework limit use statistical inference problem ', 'work  show optimal transport gan entropy regularization viewed generative model maximizes lowerbound average sample likelihood  approach vaes based ', 'particular  proof construct explicit probability model gans used compute likelihood statistic within gan framework ', 'numerical result several datasets demonstrate consistent trend proposed theory ']","Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs)., VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function., GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data., The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems., In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on., In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework., Our numerical results on several datasets demonstrate consistent trends with the proposed theory.",13,5.848837209302325,13.23076923076923
506,"['We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.', 'Our contribution is threefold.', 'First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.', 'Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.', 'Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode-thus considerably facilitating the application of Riemannian geometry in machine learning.', 'In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package']","[1, 0, 0, 0, 0, 0]","[0.5454545617103577, 0.0, 0.1538461446762085, 0.21621620655059814, 0.178571417927742, 0.11764705181121826]",rklXaoAcFX,"['We introduce geomstats, an efficient Python package for Riemannian modelization and optimization over manifolds compatible with both numpy and tensorflow .', 'The paper introduces the software package geomstats, which provides simple use of Riemannian manifolds and metrics within machine learning models', 'Proposes a Python package for optimization and applications on Reimannian manifolds and highlights the differences between Geomstats package and other packages.', 'Introduces a geometric toolbox, Geomstats, for machine learning on Reimannian manifolds.']","['introduce geomstats  python package riemannian modelization optimization manifold hyperspheres  hyperbolic space  spd matrix lie group transformation ', 'contribution threefold ', 'first  geomstats allows flexible modeling many machine learning problem efficient extensively unittested implementation manifold  well set useful riemannian metric  exponential logarithm map provide ', 'moreover  wide choice loss function implementation corresponding gradient allow fast easy optimization manifold ', 'finally  geomstats package provide unified framework riemannian geometry  operation implemented geomstats available different computing backends  numpy  tensorflow kera   well gpuenabled modethus considerably facilitating application riemannian geometry machine learning ', 'paper  present geomstats review utility advantage manifold machine learning  using concrete example span show efficiency practicality implementation using package']","We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations., Our contribution is threefold., First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide., Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds., Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode-thus considerably facilitating the application of Riemannian geometry in machine learning., In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package",18,5.994186046511628,9.555555555555555
507,"['We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference.', 'The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices.', 'However, most of the previous studies optimize for inference while neglect training or even complicate it.', 'Training is far more intractable, since', '(i) the neurons dominate the memory cost rather than the weights in inference;', '(ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;', '(iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity.', 'To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection.', 'Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.']","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.4651162624359131, 0.0, 0.11428570747375488, 0.0, 0.19999998807907104, 0.1764705777168274, 0.0, 0.12765957415103912, 0.09999999403953552]",H1goBoR9F7,"['We construct dynamic sparse graph via dimension-reduction search to reduce compute and memory cost in both DNN training and inference.', 'The authors propose to use dynamic sparse computation graph for reducing the computation memory and time cost in deep neural network (DNN).', 'This paper proposes a method to speed up training and inference of deep neural networks using dynamic pruning of the compute graph.']","['propose execute deep neural network  dnns  dynamic sparse graph  dsg  structure compressive memory accelerative execution training inference ', 'great success dnns motivates pursuing lightweight model deployment onto embedded device ', 'however  previous study optimize inference neglect training even complicate ', 'training far intractable  since', '  neuron dominate memory cost rather weight inference ', ' ii  dynamic activation make previous sparse acceleration via oneoff optimization fixed weight invalid ', ' iii  batch normalization  bn  critical maintaining accuracy activation reorganization damage sparsity ', 'address issue  dsg activates small amount neuron high selectivity iteration via dimensionreduction search obtains bn compatibility via doublemask selection ', 'experiment show significant memory saving  1745x  operation reduction  2344x  little accuracy loss various benchmark ']","We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference., The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices., However, most of the previous studies optimize for inference while neglect training or even complicate it., Training is far more intractable, since, (i) the neurons dominate the memory cost rather than the weights in inference;, (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;, (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity., To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection., Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.",12,6.0,13.083333333333334
508,"['Efficient exploration remains a major challenge for reinforcement learning.', 'One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic.', 'Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting.', 'Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning.', 'As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning.', 'The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise.', 'We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.']","[0, 0, 0, 0, 0, 1, 0]","[0.19354838132858276, 0.1538461446762085, 0.2222222238779068, 0.23255813121795654, 0.21739129722118378, 0.3333333432674408, 0.1621621549129486]",Byx83s09Km,"['We develop a practical extension of Information-Directed Sampling for Reinforcement Learning, which accounts for parametric uncertainty and heteroscedasticity in the return distribution for exploration.', 'The authors propose a way of extending Information-Directed Sampling to reinforcement learning by combining two types of uncertainty to obtain a simple exploration strategy based on IDS. ', 'This paper investigates sophistical exploration approaches for reinforcement learning built on Information Direct Sampling and on Distributional Reinforcement Learning']","['efficient exploration remains major challenge reinforcement learning ', 'one reason variability return often depends current state action  therefore heteroscedastic ', 'classical exploration strategy upper confidence bound algorithm thompson sampling fail appropriately account heteroscedasticity  even bandit setting ', 'motivated recent finding address issue bandit  propose use informationdirected sampling  id  exploration reinforcement learning ', 'main contribution  build recent advance distributional reinforcement learning propose novel  tractable approximation id deep qlearning ', 'resulting exploration strategy explicitly account parametric uncertainty heteroscedastic observation noise ', 'evaluate method atari game demonstrate significant improvement alternative approach ']","Efficient exploration remains a major challenge for reinforcement learning., One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic., Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting., Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning., As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning., The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise., We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.",12,6.390625,10.666666666666666
509,"['We address the problem of learning structured policies for continuous control.', 'In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.', 'In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.', ""Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent."", 'In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.', 'We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.', 'We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n']","[0, 0, 1, 0, 0, 0, 0]","[0.1538461446762085, 0.1538461446762085, 0.2857142686843872, 0.2702702581882477, 0.12121211737394333, 0.10810810327529907, 0.10526315122842789]",S1sqHMZCb,"['using graph neural network to model structural information of the agents to improve policy and transferability ', 'A method for representing and learning structured policy for continuous control tasks using Graph Neural Networks', ""The submission proposes incorporation of additional structure into reinforcement learning problems, particularly the structure of the agent's morphology"", 'Propose an application of Graph Neural Networks to learning policies for controlling ""centipede"" robots of different lengths.']","['address problem learning structured policy continuous control ', 'traditional reinforcement learning  policy agent learned mlps take concatenation observation environment input predicting action ', 'work  propose nervenet explicitly model structure agent  naturally take form graph ', 'specifically  serving agent policy network  nervenet first propagates information structure agent predict action different part agent ', 'experiment  first show nervenet comparable stateoftheart method standard mujoco environment ', 'propose customized reinforcement learning environment benchmarking two type structure transfer learning task  ie  size disability transfer ', 'demonstrate policy learned nervenet significantly better policy learned model able transfer even zeroshot setting ']","We address the problem of learning structured policies for continuous control., In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions., In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph., Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent., In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments., We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer., We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.
",15,5.723684210526316,10.133333333333333
510,"['Real-world tasks are often highly structured.', 'Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).', 'However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.', 'In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.', 'Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.', 'To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  \n', 'In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.', 'This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  ', 'Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.']","[0, 0, 0, 0, 0, 0, 0, 1, 0]","[0.0, 0.19999998807907104, 0.11764705181121826, 0.2631579041481018, 0.1111111044883728, 0.1666666567325592, 0.2857142686843872, 0.4000000059604645, 0.09756097197532654]",Hyl_vjC5KQ,"['This paper presents a hierarchical reinforcement learning framework based on deterministic option policies and mutual information maximization. ', 'Proposes an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy.', 'This paper proposes an HRL system in which the mutal information of the latent variable and the state-action pairs is approximately maximized.', 'Proposes a criterion that aims to maximize the mutual information between options and state-action pairs and show empirically that the learned options decompose the state-action space but not the state space. ']","['realworld task often highly structured ', 'hierarchical reinforcement learning  hrl  attracted research interest approach leveraging hierarchical structure given task reinforcement learning  rl  ', 'however  identifying hierarchical policy structure enhances performance rl trivial task ', 'paper  propose hrl method learns latent variable hierarchical policy using mutual information maximization ', 'approach interpreted way learn discrete latent representation stateaction space ', 'learn option policy correspond mode advantage function  introduce advantageweighted importance sampling ', 'hrl method  gating policy learns select option policy based optionvalue function  option policy optimized based deterministic policy gradient method ', 'framework derived leveraging analogy monolithic policy standard rl hierarchical policy hrl using deterministic option policy ', 'experimental result indicate hrl approach learn diversity option enhance performance rl continuous control task ']","Real-world tasks are often highly structured., Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL)., However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task., In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization., Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space., To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  
, In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method., This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  , Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.",14,5.609625668449198,13.357142857142858
511,"['Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.', 'However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.', 'To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).', 'Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.', 'This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.', 'We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.', ""Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs."", ""We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.""]","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0476190410554409, 0.13636362552642822, 0.21739129722118378, 0.13636362552642822, 0.10526315122842789, 0.16326530277729034, 0.12765957415103912, 0.1702127605676651]",SkEqro0ctQ,"['We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.', 'A novel approach to explain neural network predictions by learning hierarchical representations of groups of input features and their contribution to the final prediction', 'Extends an existing feature interpretation method for LSTMs to more generic DNNs and introduces a hierarchical clustering of the input features and the contributions of each cluster to the final prediction.', 'This paper proposes a hierarchical extension of contextual decomposition.']","['deep neural network  dnns  achieved impressive predictive performance due ability learn complex  nonlinear relationship variable ', 'however  inability effectively visualize relationship led dnns characterized black box consequently limited application ', 'ameliorate problem  introduce use hierarchical interpretation explain dnn prediction proposed method  agglomerative contextual decomposition  acd  ', 'given prediction trained dnn  acd produce hierarchical clustering input feature  along contribution cluster final prediction ', 'hierarchy optimized identify cluster feature dnn learned predictive ', 'introduce acd using example stanford sentiment treebank imagenet  order diagnose incorrect prediction  identify dataset bias  extract polarizing phrase varying length ', 'human experiment  demonstrate acd enables user identify accurate two dnns better trust dnn output ', 'also find acd hierarchy largely robust adversarial perturbation  implying capture fundamental aspect input ignores spurious noise ']","Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables., However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications., To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD)., Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction., This hierarchy is optimized to identify clusters of features that the DNN learned are predictive., We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths., Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs., We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.",18,5.951086956521739,10.222222222222221
512,"['Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.', 'PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.', 'We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.', 'Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.', 'In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.', 'Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.']","[0, 1, 0, 0, 0, 0]","[0.5116279125213623, 0.6976743936538696, 0.1875, 0.138888880610466, 0.25925925374031067, 0.2380952388048172]",rkl42iA5t7,"['We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.', 'Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix.', 'This paper introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies.', 'This paper proposes a compression method based on spectral analysis']","['principal filter analysis  pfa  easy implement  yet effective method neural network compression ', 'pfa exploit intrinsic correlation filter response within network layer recommend smaller network footprint ', 'propose two compression algorithm  first allows user specify proportion original spectral energy preserved layer compression  second heuristic lead parameterfree approach automatically selects compression used layer ', 'algorithm evaluated several architecture datasets  show considerable compression rate without compromising accuracy  eg  vgg16 cifar10  cifar100 imagenet  pfa achieves compression rate 8x  3x  14x accuracy gain 04   14  point  24  respectively ', 'test also demonstrate network compressed pfa achieve accuracy close empirical upper bound given compression ratio ', 'finally  show pfa effective tool simultaneous compression domain adaptation ']","Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression., PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint., We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer., Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively., In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio., Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.",18,5.540697674418604,9.555555555555555
513,"['We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.', 'In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.', 'Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.', 'Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.', 'We show that the improved performance is due to the increased diversity of reformulation strategies.']","[1, 0, 0, 0, 0]","[0.3333333432674408, 0.0555555522441864, 0.0714285671710968, 0.1428571343421936, 0.14814814925193787]",rkMhusC5Y7,"['Multiple diverse query reformulation agents trained with reinforcement learning to improve search engines.', 'Parellelization of ensemble method in reinforement learning for query reformulation, speeding up training and improving the diversity of learnt freformulations', 'The authors propose to train multiple distinct agents, each over a different subset of the training set.', 'The authors propose an ensemble approach for query reformulation']","['propose method efficiently learn diverse strategy reinforcement learning query reformulation task document retrieval question answering ', 'proposed framework agent consists multiple specialized subagents metaagent learns aggregate answer subagents produce final answer ', 'subagents trained disjoint partition training data  metaagent trained full training set ', 'method make learning faster  highly parallelizable  better generalization performance strong baseline  ensemble agent trained full data ', 'show improved performance due increased diversity reformulation strategy ']","We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering., In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer., Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set., Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data., We show that the improved performance is due to the increased diversity of reformulation strategies.",9,5.608695652173913,12.777777777777779
514,"['Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\\mathbb{R}^d$.', ""Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label')."", 'In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.\n\n', 'A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc.', 'To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.).', 'We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.\n\n', 'We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity.', 'Finally, we illustrate the potential of CNE for network visualization.']","[0, 0, 0, 0, 0, 0, 0, 1]","[0.1818181723356247, 0.1090909019112587, 0.16949151456356049, 0.04651162400841713, 0.1818181723356247, 0.1538461446762085, 0.1860465109348297, 0.2142857164144516]",ryepUj0qtX,"['We introduce a network embedding method that accounts for prior information about the network, yielding superior empirical performance.', 'The paper proposed to use a prior distribution to constraint the network embedding, for the formulation this paper used very restricted Gaussian distributions.', 'Proposes learning unsupervised node embeddings by considering the structural properties of networks.']","['network embeddings  ne  map node given network   dimensional euclidean space  mathbb  r   ', 'ideally  mapping similar  node mapped onto nearby point  ne used purpose link prediction  similar  mean likely connected   classification  similar  mean likely label   ', 'recent year various method ne introduced  following similar strategy  defining notion similarity node  typically distance measure within network   distance measure embedding space  loss function penalizes large distance similar node small distance dissimilar node ', 'difficulty faced existing method certain network fundamentally hard embed due structural property   approximate  multipartiteness  certain degree distribution  assortativity  etc ', 'overcome  introduce conceptual innovation ne literature propose create emph  conditional network embeddings   cnes   embeddings maximally add information respect given structural property  eg  node degree  block density  etc   ', 'use simple bayesian approach achieve  propose block stochastic gradient descent algorithm fitting efficiently ', 'demonstrate cnes superior link prediction multilabel classification compared stateoftheart method  without adding significant mathematical computational complexity ', 'finally  illustrate potential cne network visualization ']","Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\mathbb{R}^d$., Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label')., In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.

, A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc., To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.)., We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.

, We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity., Finally, we illustrate the potential of CNE for network visualization.",22,5.919831223628692,9.916666666666666
515,"['This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients.', ""This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad."", 'Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question.', 'In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\\log{T}/\\sqrt{T})$ for non-convex stochastic optimization.', 'Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum).', 'We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge.', 'Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.']","[0, 0, 0, 0, 0, 1, 0]","[0.1666666567325592, 0.19512194395065308, 0.25531914830207825, 0.40740740299224854, 0.14999999105930328, 0.41860464215278625, 0.3636363446712494]",H1x-x309tm,"['We analyze convergence of Adam-type algorithms and provide mild sufficient conditions to guarantee their convergence, we also show  violating the conditions can makes an algorithm diverge.', 'Presents a convergence analysis in the non-convex setting for a family of optimization algorithms.', 'This paper investigates the convergence condition of Adam-type optimizers in the unconstrained non-convex optimization problems.']","['paper study class adaptive gradient based momentum algorithm update search direction learning rate simultaneously using past gradient ', 'class  refer   adamtype   includes popular algorithm adam  amsgrad  adagrad ', 'despite popularity training deep neural network  dnns   convergence algorithm solving nonconvex problem remains open question ', 'paper  develop analysis framework set mild sufficient condition guarantee convergence adamtype method  convergence rate order   log   sqrt     nonconvex stochastic optimization ', 'convergence analysis applies new algorithm called adafom  adagrad first order momentum  ', 'show condition essential  identifying concrete example violating condition make algorithm diverge ', 'besides providing one first comprehensive analysis adamtype method nonconvex setting  result also help practitioner easily monitor progress algorithm determine convergence behavior ']","This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients., This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad., Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question., In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\log{T}/\sqrt{T})$ for non-convex stochastic optimization., Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum)., We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge., Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.",16,5.9397590361445785,10.375
516,"['This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.', 'These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data.', 'The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required.', 'This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture.', 'Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.']","[0, 0, 1, 0, 0]","[0.11764705181121826, 0.10169491171836853, 0.21739129722118378, 0.1702127605676651, 0.1463414579629898]",rkhxwltab,"['Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.', 'The paper proposes using the absolute value activation function in an autoencoder architecture with an additional supervised learning term in the objective function', 'This paper introduces a reversible network with absolute value used as the activation function']","['research paper describes simplistic architecture named aann  absolute artificial neural network  used create highly interpretable representation input data ', 'representation generated penalizing learning network way learned representation correspond respective label present labelled dataset used supervised training  thereby  simultaneously giving network ability classify input data ', 'network used reverse direction generate data closely resembles input feeding representation vector required ', 'research paper also explores use mathematical ab  absolute valued  function activation function constitutes core part neural network architecture ', 'finally result obtained mnist dataset using technique presented discussed brief ']","This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data., These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data., The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required., This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture., Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.",7,5.7214285714285715,20.0
517,"['Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.', 'Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.', 'Similarly, pre-processing introduces an additional source of error.', 'To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].', 'Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.', 'TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.', 'TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.', 'Furthermore, we observe a significant increase in sample efficiency.', 'With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.', 'We open-source our trained models, experiments, and source code.']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.20512819290161133, 0.20512819290161133, 0.07692307233810425, 0.09999999403953552, 0.5, 0.27272728085517883, 0.0952380895614624, 0.07407406717538834, 0.10256409645080566, 0.07407406717538834]",BJgrxbqp67,"['We propose a Transformer based relation extraction model that uses pre-trained language representations instead of explicit linguistic features.', 'Presents a transformer-based relation extraction model that leverages pre-training on unlabeled text with a language modeling objective.', 'This article describes a novel application of Transformer networks for relation extraction.', 'The paper presents a Transformer based architecture for relaxation extraction, evaluating on two datasets.']","['current stateoftheart relation extraction method typically rely set lexical  syntactic  semantic feature  explicitly computed preprocessing step ', 'training feature extraction model requires additional annotated language resource  severely restricts applicability portability relation extraction novel language ', 'similarly  preprocessing introduces additional source error ', 'address limitation  introduce tre  transformer relation extraction  extending openai generative pretrained transformer  radford et al  2018  ', 'unlike previous relation extraction model  tre us pretrained deep language representation instead explicit linguistic feature inform relation classification combine selfattentive transformer architecture effectively model longrange dependency entity mention ', 'tre allows u learn implicit linguistic feature solely plain text corpus unsupervised pretraining  finetuning learned language representation relation extraction task ', 'tre obtains new stateoftheart result tacred semeval 2010 task 8 datasets  achieving test f1 674 871  respectively ', 'furthermore  observe significant increase sample efficiency ', '20  training example  tre match performance baseline model trained scratch 100  tacred dataset ', 'opensource trained model  experiment  source code ']","Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step., Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages., Similarly, pre-processing introduces an additional source of error., To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]., Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions., TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task., TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively., Furthermore, we observe a significant increase in sample efficiency., With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset., We open-source our trained models, experiments, and source code.",27,6.334951456310679,7.62962962962963
518,"['Neural networks have recently had a lot of success for many tasks.', 'However, neural\n', 'network architectures that perform well are still typically designed manually\n', 'by experts in a cumbersome trial-and-error process.', 'We propose a new method\n', 'to automatically search for well-performing CNN architectures based on a simple\n', 'hill climbing procedure whose operators apply network morphisms, followed\n', 'by short optimization runs by cosine annealing.', 'Surprisingly, this simple method\n', 'yields competitive results, despite only requiring resources in the same order of\n', 'magnitude as training a single network.', 'E.g., on CIFAR-10, our method designs\n', 'and trains networks with an error rate below 6% in only 12 hours on a single GPU;\n', 'training for one day reduces this error further, to almost 5%.']","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.23999999463558197, 0.0, 0.09999999403953552, 0.4444444477558136, 0.3333333432674408, 0.0, 0.0, 0.23529411852359772, 0.0, 0.10526315122842789, 0.0952380895614624, 0.19999998807907104, 0.0833333283662796]",SySaJ0xCZ,"['We propose a simple and efficent method for architecture search for convolutional neural networks.', 'Proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources.', 'Presents a method to search neural network architectures at the same time of training which dramatically saves training time and architecture searching time.', 'Proposes variant of neural architecture search using network morphisms to define a search space using CNN architectures completing the CIFAR image classification task']","['neural network recently lot success many task ', 'however  neural', 'network architecture perform well still typically designed manually', 'expert cumbersome trialanderror process ', 'propose new method', 'automatically search wellperforming cnn architecture based simple', 'hill climbing procedure whose operator apply network morphisms  followed', 'short optimization run cosine annealing ', 'surprisingly  simple method', 'yield competitive result  despite requiring resource order', 'magnitude training single network ', 'eg  cifar10  method design', 'train network error rate 6  12 hour single gpu ', 'training one day reduces error  almost 5  ']","Neural networks have recently had a lot of success for many tasks., However, neural
, network architectures that perform well are still typically designed manually
, by experts in a cumbersome trial-and-error process., We propose a new method
, to automatically search for well-performing CNN architectures based on a simple
, hill climbing procedure whose operators apply network morphisms, followed
, by short optimization runs by cosine annealing., Surprisingly, this simple method
, yields competitive results, despite only requiring resources in the same order of
, magnitude as training a single network., E.g., on CIFAR-10, our method designs
, and trains networks with an error rate below 6% in only 12 hours on a single GPU;
, training for one day reduces this error further, to almost 5%.",21,5.470588235294118,5.666666666666667
519,"['We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.\n', 'We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.\n', 'Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective.', 'GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph.', ""Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties."", 'We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them.', 'Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.']","[1, 0, 0, 0, 0, 0, 0]","[0.1538461446762085, 0.1538461446762085, 0.0, 0.1428571343421936, 0.08695651590824127, 0.06666666269302368, 0.0]",H15RufWAW,"['Using GANs to generate graphs via random walks.', 'The authors proposed a generative model of random walks on graphs that allows for model-agnostic learning, controllable fitting, ensemble graph generation', 'Proposes a WGAN formulation for generating graphs based on random walks using node embeddings and an LSTM architecture for modeling.']","['propose graphgan  first implicit generative model graph enables mimic realworld network ', 'pose problem graph generation learning distribution biased random walk single input graph ', 'model based stochastic neural network generates discrete output sample  trained using wasserstein gan objective ', 'graphgan enables u generate sibling graph  similar property yet exact replica original graph ', 'moreover  graphgan learns semantic mapping latent input space generated graph property ', 'discover sampling certain region latent space lead varying property output graph  smooth transition ', 'strong generalization property graphgan highlighted competitive performance link prediction well promising result node classification  even though specifically trained task ']","We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.
, We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.
, Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective., GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph., Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties., We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them., Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.",12,5.570469798657718,12.416666666666666
520,"['The ability of a classifier to recognize unknown inputs is important for many classification-based systems.', 'We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.', 'We propose a method based on the Generative Adversarial Networks (GAN) framework.', 'We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.', 'We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.', 'Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.']","[0, 0, 0, 0, 0, 1]","[0.19354838132858276, 0.375, 0.3571428656578064, 0.29999998211860657, 0.307692289352417, 0.42424240708351135]",Hy7EPh10W,"['We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework.', 'Proposes a GAN to unify classification and novelty detection.', 'The paper presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions.', 'The paper proposes a GAN for novelty detection using a mixture generator with feature matching loss']","['ability classifier recognize unknown input important many classificationbased system ', 'discus problem simultaneous classification novelty detection  ie  determining whether input known set class specific class  unknown domain belong known class ', 'propose method based generative adversarial network  gan  framework ', 'show multiclass discriminator trained generator generates sample mixture nominal novel data distribution optimal novelty detector ', 'approximate generator mixture generator trained feature matching loss empirically show proposed method outperforms conventional method novelty detection ', 'finding demonstrate simple  yet powerful new application gan framework task novelty detection ']","The ability of a classifier to recognize unknown inputs is important for many classification-based systems., We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes., We propose a method based on the Generative Adversarial Networks (GAN) framework., We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector., We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection., Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.",9,5.549295774647887,14.2
521,"[""  Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security."", 'A crucial requirement of such speaker verification systems is to be domain robust.', 'Performance should not degrade even if speakers are talking in languages not seen during training.', 'To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks.', 'We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes.', 'We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy.', 'Our results also show that models with adversarial adaptation perform significantly better than unadapted models.', 'In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances.', 'Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.04999999701976776, 0.10526315122842789, 0.05128204822540283, 0.17777776718139648, 0.23076923191547394, 0.0952380895614624, 0.10256409645080566, 0.19230768084526062, 0.11538460850715637]",Byx4xH3is7,"['Speaker verificaiton performance can be significantly improved by adapting the model to in-domain data using Generative Adversarial Networks. Furthermore, the adaptation can be performed in an unsupervised way.', 'Propose a number of GAN variants on the task of speaker recognition in the domain mismatched condition.']","['verifying person identity based voice challenging  realworld problem biometric security ', 'crucial requirement speaker verification system domain robust ', 'performance degrade even speaker talking language seen training ', 'end  present flexible interpretable framework learning domain invariant speaker embeddings using generative adversarial network ', 'combine adversarial training angular margin loss function  encourages speaker embedding model discriminative directly optimizing cosine similarity class ', 'able beat strong baseline system using cosine distance classifier simple scoreaveraging strategy ', 'result also show model adversarial adaptation perform significantly better unadapted model ', 'attempt better understand behavior  quantitatively measure degree invariance induced proposed method using maximum mean discrepancy frechet distance ', 'analysis show proposed adversarial speaker embedding model significantly reduce distance source target data distribution  performing similarly former better latter ']","  Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security., A crucial requirement of such speaker verification systems is to be domain robust., Performance should not degrade even if speakers are talking in languages not seen during training., To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks., We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes., We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy., Our results also show that models with adversarial adaptation perform significantly better than unadapted models., In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances., Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter.",14,5.983606557377049,13.071428571428571
522,"['Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.', 'In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).', 'Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.', 'If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.', 'By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.', 'Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.', 'In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.']","[0, 0, 0, 0, 1, 0, 0]","[0.1666666567325592, 0.2702702581882477, 0.14999999105930328, 0.11428570747375488, 0.4117647111415863, 0.1538461446762085, 0.2702702581882477]",Skl3M20qYQ,"['Minimising the synergistic mutual information within the latents and the data for the task of disentanglement using the VAE framework.', 'Proposes a new objective function for learning dientangled representations in a variational framework by minimizing the synergy of the information provided.', 'The authors aim at training a VAE that has disentangled latent representations in a ""synergistically"" maximal way. ', 'This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables.']","['learning disentangling representation independent factor variation explain data unsupervised setting still major challenge ', 'following paper address task disentanglement introduce new stateoftheart approach called nonsynergistic variational autoencoder  nonsyn vae  ', 'model draw inspiration population coding  notion synergy arises describe encoded information neuron form response stimulus ', 'response convey information together separate independent source encoding information  acting synergetically ', 'penalizing synergistic mutual information within latents encourage information independence disentangle latent factor ', 'notably  approach could added vae framework easily  new elbo function still lower bound log likelihood ', 'addition  qualitatively compare model factor vae show one implicitly minimises synergy latents ']","Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge., In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE)., Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli., If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically., By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors., Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood., In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.",12,5.7625,13.333333333333334
523,"['   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  ', 'Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations.', 'Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random.', 'In this work, we propose the use of {\\em structured arrangements} through randomized {\\em microbatches} of examples that are more likely to include similar ones.', 'We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  ', 'Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\\%.', 'Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.']","[0, 0, 0, 0, 0, 1, 0]","[0.0, 0.09090908616781235, 0.08695651590824127, 0.06896551698446274, 0.0624999962747097, 0.1111111044883728, 0.06896551698446274]",r1erRoCqtX,"['Accelerating SGD by arranging examples differently', 'The paper presents a method for improving the convergence rate of Stochastic Gradient Descent for learning embeddings by grouping similar training samples together.', 'Proposes a non-uniform sampling strategy to construct minibatches in SGD for the task of learning embeddings for object associations.']","['metric embeddings immensely useful representation association entity  image  user  search query  word   ', 'embeddings learned optimizing loss objective general form sum example association ', 'typically  optimization us stochastic gradient update minibatches example arranged independently random ', 'work  propose use  em structured arrangement  randomized  em microbatches  example likely include similar one ', 'make principled argument property arrangement accelerate training present efficient algorithm generate microbatches respect marginal distribution training example ', 'finally  observe experimentally structured arrangement accelerate training 320  ', 'structured arrangement emerge powerful novel performance knob sgd independent complementary sgd hyperparameters thus candidate wide deployment ']","   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  , Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations., Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random., In this work, we propose the use of {\em structured arrangements} through randomized {\em microbatches} of examples that are more likely to include similar ones., We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  , Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\%., Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.",14,6.054054054054054,10.571428571428571
524,"['Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data.', 'One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words.', 'In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  ', ""We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method."", 'For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.', 'In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.']","[0, 0, 1, 0, 0, 0]","[0.05714285373687744, 0.07999999821186066, 0.307692289352417, 0.17142856121063232, 0.045454543083906174, 0.07692307233810425]",rJ7yZ2P6-,"['Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue', 'This paper proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations with sizable gains over the baselines.', 'Proposes combining external pretrained word embeddings and pretrained word embeddings on training data by keeping them as two views.', 'Proposes method to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to conversational datasets and applies new variants of LSTM-based model to the task of response-selection in dialogue modeling.']","['ubuntu dialogue corpus largest public available dialogue corpus make feasible build endtoend deep neural network model directly conversation data ', 'one challenge ubuntu dialogue corpus large number outofvocabulary word ', 'paper proposed algorithm combine general pretrained word embedding vector generated taskspecific training set address issue ', 'integrated character embedding chen et al enhanced lstm method  esim  used evaluate effectiveness proposed method ', 'task next utterance selection  proposed method demonstrated significant performance improvement original esim new model achieved stateoftheart result ubuntu dialogue corpus douban conversation corpus ', 'addition  investigated performance impact endofutterance endofturn token tag ']","Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end
deep neural network models directly from the conversation data., One challenge of Ubuntu dialogue corpus is 
the large number of out-of-vocabulary words., In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  , We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method., For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus., In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.",8,5.840579710144928,17.25
525,"['Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.', 'Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.', 'However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.', 'Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.', 'The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).', 'In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.', 'This allows us to train neural networks to perform optimization faster than well tuned first-order methods.', 'Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.', 'We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement']","[0, 0, 1, 0, 0, 0, 0, 0, 0]","[0.08695651590824127, 0.12244897335767746, 0.29629629850387573, 0.17777776718139648, 0.04255318641662598, 0.1355932205915451, 0.1702127605676651, 0.16949151456356049, 0.2295081913471222]",HJxwAo09KQ,"['We analyze problems when training learned optimizers, address those problems via variational optimization using two complementary gradient estimators, and train optimizers that are 5x faster in wall-clock time than baseline optimizers (e.g. Adam).', 'This paper uses un-rolled optimization to learn neural networks for optimization.', 'This paper tackles the problem of learning an optimizer, specifically the authors focus on obtaining cleaner gradients from the unrolled training procedure.', 'Presents a method for ""learning an optimizer"" by using a Variational Optimization for the ""outer"" optimizer loss and proposes the idea to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weights them using a product of Gaussians formula for the mean.']","['deep learning shown learned function dramatically outperform handdesigned function perceptual task ', 'analogously  suggests learned update function may similarly outperform current handdesigned optimizers  especially specific task ', 'however  learned optimizers notoriously difficult train yet demonstrate wallclock speedup handdesigned optimizers  thus rarely used practice ', 'typically  learned optimizers trained truncated backpropagation unrolled optimization process ', 'resulting gradient either strongly biased  short truncation  exploding norm  long truncation  ', 'work propose training scheme overcomes difficulty  dynamically weighting two unbiased gradient estimator variational loss optimizer performance ', 'allows u train neural network perform optimization faster well tuned firstorder method ', 'moreover  training optimizer validation loss  opposed training loss  able use train model generalize better trained first order method ', 'demonstrate result problem learned optimizer train convolutional network fifth wallclock time compared tuned firstorder method  improvement']","Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks., Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks., However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice., Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process., The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations)., In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance., This allows us to train neural networks to perform optimization faster than well tuned first-order methods., Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods., We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement",19,6.104166666666667,10.105263157894736
526,"['Asynchronous distributed gradient descent algorithms for training of deep neural\n', 'networks are usually considered as inefficient, mainly because of the Gradient delay\n', 'problem.', 'In this paper, we propose a novel asynchronous distributed algorithm\n', 'that tackles this limitation by well-thought-out averaging of model updates, computed\n', 'by workers.', 'The algorithm allows computing gradients along the process\n', 'of gradient merge, thus, reducing or even completely eliminating worker idle time\n', 'due to communication overhead, which is a pitfall of existing asynchronous methods.\n', 'We provide theoretical analysis of the proposed asynchronous algorithm,\n', 'and show its regret bounds.', 'According to our analysis, the crucial parameter for\n', 'keeping high convergence rate is the maximal discrepancy between local parameter\n', 'vectors of any pair of workers.', 'As long as it is kept relatively small, the\n', 'convergence rate of the algorithm is shown to be the same as the one of a sequential\n', 'online learning.', 'Furthermore, in our algorithm, this discrepancy is bounded\n', 'by an expression that involves the staleness parameter of the algorithm, and is\n', 'independent on the number of workers.', 'This is the main differentiator between\n', 'our approach and other solutions, such as Elastic Asynchronous SGD or Downpour\n', 'SGD, in which that maximal discrepancy is bounded by an expression that\n', 'depends on the number of workers, due to gradient delay problem.', 'To demonstrate\n', 'effectiveness of our approach, we conduct a series of experiments on image\n', 'classification task on a cluster with 4 machines, equipped with a commodity communication\n', 'switch and with a single GPU card per machine.', 'Our experiments\n', 'show a linear scaling on 4-machine cluster without sacrificing the test accuracy,\n', 'while eliminating almost completely worker idle time.', 'Since our method allows\n', 'using commodity communication switch, it paves a way for large scale distributed\n', 'training performed on commodity clusters.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.37037035822868347, 0.06896550953388214, 0.14814814925193787, 0.0714285671710968, 0.07999999821186066, 0.06896550953388214, 0.13333332538604736, 0.23076923191547394, 0.1818181723356247, 0.07999999821186066, 0.0, 0.09090908616781235, 0.0, 0.06451612710952759, 0.0, 0.13793103396892548, 0.08695651590824127, 0.0, 0.0, 0.0714285671710968, 0.0714285671710968, 0.0714285671710968, 0.0714285671710968, 0.07692307233810425, 0.0, 0.0, 0.0952380895614624, 0.13793103396892548, 0.09090908616781235]",H1lo3sC9KX,"['A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.', 'The paper proposes an algorithm to restrict the staleness in asynchronous SGD and provides theoretical analysis', 'Proposes a hybrid-algorithm to eliminate the gradient delay of asynchronous methods.']","['asynchronous distributed gradient descent algorithm training deep neural', 'network usually considered inefficient  mainly gradient delay', 'problem ', 'paper  propose novel asynchronous distributed algorithm', 'tackle limitation wellthoughtout averaging model update  computed', 'worker ', 'algorithm allows computing gradient along process', 'gradient merge  thus  reducing even completely eliminating worker idle time', 'due communication overhead  pitfall existing asynchronous method ', 'provide theoretical analysis proposed asynchronous algorithm ', 'show regret bound ', 'according analysis  crucial parameter', 'keeping high convergence rate maximal discrepancy local parameter', 'vector pair worker ', 'long kept relatively small ', 'convergence rate algorithm shown one sequential', 'online learning ', 'furthermore  algorithm  discrepancy bounded', 'expression involves staleness parameter algorithm ', 'independent number worker ', 'main differentiator', 'approach solution  elastic asynchronous sgd downpour', 'sgd  maximal discrepancy bounded expression', 'depends number worker  due gradient delay problem ', 'demonstrate', 'effectiveness approach  conduct series experiment image', 'classification task cluster 4 machine  equipped commodity communication', 'switch single gpu card per machine ', 'experiment', 'show linear scaling 4machine cluster without sacrificing test accuracy ', 'eliminating almost completely worker idle time ', 'since method allows', 'using commodity communication switch  pave way large scale distributed', 'training performed commodity cluster ']","Asynchronous distributed gradient descent algorithms for training of deep neural
, networks are usually considered as inefficient, mainly because of the Gradient delay
, problem., In this paper, we propose a novel asynchronous distributed algorithm
, that tackles this limitation by well-thought-out averaging of model updates, computed
, by workers., The algorithm allows computing gradients along the process
, of gradient merge, thus, reducing or even completely eliminating worker idle time
, due to communication overhead, which is a pitfall of existing asynchronous methods.
, We provide theoretical analysis of the proposed asynchronous algorithm,
, and show its regret bounds., According to our analysis, the crucial parameter for
, keeping high convergence rate is the maximal discrepancy between local parameter
, vectors of any pair of workers., As long as it is kept relatively small, the
, convergence rate of the algorithm is shown to be the same as the one of a sequential
, online learning., Furthermore, in our algorithm, this discrepancy is bounded
, by an expression that involves the staleness parameter of the algorithm, and is
, independent on the number of workers., This is the main differentiator between
, our approach and other solutions, such as Elastic Asynchronous SGD or Downpour
, SGD, in which that maximal discrepancy is bounded by an expression that
, depends on the number of workers, due to gradient delay problem., To demonstrate
, effectiveness of our approach, we conduct a series of experiments on image
, classification task on a cluster with 4 machines, equipped with a commodity communication
, switch and with a single GPU card per machine., Our experiments
, show a linear scaling on 4-machine cluster without sacrificing the test accuracy,
, while eliminating almost completely worker idle time., Since our method allows
, using commodity communication switch, it paves a way for large scale distributed
, training performed on commodity clusters.",51,5.601374570446735,5.705882352941177
527,"['Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs.', 'However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks.', ""This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models."", 'We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks.', 'We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise.', 'Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference.', 'Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches.', 'As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.2926829159259796, 0.19999998807907104, 0.7272727489471436, 0.1818181723356247, 0.21052631735801697, 0.1860465109348297, 0.18518517911434174, 0.25]",ryetZ20ctX,"['We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.', ""Proposes a regularization scheme to protect quantized neural networks from adversarial attacks using a Lipschitz constant filitering of the inner layers' inpout-output.""]","['neural network quantization becoming industry standard efficiently deploy deep learning model hardware platform  cpu  gpu  tpu  fpgas ', 'however  observe conventional quantization approach vulnerable adversarial attack ', 'paper aim raise people awareness security quantized model  designed novel quantization methodology jointly optimize efficiency robustness deep learning model ', 'first conduct empirical study show vanilla quantization suffers adversarial attack ', 'observe inferior robustness come error amplification effect  quantization operation enlarges distance caused amplified noise ', 'propose novel defensive quantization  dq  method controlling lipschitz constant network quantization  magnitude adversarial noise remains nonexpansive inference ', 'extensive experiment cifar10 svhn datasets demonstrate new quantization method defend neural network adversarial example  even achieves superior robustness fullprecision counterpart  maintaining hardware efficiency vanilla quantization approach ', 'byproduct  dq also improve accuracy quantized model without adversarial attack ']","Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs., However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks., This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models., We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks., We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise., Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference., Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches., As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.",19,6.134020618556701,10.210526315789474
528,"['Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks.', 'However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies.', 'In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.', 'We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.', 'This model can also be encouraged to perform fewer state updates through a budget constraint.', 'We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.', 'Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.']","[0, 0, 0, 1, 0, 0, 0]","[0.1249999925494194, 0.0, 0.09999999403953552, 0.4285714328289032, 0.1764705777168274, 0.260869562625885, 0.0]",HkwVAXyCW,"['A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.', 'Proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time.', 'Proposes a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps.']","['recurrent neural network  rnns  continue show outstanding performance sequence modeling task ', 'however  training rnns long sequence often face challenge like slow inference  vanishing gradient difficulty capturing long term dependency ', 'backpropagation time setting  issue tightly coupled large  sequential computational graph resulting unfolding rnn time ', 'introduce skip rnn model extends existing rnn model learning skip state update shortens effective size computational graph ', 'model also encouraged perform fewer state update budget constraint ', 'evaluate proposed model various task show reduce number required rnn update preserving  sometimes even improving  performance baseline rnn model ', 'source code publicly available http  imatgeupcgithubioskiprnn2017telecombcn ']","Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks., However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies., In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time., We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph., This model can also be encouraged to perform fewer state updates through a budget constraint., We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models., Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.",13,5.9855072463768115,10.615384615384615
529,"['We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.', 'Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement.', 'Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step.', 'Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network.', 'This estimate has the same size and is similar to the momentum variable that is commonly used in SGD.', 'No estimate of the Hessian is maintained.\n', 'We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle.', 'We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning.', ""We also show our optimiser's generality by testing on a large set of randomly-generated architectures.""]","[1, 0, 0, 0, 0, 0, 0, 0, 0]","[0.3529411852359772, 0.0, 0.12765957415103912, 0.045454539358615875, 0.060606054961681366, 0.0, 0.23255813121795654, 0.20512819290161133, 0.06451612710952759]",Sygx4305KQ,"['A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning', 'Choosing direction by using a single step of gradient descent ""towards Newton step"" from an original estimate, and then taking this direction instead of original gradient', 'A new approximate second-order optimization method with low computational cost that replaces the computation of the Hessian matrix with a single gradient step and a warm start strategy.']","['propose fast secondorder method used dropin replacement current deep learning solver ', 'compared stochastic gradient descent  sgd   requires two additional forwardmode automatic differentiation operation per iteration  computational cost comparable two standard forward pass easy implement ', 'method address longstanding issue current secondorder solver  invert approximate hessian matrix every iteration exactly conjugategradient method  procedure much slower sgd step ', 'instead  propose keep single estimate gradient projected inverse hessian matrix  update per iteration two pass network ', 'estimate size similar momentum variable commonly used sgd ', 'estimate hessian maintained ', 'first validate method  called curveball  small problem known solution  noisy rosenbrock function degenerate 2layer linear network   current deep learning solver struggle ', 'train several large model cifar imagenet  including resnet vggf network  demonstrate faster convergence hyperparameter tuning ', 'also show optimiser generality testing large set randomlygenerated architecture ']","We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers., Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement., Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step., Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network., This estimate has the same size and is similar to the momentum variable that is commonly used in SGD., No estimate of the Hessian is maintained.
, We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle., We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning., We also show our optimiser's generality by testing on a large set of randomly-generated architectures.",20,5.648780487804878,10.25
530,"['The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development.', 'However, to push this idea towards practical implementation, we need better models and better ways of training.', 'We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function.', 'We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes.', 'With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.']","[0, 0, 1, 0, 0]","[0.20512819290161133, 0.1111111044883728, 0.29999998211860657, 0.1904761791229248, 0.20000000298023224]",ByxBFsRqYm,"['Attention based model trained with REINFORCE with greedy rollout baseline to learn heuristics with competitive results on TSP and other routing problems', 'Presents an attention-based approach to learning a policy for solving TSP and other routing-type combinatorial optimzation problems.', 'This paper trys to learn heuristics for solving combinatorial optimisation problems']","['recently presented idea learn heuristic combinatorial optimization problem promising save costly development ', 'however  push idea towards practical implementation  need better model better way training ', 'contribute direction  propose model based attention layer benefit pointer network show train model using reinforce simple baseline based deterministic greedy rollout  find efficient using value function ', 'significantly improve recent learned heuristic travelling salesman problem  tsp   getting close optimal result problem 100 node ', 'hyperparameters  learn strong heuristic two variant vehicle routing problem  vrp   orienteering problem  op   stochastic variant  prize collecting tsp  pctsp   outperforming wide range baseline getting result close highly optimized specialized algorithm ']","The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development., However, to push this idea towards practical implementation, we need better models and better ways of training., We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function., We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes., With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.",12,5.490445859872612,13.083333333333334
531,"['We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.', 'While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.', 'Finally, we show its effectiveness on two distinct tasks.']","[1, 0, 0]","[0.0, 0.0, 0.0]",H1OQukZ0-,"['An algorithm for optimizing regularization hyper-parameters during training', 'The paper proposes a way to re-initialize y at each update of lambda and a clipping procedure of y to maintain the stability of the dynamical system.', 'Proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method.', 'Proposes an extension to an existing method to optimize regularization hyperparameters.']","['propose efficient online hyperparameter optimization method us joint dynamical system evaluate gradient respect hyperparameters ', 'similar method usually limited hyperparameters smooth impact model  show apply probability dropout neural network ', 'finally  show effectiveness two distinct task ']","We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters., While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks., Finally, we show its effectiveness on two distinct tasks.",5,5.39344262295082,12.2
532,"['Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks.', 'However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation.', 'We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models.', 'We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n']","[0, 0, 1, 0]","[0.051282044500112534, 0.052631575614213943, 0.11999999731779099, 0.09756097197532654]",ByJHuTgA-,"['Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.', 'This paper describes a comprehensive validation of LSTM-based word and character language models, leading to a significant result in language modeling and a milestone in deep learning.']","['ongoing innovation recurrent neural network architecture provided steady influx apparently stateoftheart result language modelling benchmark ', 'however  evaluated using differing codebases limited computational resource  represent uncontrolled source experimental variation ', 'reevaluate several popular architecture regularisation method largescale automatic blackbox hyperparameter tuning arrive somewhat surprising conclusion standard lstm architecture  properly regularised  outperform recent model ', 'establish new state art penn treebank wikitext2 corpus  well strong baseline hutter prize dataset ']","Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks., However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation., We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models., We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.
",9,6.614583333333333,10.666666666666666
533,"['  Residual and skip connections play an important role in many current\n  generative models.', 'Although their theoretical and numerical advantages\n  are understood, their role in speech enhancement systems has not been\n  investigated so far.', 'When performing spectral speech enhancement,\n  residual connections are very similar in nature to spectral subtraction,\n  which is the one of the most commonly employed speech enhancement approaches.\n  ', 'Highway networks, on the other hand, can be seen as a combination of spectral\n  masking and spectral subtraction.', 'However, when using deep neural networks, such operations would\n  normally happen in a transformed spectral domain, as opposed to traditional speech\n  enhancement where all operations are often done directly on the spectrum.\n  ', 'In this paper, we aim to investigate the role of residual and highway\n  connections in deep neural networks for speech enhancement, and verify whether\n  or not they operate similarly to their traditional, digital signal processing\n  counterparts.', 'We visualize the outputs of such connections, projected back to\n  the spectral domain, in models trained for speech denoising, and show that while\n  skip connections do not necessarily improve performance with regards to the\n  number of parameters, they make speech enhancement models more interpretable.']","[0, 0, 0, 0, 0, 0, 1]","[0.19512194395065308, 0.1702127605676651, 0.22641508281230927, 0.13333332538604736, 0.19999998807907104, 0.12903225421905518, 0.3636363446712494]",rkzeXBDos7,"['We show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the DSP literature.', 'The authors propose incorporating Residual, Highway and Masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task', 'The authors interpret highway, residual and masking connections. ', 'The authors generate their own noisy speech by artificially adding noise from a well established noise data-set to a less know clean speech data-set.']","['residual skip connection play important role many current generative model ', 'although theoretical numerical advantage understood  role speech enhancement system investigated far ', 'performing spectral speech enhancement  residual connection similar nature spectral subtraction  one commonly employed speech enhancement approach ', 'highway network  hand  seen combination spectral masking spectral subtraction ', 'however  using deep neural network  operation would normally happen transformed spectral domain  opposed traditional speech enhancement operation often done directly spectrum ', 'paper  aim investigate role residual highway connection deep neural network speech enhancement  verify whether operate similarly traditional  digital signal processing counterpart ', 'visualize output connection  projected back spectral domain  model trained speech denoising  show skip connection necessarily improve performance regard number parameter  make speech enhancement model interpretable ']","  Residual and skip connections play an important role in many current
  generative models., Although their theoretical and numerical advantages
  are understood, their role in speech enhancement systems has not been
  investigated so far., When performing spectral speech enhancement,
  residual connections are very similar in nature to spectral subtraction,
  which is the one of the most commonly employed speech enhancement approaches.
  , Highway networks, on the other hand, can be seen as a combination of spectral
  masking and spectral subtraction., However, when using deep neural networks, such operations would
  normally happen in a transformed spectral domain, as opposed to traditional speech
  enhancement where all operations are often done directly on the spectrum.
  , In this paper, we aim to investigate the role of residual and highway
  connections in deep neural networks for speech enhancement, and verify whether
  or not they operate similarly to their traditional, digital signal processing
  counterparts., We visualize the outputs of such connections, projected back to
  the spectral domain, in models trained for speech denoising, and show that while
  skip connections do not necessarily improve performance with regards to the
  number of parameters, they make speech enhancement models more interpretable.",20,5.773684210526316,9.5
534,"['Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data.', 'Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient.', 'With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?', 'We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates.', 'We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances.', 'Combining these two innovations, the resulting method is highly efficient and robust.', 'On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.']","[0, 0, 0, 1, 0, 0, 0]","[0.15789473056793213, 0.10810810327529907, 0.10256409645080566, 0.31111109256744385, 0.24137930572032928, 0.1428571343421936, 0.06666666269302368]",B1l08oAct7,"['A method for eliminating gradient variance and automatically tuning priors for effective training of bayesian neural networks', 'Proposes a new approach to perform deterministic variational inference for feed-forward BNN with specific nonlinear activation functions by approximating layerwise moments.', 'The paper considers a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.']","['bayesian neural network  bnns  hold great promise flexible principled solution deal uncertainty learning finite data ', 'among approach realize probabilistic inference deep neural network  variational bayes  vb  theoretically grounded  generally applicable  computationally efficient ', 'wide recognition potential advantage  variational bayes seen limited practical use bnns real application ', 'argue variational inference neural network fragile  successful implementation require careful initialization tuning prior variance  well controlling variance monte carlo gradient estimate ', 'provide two innovation aim turn vb robust inference tool bayesian neural network  first  introduce novel deterministic method approximate moment neural network  eliminating gradient variance  second  introduce hierarchical prior parameter novel empirical bayes procedure automatically selecting prior variance ', 'combining two innovation  resulting method highly efficient robust ', 'application heteroscedastic regression demonstrate good predictive performance alternative approach ']","Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data., Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient., With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?, We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates., We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances., Combining these two innovations, the resulting method is highly efficient and robust., On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.",16,6.198863636363637,11.0
535,"['Skills learned through (deep) reinforcement learning often generalizes poorly\n', 'across tasks and re-training is necessary when presented with a new task.', 'We\n', 'present a framework that combines techniques in formal methods with reinforcement\n', 'learning (RL) that allows for the convenient specification of complex temporal\n', 'dependent tasks with logical expressions and construction of new skills from existing\n', 'ones with no additional exploration.', 'We provide theoretical results for our\n', 'composition technique and evaluate on a simple grid world simulation as well as\n', 'a robotic manipulation task.']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.19999998807907104, 0.08695651590824127, 0.27272728085517883, 0.09090908616781235, 0.08695651590824127, 0.0, 0.0, 0.08695651590824127, 0.0]",HkfwpiA9KX,"[""A formal method's approach to skill composition in reinforcement learning tasks"", 'The paper combines RL and constraints expressed by logical formulas by setting up an automation from scTLTL formulas.', 'Proposes a method that helps to construct policy from learned subtasks on the topic of combining RL tasks with linear temporal logic formulas.']","['skill learned  deep  reinforcement learning often generalizes poorly', 'across task retraining necessary presented new task ', '', 'present framework combine technique formal method reinforcement', 'learning  rl  allows convenient specification complex temporal', 'dependent task logical expression construction new skill existing', 'one additional exploration ', 'provide theoretical result', 'composition technique evaluate simple grid world simulation well', 'robotic manipulation task ']","Skills learned through (deep) reinforcement learning often generalizes poorly
, across tasks and re-training is necessary when presented with a new task., We
, present a framework that combines techniques in formal methods with reinforcement
, learning (RL) that allows for the convenient specification of complex temporal
, dependent tasks with logical expressions and construction of new skills from existing
, ones with no additional exploration., We provide theoretical results for our
, composition technique and evaluate on a simple grid world simulation as well as
, a robotic manipulation task.",10,5.988095238095238,8.4
536,"['The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.\n', 'This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.\n', 'Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.\n', 'Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.\n']","[0, 0, 1, 0]","[0.15789473056793213, 0.1764705777168274, 0.4324324429035187, 0.25]",rJl8FoRcY7,"['Deriving a general formulation of a multi-modal VAE from the joint marginal log-likelihood.', 'Proposes a multi-modal VAE with a variational bound derived from the chain rule.', 'This paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation.']","['application multimodal generative model mean variational auto encoder  vae  upcoming research topic sensor fusion bidirectional modality exchange ', 'contribution give insight learned joint latent representation show expressiveness coherence decisive property multimodal datasets ', 'furthermore  propose multimodal vae derived full joint marginal loglikelihood able learn meaningful representation ambiguous observation ', 'since property multimodal sensor setup essential approach hardly available  also propose technique generate correlated datasets unimodal one ']","The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.
, This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.
, Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.
, Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.
",6,6.13,16.666666666666668
537,"['We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models.', 'Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions.', 'We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning.', 'We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.']","[0, 0, 1, 0]","[0.3199999928474426, 0.1666666567325592, 0.4444444477558136, 0.09756097197532654]",BJ8c3f-0b,"['We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.', 'The paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling.', 'This work proposes auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carto objective based on SMC. ']","['build autoencoding sequential monte carlo  aesmc   method model proposal learning based maximizing lower bound log marginal likelihood broad family structured probabilistic model ', 'approach relies efficiency sequential monte carlo  smc  performing inference structured probabilistic model flexibility deep neural network model complex conditional probability distribution ', 'develop additional theoretical insight introduce new training procedure improves model proposal learning ', 'demonstrate approach provides fast  easytoimplement scalable mean simultaneous model learning proposal adaptation deep generative model ']","We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models., Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions., We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning., We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",5,6.0754716981132075,21.2
538,"['A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.', 'Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.', 'Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.', 'In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.', 'The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.', 'We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.', 'We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ']","[0, 0, 0, 0, 0, 0, 1]","[0.2380952388048172, 0.24390242993831635, 0.1428571343421936, 0.16326530277729034, 0.3461538553237915, 0.25531914830207825, 0.35555556416511536]",rJleN20qK7,"['We propose an architecture for learning value functions which allows the use of any linear policy evaluation algorithm in tandem with nonlinear feature learning.', 'The paper proposes a two-timescale framework for learning the value function and a state representation altogether with nonlinear approximators.', 'This paper proposes Two-Timescale Networks (TTNs) and prove the convergence of this method using methods from two time-scale stochastic approximation. ', 'This paper presents a Two-Timescale Network (TTN) that enables linear methods to be used to learn values. ']","['key component many reinforcement learning agent learn value function  either policy evaluation control ', 'many algorithm learning value  however  designed linear function approximation  fixed basis fixed representation ', 'though sound extension nonlinear function approximation  nonlinear gradient temporal difference learning  method largely adopted  eschewed favour simpler sound method like temporal difference learning qlearning ', 'work  provide twotimescale network  ttn  architecture enables linear method used learn value  nonlinear representation learned slower timescale ', 'approach facilitates use algorithm developed linear setting  dataefficient leastsquares method  eligibility trace myriad recently developed linear policy evaluation algorithm  provide nonlinear value estimate ', 'prove convergence ttns  particular care given ensure convergence fast linear component potentially dependent feature provided learned representation ', 'empirically demonstrate benefit ttns  compared nonlinear value function approximation algorithm  policy evaluation control ']","A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control., Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation., Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning., In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale., The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates., We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation., We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ",21,5.984293193717278,9.095238095238095
539,"['Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP).', 'However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable.', 'In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain).', 'To show the effectiveness of our method across different tasks, we examine two settings:', '1) compressing core LSTM layers in Language Models,', '2) compressing biLSTM layers of ELMo~\\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering).', 'The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations.', 'Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.\n\n']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.17391303181648254, 0.19607841968536377, 0.5714285373687744, 0.10526315122842789, 0.0624999962747097, 0.1395348757505417, 0.0, 0.18867923319339752]",BylahsR9tX,"['We propose simple, but effective, low-rank matrix factorization (MF) algorithms to speed up in running time, save memory, and improve the performance of LSTMs.', 'Proposes to accelerate LSTM by using MF as the post-processing compression strategy and conducts extensive experiements to show the performance.']","['largescale long shortterm memory  lstm  cell often building block many stateoftheart algorithm task natural language processing  nlp  ', 'however  lstms known computationally inefficient memory capacity model depends number parameter  inherent recurrence model temporal dependency parallelizable ', 'paper  propose simple  effective  lowrank matrix factorization  mf  algorithm compress network parameter significantly speed lstms almost loss performance  sometimes even gain  ', 'show effectiveness method across different task  examine two setting ', '1  compressing core lstm layer language model ', '2  compressing bilstm layer elmocitep  elmo  evaluate three downstream nlp task  sentiment analysis  textual entailment  question answering  ', 'latter particularly interesting embeddings large pretrained bilstm language model often used contextual word representation ', 'finally  discover matrix factorization performs better general  additive recurrence often important multiplicative recurrence  identify interesting correlation matrix norm compression performance ']","Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP)., However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable., In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain)., To show the effectiveness of our method across different tasks, we examine two settings:, 1) compressing core LSTM layers in Language Models,, 2) compressing biLSTM layers of ELMo~\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering)., The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations., Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.

",19,6.1722222222222225,9.473684210526315
540,"['Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  ', 'Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.', 'This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.', 'We propose a solution based on a 3-branch Siamese Convolutional Neural Network.', 'The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.', 'Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.', 'We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.']","[1, 0, 0, 0, 0, 0, 0]","[0.1666666567325592, 0.13636362552642822, 0.1428571343421936, 0.06896550953388214, 0.1395348757505417, 0.1463414579629898, 0.14999999105930328]",rJxY_oCqKQ,"['A forensic metric to determine if a given image is a copy (with possible manipulation) of another image from a given dataset.', 'Introduces the siamese network to identify duplicate and copied/modified images, which can be used to improve surveillance of the published and in-peer-review literature.', 'The paper presents an application of deep convolutional networks for the task of duplicate image detection', 'This work addresses the problem of finding duplicate/near duplicate images from biomedical publications and proposes a standard CNN and loss functions and apply it to this field.']","['manipulation reuse image scientific publication recurring problem  present lacking scalable solution ', 'existing tool detecting image duplication mostly manual semiautomated  despite fact generating data learningbased approach straightforward  illustrate ', 'paper address problem determining  given two image  one manipulated version mean certain geometric statistical manipulation  eg  copy  rotation  translation  scale  perspective transform  histogram adjustment  partial erasing  compression artifact ', 'propose solution based 3branch siamese convolutional neural network ', 'convnet model trained map image 128dimensional space  euclidean distance duplicate  respectively  unique  image greater  respectively  greater  1 ', 'result suggest approach serve tool improve surveillance published inpeerreview literature image manipulation ', 'also show byproduct network learns useful representation semantic segmentation  performance comparable domainspecific model ']","Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  , Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate., This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts., We propose a solution based on a 3-branch Siamese Convolutional Neural Network., The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1., Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation., We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.",24,5.988372093023256,6.88
541,"['Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.', 'The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.', 'In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.', 'Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  ', 'Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.', 'We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.']","[0, 0, 1, 0, 0, 0]","[0.1395348757505417, 0.20408162474632263, 0.31111109256744385, 0.260869562625885, 0.1111111044883728, 0.2083333283662796]",SJahqJZAW,"['Stable GAN training in high dimensions by using an array of discriminators, each with a low dimensional view of generated samples', 'The paper proposes to stabilize GAN training by using an ensemble of discriminators, each working on a random projection of the input data, to provide the training signal for the generator model.', 'The paper proposes a GAN training method for improving the training stability. ', 'The paper proposes a new approach to GAN training, which provides stable gradients to train the generator.']","['training generative adversarial network unstable highdimensions true data distribution tends concentrated small fraction ambient space ', 'discriminator quickly able classify nearly generated sample fake  leaving generator without meaningful gradient causing deteriorate point training ', 'work  propose training single generator simultaneously array discriminator  look different random lowdimensional projection data ', 'individual discriminator  provided restricted view input  unable reject generated sample perfectly continue provide meaningful gradient generator throughout training ', 'meanwhile  generator learns produce sample consistent full data distribution satisfy discriminator simultaneously ', 'demonstrate practical utility approach experimentally  show able produce image sample higher quality traditional training single discriminator ']","Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space., The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training., In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data., Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  , Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously., We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",13,5.886075949367089,12.153846153846153
542,"['We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).', 'Inclusion relations among N-balls implicitly encode subordinate relations among categories.', 'The similarity measurement in terms of the cosine function is enriched by category information.', 'Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.', 'A new benchmark data set is created for validating the category of unknown words.', 'Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.', 'Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}.']","[1, 0, 0, 0, 0, 0, 0]","[0.21621620655059814, 0.09090908616781235, 0.0714285671710968, 0.21276594698429108, 0.0, 0.04878048226237297, 0.0]",rJlWOj0qF7,"['we show a geometric method to perfectly encode categroy tree information into pre-trained word-embeddings.', 'The paper proposes N-ball embedding for taxonomic data where an N-ball is a pair of a centroid vector and the radius from the center.', 'The paper presents a method for tweaking existing vector embeddings of categorical objects (such as words), to convert them to ball embeddings that follow hierarchies.', 'Focuses on adjusting the pretrained word embeddings so that they respect the hypernymy/hyponymy relationship by appropriate n-ball encapsulation.']","['present novel method precisely impose treestructured category information onto wordembeddings  resulting ball embeddings higher dimensional space  nballs short  ', 'inclusion relation among nballs implicitly encode subordinate relation among category ', 'similarity measurement term cosine function enriched category information ', 'using geometric construction method instead backpropagation  create large nball embeddings satisfy two condition   1  category tree precisely imposed onto word embeddings zero energy cost   2  pretrained word embeddings well preserved ', 'new benchmark data set created validating category unknown word ', 'experiment show nball embeddings  carrying category information  significantly outperform word embeddings test nearest neighborhood  demonstrate surprisingly good performance validating category unknown word ', 'source code datasets free public access url  http  githubcomgnodisnaitnball4treegit  url  http  githubcomgnodisnaitbp94nballgit  ']","We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short)., Inclusion relations among N-balls implicitly encode subordinate relations among categories., The similarity measurement in terms of the cosine function is enriched by category information., Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved., A new benchmark data set is created for validating the category of unknown words., Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words., Source codes and data-sets are free for public access \url{https://github.com/gnodisnait/nball4tree.git} and \url{https://github.com/gnodisnait/bp94nball.git}.",12,6.957142857142857,11.666666666666666
543,"['For the challenging semantic image segmentation task the best performing models\n', 'have traditionally combined the structured modelling capabilities of Conditional\n', 'Random Fields (CRFs) with the feature extraction power of CNNs.', 'In more recent\n', 'works however, CRF post-processing has fallen out of favour.', 'We argue that this\n', 'is mainly due to the slow training and inference speeds of CRFs, as well as the\n', 'difficulty of learning the internal CRF parameters.', 'To overcome both issues we\n', 'propose to add the assumption of conditional independence to the framework of\n', 'fully-connected CRFs.', 'This allows us to reformulate the inference in terms of\n', 'convolutions, which can be implemented highly efficiently on GPUs.Doing so\n', 'speeds up inference and training by two orders of magnitude.', 'All parameters of\n', 'the convolutional CRFs can easily be optimized using backpropagation.', 'Towards\n', 'the goal of facilitating further CRF research we have made our implementations\n', 'publicly available.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705181121826, 0.14814814925193787, 0.0, 0.0, 0.1818181723356247, 0.08695651590824127, 0.0, 0.08695651590824127, 0.0, 0.09090908616781235, 0.0]",H1xEwsR9FX,"['We propose Convolutional CRFs a fast, powerful and trainable alternative to Fully Connected CRFs.', 'The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel and show that inference is more efficient and training is easier. ', 'Proposes to perform message passing on a truncated Gaussian kernel CRF using a defined kernel and parallelized message passing on GPU.']","['challenging semantic image segmentation task best performing model', 'traditionally combined structured modelling capability conditional', 'random field  crfs  feature extraction power cnns ', 'recent', 'work however  crf postprocessing fallen favour ', 'argue', 'mainly due slow training inference speed crfs  well', 'difficulty learning internal crf parameter ', 'overcome issue', 'propose add assumption conditional independence framework', 'fullyconnected crfs ', 'allows u reformulate inference term', 'convolution  implemented highly efficiently gpusdoing', 'speed inference training two order magnitude ', 'parameter', 'convolutional crfs easily optimized using backpropagation ', 'towards', 'goal facilitating crf research made implementation', 'publicly available ']","For the challenging semantic image segmentation task the best performing models
, have traditionally combined the structured modelling capabilities of Conditional
, Random Fields (CRFs) with the feature extraction power of CNNs., In more recent
, works however, CRF post-processing has fallen out of favour., We argue that this
, is mainly due to the slow training and inference speeds of CRFs, as well as the
, difficulty of learning the internal CRF parameters., To overcome both issues we
, propose to add the assumption of conditional independence to the framework of
, fully-connected CRFs., This allows us to reformulate the inference in terms of
, convolutions, which can be implemented highly efficiently on GPUs.Doing so
, speeds up inference and training by two orders of magnitude., All parameters of
, the convolutional CRFs can easily be optimized using backpropagation., Towards
, the goal of facilitating further CRF research we have made our implementations
, publicly available.",22,5.641379310344828,6.590909090909091
544,"['Deep Learning NLP domain lacks procedures for the analysis of model robustness.', 'In this paper we propose a framework which validates robustness of any Question Answering model through model explainers.', 'We propose that output of a robust model should be invariant to alterations that do not change its semantics.', 'We test this property by manipulating question in two ways: swapping important question word for', '1) its semantically correct synonym and', '2) for word vector that is close in embedding space.', 'We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME).', 'With these two steps we compare state-of-the-art Q&A models.', 'We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input.', 'We can choose architecture that is more immune to attacks and thus more robust and stable in production environment.', 'Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure.', 'Our findings help to understand which models are more stable and how they can be improved.', 'In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.19999998807907104, 0.2857142686843872, 0.3333333134651184, 0.0624999962747097, 0.0833333283662796, 0.0, 0.11764705181121826, 0.2222222238779068, 0.2702702581882477, 0.17142856121063232, 0.1621621549129486, 0.1764705777168274, 0.3684210479259491]",SJlgRqjssQ,"['We propose a model agnostic approach to validation of Q&A system robustness and demonstrate results on state-of-the-art Q&A models.', 'Addresses the problem of robustness to adversarial information in question answering.', 'Improving robustness of machine comprehension/question answering.']","['deep learning nlp domain lack procedure analysis model robustness ', 'paper propose framework validates robustness question answering model model explainers ', 'propose output robust model invariant alteration change semantics ', 'test property manipulating question two way  swapping important question word', '1  semantically correct synonym', '2  word vector close embedding space ', 'estimate importance word asked question locally interpretable model agnostic explanation method  lime  ', 'two step compare stateoftheart q  model ', 'show although accuracy stateoftheart model high  fragile change input ', 'choose architecture immune attack thus robust stable production environment ', 'morevoer  propose 2 adversarial training scenario raise model sensitivity true synonym 7  accuracy measure ', 'finding help understand model stable improved ', 'addition  created published new dataset may used validation robustness q  model ']","Deep Learning NLP domain lacks procedures for the analysis of model robustness., In this paper we propose a framework which validates robustness of any Question Answering model through model explainers., We propose that output of a robust model should be invariant to alterations that do not change its semantics., We test this property by manipulating question in two ways: swapping important question word for, 1) its semantically correct synonym and, 2) for word vector that is close in embedding space., We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME)., With these two steps we compare state-of-the-art Q&A models., We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input., We can choose architecture that is more immune to attacks and thus more robust and stable in production environment., Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure., Our findings help to understand which models are more stable and how they can be improved., In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.",16,5.233830845771144,12.5625
545,"['In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution.', 'In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions.', 'To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure.', 'To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel.', 'This means that our work can be scaled up to large parallel computation frameworks.', 'We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator.', 'We also show how to introduce the decay factor to stabilize the training procedure.', 'We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10.', 'These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.054054051637649536, 0.1463414579629898, 0.11764705181121826, 0.19999998807907104, 0.07999999821186066, 0.2222222238779068, 0.17391303181648254, 0.09090908616781235, 0.08888888359069824]",rJHcpW-CW,"['multi generator to capture Pdata, solve the competition and one-beat-all problem', 'Proposes parallel GANs to avoid mode collapse in GANs through a combination of multiple weak generators. ']","['paper  propose mixgenerator generative adversarial network  pgan  model work parallel mixing multiple disjoint generator approximate complex real distribution ', 'model  propose adjustment component collect generated data point generator  learns boundary pair generator  provides error separate support generated distribution ', 'overcome instability multiplayer game  shrinkage adjustment component method introduced gradually reduce boundary generator training procedure ', 'address linearly growing training time problem multiple generator model  propose method train generator parallel ', 'mean work scaled large parallel computation framework ', 'present efficient loss function discriminator  effective adjustment component  suitable generator ', 'also show introduce decay factor stabilize training procedure ', 'performed extensive experiment synthetic datasets  mnist  cifar10 ', 'experiment reveal error provided adjustment component could successfully separate generated distribution generator stably learn part real distribution even mode contained real distribution ']","In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution., In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions., To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure., To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel., This means that our work can be scaled up to large parallel computation frameworks., We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator., We also show how to introduce the decay factor to stabilize the training procedure., We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10., These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution.",19,5.598130841121495,11.263157894736842
546,"['We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.', 'The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.', 'We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.', 'However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.', 'Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.']","[1, 0, 0, 0, 0]","[0.3448275923728943, 0.06666666269302368, 0.1666666567325592, 0.05714285373687744, 0.19354838132858276]",SyYYPdg0-,"['Weakly-supervised image segmentation using compositional structure of images and generative models.', 'This paper creates a layered representation in order to better learn segmentation from unlabeled images.', 'This paper proposes a GAN-based generative model that decomposes images into multiple layers, where the objective of the GAN is to distinguish real images from images formed by combining the layers.', 'This paper proposes a neural network architecture around the idea of layered scene composition']","['capitalize natural compositional structure image order learn object segmentation weakly labeled image ', 'intuition behind approach removing object image yield natural image  however removing random patch yield unnatural image ', 'leverage signal develop generative model decomposes image layer  layer combined  reconstructs input image ', 'however  layer removed  model learns produce different image still look natural adversary  possible removing object ', 'experiment visualization suggest model automatically learns object segmentation image labeled scene better baseline ']","We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images., The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images., We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image., However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects., Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.",11,5.547826086956522,10.454545454545455
547,"['Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models.', 'We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples.', 'In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples.', 'Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly.', 'Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.']","[0, 1, 0, 0, 0]","[0.2380952388048172, 0.3589743673801422, 0.3478260934352875, 0.19999998807907104, 0.28070175647735596]",H1lug3R5FX,"['We present a geometric framework for proving robustness guarantees and highlight the importance of codimension in adversarial examples. ', 'This paper gives a theoretical analysis of adversarial examples, showing that  there exists a tradeoff between robustness in different norms, adversarial training is sample inefficient, and the nearest neighbor classifier can be robust under certain conditions.']","['adversarial example pervasive phenomenon machine learning model seemingly imperceptible perturbation input lead misclassifications otherwise statistically accurate model ', 'propose geometric framework  drawing tool manifold reconstruction literature  analyze highdimensional geometry adversarial example ', 'particular  highlight importance codimension  lowdimensional data manifold embedded highdimensional space many direction manifold construct adversarial example ', 'adversarial example natural consequence learning decision boundary classifies lowdimensional data manifold well  classifies point near manifold incorrectly ', 'using geometric framework prove  1  tradeoff robustness different norm   2  adversarial training ball around data sample inefficient   3  sufficient sampling condition nearest neighbor classifier ballbased adversarial training robust ']","Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models., We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples., In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples., Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly., Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.",11,6.507042253521127,12.909090909090908
548,"['Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  ', 'Unfortunately, they are also very brittle and easily falter when presented with noisy data.', 'In this paper, we confront NMT models with synthetic and natural sources of noise.', 'We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.', 'We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.', 'We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.']","[0, 1, 0, 0, 0, 0]","[0.0, 0.11764705926179886, 0.0, 0.0, 0.0, 0.07999999821186066]",BJ8vJebC-,"['CharNMT is brittle', 'This paper investigates the impact of character-level noise on 4 different neural machine translation systems', 'This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural.', 'This paper investigates the impact of noisy input on Machine Translation and tests ways to make NMT models more robust']","['characterbased neural machine translation  nmt  model alleviate outofvocabulary issue  learn morphology  move u closer completely endtoend translation system ', 'unfortunately  also brittle easily falter presented noisy data ', 'paper  confront nmt model synthetic natural source noise ', 'find stateoftheart model fail translate even moderately noisy text human trouble comprehending ', 'explore two approach increase model robustness  structureinvariant word representation robust training noisy text ', 'find model based character convolutional neural network able simultaneously learn representation robust multiple kind noise ']","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  , Unfortunately, they are also very brittle and easily falter when presented with noisy data., In this paper, we confront NMT models with synthetic and natural sources of noise., We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending., We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts., We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",10,6.093457943925234,10.7
549,"['As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.', 'However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.', 'We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.', 'The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.', 'Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.', 'Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.', 'Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.']","[0, 0, 1, 0, 0, 0, 0]","[0.1492537260055542, 0.1395348757505417, 0.3396226465702057, 0.12765957415103912, 0.0476190410554409, 0.1599999964237213, 0.15686273574829102]",B1Lc-Gb0Z,"['We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet.', 'The paper explains and generalizes approaches for learning neural nets with hard activation.', 'This paper examines the problem of optimizing deep networks of hard-threshold units.', 'The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it with a collection of heuristics/approximations.']","['neural network grow deeper wider  learning network hardthreshold activation becoming increasingly important  network quantization  drastically reduce time energy requirement  creating large integrated system deep network  may nondifferentiable component must avoid vanishing exploding gradient effective learning ', 'however  since gradient descent applicable hardthreshold function  clear learn principled way ', 'address problem observing setting target hardthreshold hidden unit order minimize loss discrete optimization problem  solved ', 'discrete optimization goal find set target unit  including output  linearly separable problem solve ', 'given target  network decomposes individual perceptrons  learned standard convex approach ', 'based  develop recursive minibatch algorithm learning deep hardthreshold network includes popular poorly justified straightthrough estimator special case ', 'empirically  show algorithm improves classification accuracy number setting  including alexnet resnet18 imagenet  compared straightthrough estimator ']","As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning., However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way., We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such., The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve., Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches., Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case., Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.",23,5.8,8.695652173913043
550,"['The robust and efficient recognition of visual relations in images is a hallmark of biological vision.', 'Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations.', 'Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs).', 'The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity.', 'We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations.', 'Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.']","[0, 0, 0, 0, 0, 1]","[0.1904761791229248, 0.16326530277729034, 0.20000000298023224, 0.08888888359069824, 0.19672130048274994, 0.22641508281230927]",HymuJz-A-,"['Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies.', 'Demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. ', ""This paper explores how current CNN's and Relational Networks fail to recognize visual relations in images.""]","['robust efficient recognition visual relation image hallmark biological vision ', ' argue  despite recent progress visual recognition  modern machine vision algorithm severely limited ability learn visual relation ', 'controlled experiment  demonstrate visualrelation problem strain convolutional neural network  cnns  ', 'network eventually break altogether rote memorization becomes impossible intraclass variability exceeds capacity ', 'show another type feedforward network  called relational network  rn   shown successfully solve seemingly difficult visual question answering  vqa  problem clevr datasets  suffers similar limitation ', 'motivated comparable success biological vision  argue feedback mechanism including working memory attention key computational component underlying abstract visual reasoning ']","The robust and efficient recognition of visual relations in images is a hallmark of biological vision., Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations., Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs)., The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity., We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations., Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.",14,6.368421052631579,9.5
551,"['Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.', 'Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.', 'However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the trackers generalization on the unseen object moving patterns.', 'To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.', 'In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.', 'They are asymmetric in that the target is aware of the tracker, but not vice versa.', 'Specifically, besides its own observation, the target is fed with the trackers observation and action, and learns to predict the trackers reward as an auxiliary task.', 'We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.', 'To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.', 'The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.', 'For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS', '\n The code is available at https://github.com/zfw1226/active_tracking_rl']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.1428571343421936, 0.19512194395065308, 0.12765957415103912, 0.12765957415103912, 0.23076923191547394, 0.1666666567325592, 0.1860465109348297, 0.09999999403953552, 0.11428570747375488, 0.12244897335767746, 0.0, 0.0]",HkgYmhR9KX,"['We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training.', 'This work aims to address the visual active tracking problem with a training mechanism in which the tracker and target serve as mutual opponents', 'This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target.', 'Proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.']","['visual active tracking  vat  aim following target object autonomously controlling motion system tracker given visual observation ', 'previous work shown tracker trained simulator via reinforcement learning deployed realworld scenario ', 'however  training  method requires manually specifying moving path target object tracked  ensure tracker  generalization unseen object moving pattern ', 'learn robust tracker vat  paper  propose novel adversarial rl method adopts asymmetric dueling mechanism  referred advat ', 'advat  tracker target approximated endtoend neural network  trained via rl duelingcompetitive manner  ie  tracker intends lockup target  target try escape tracker ', 'asymmetric target aware tracker  vice versa ', 'specifically  besides observation  target fed tracker  observation action  learns predict tracker  reward auxiliary task ', 'show asymmetric dueling mechanism produce stronger target  turn induces robust tracker ', 'stabilize training  also propose novel partial zerosum reward trackertarget ', 'experimental result  2d 3d environment  demonstrate proposed method lead faster convergence training yield robust tracking behavior different testing scenario ', 'supplementary video  see  http  wwwyoutubecomplaylist  listpl9rzj4mea7wozkdajk1tsprrg8iuf51bs', 'code available http  githubcomzfw1226activetrackingrl']","Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations., Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios., However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the trackers generalization on the unseen object moving patterns., To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT., In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker., They are asymmetric in that the target is aware of the tracker, but not vice versa., Specifically, besides its own observation, the target is fed with the trackers observation and action, and learns to predict the trackers reward as an auxiliary task., We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker., To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target., The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios., For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS, 
 The code is available at https://github.com/zfw1226/active_tracking_rl",31,5.696153846153846,8.387096774193548
552,"['Identifying the hypernym relations that hold between words is a fundamental task in NLP.', 'Word embedding methods have recently shown some capability to encode hypernymy.', 'However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.', 'In this paper, we propose a method to learn a hierarchical word embedding in a specic order to capture the hypernymy.', 'To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.', 'The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.', 'Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-specic word embeddings on multiple benchmarks.']","[0, 0, 0, 0, 1, 0, 0]","[0.277777761220932, 0.1818181723356247, 0.21621620655059814, 0.29999998211860657, 0.3199999928474426, 0.2666666507720947, 0.21052631735801697]",S1xf-W5paX,"['We presented a method to jointly learn a Hierarchical Word Embedding (HWE) using a corpus and a taxonomy for identifying the hypernymy relations between words.', 'The paper presents a method to jointly learn word embeddings using co-occurrence statistics as well as incorporating hierarchal information from semantic networks.', 'This paper proposed a joint learning method of hypernym from both raw text and supervised taxonomy data. ', 'This paper proposes adding a measure of ""distributional inclusion"" difference to the GloVE objective for the purpose of representing hypernym relations.']","['identifying hypernym relation hold word fundamental task nlp ', 'word embedding method recently shown capability encode hypernymy ', 'however  method tend explicitly encode hypernym hierarchy exists word ', 'paper  propose method learn hierarchical word embedding specic order capture hypernymy ', 'learn word embeddings  proposed method considers hypernym relation exists word taxonomy  also contextual information large text corpus ', 'experimental result supervised hypernymy detection newlyproposed hierarchical path completion task show ability proposed method encode hierarchy ', 'moreover  proposed method outperforms previously proposed method learning word hypernymspecic word embeddings multiple benchmark ']","Identifying the hypernym relations that hold between words is a fundamental task in NLP., Word embedding methods have recently shown some capability to encode hypernymy., However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words., In this paper, we propose a method to learn a hierarchical word embedding in a specic order to capture the hypernymy., To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus., The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy., Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-specic word embeddings on multiple benchmarks.",12,5.698529411764706,11.333333333333334
553,"['While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.', 'Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.', 'Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.', 'Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.']","[0, 0, 1, 0]","[0.2142857164144516, 0.2142857164144516, 0.42424240708351135, 0.2222222238779068]",BJ8lbVAfz,"['integration of self-organization and supervised learning in a hierarchical neural network', 'The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized and investigates interplay between unsupervised and hierarchical supervised learning in biological context.', ""A supervised variant of Kohonen's self-organizing map (SOM), but where the linear output layer is replaced with squared error by a softmax layer with cross-entropy."", 'Proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function. ']","['selforganizing principle motivated much early learning model  principle rarely included deep learning architecture ', 'indeed  supervised learning perspective seems topographic constraint rather decremental optimal performance ', 'study network model incorporates selforganizing map supervised network show gradient learning result form selforganizing learning rule ', 'moreover  show model robust sense application variety area  believed hallmark biological learning system ']","While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures., Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance., Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule., Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.",8,5.552083333333333,12.0
554,"['Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.', 'To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.', 'First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.', 'We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.', 'In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.', 'We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.']","[0, 1, 0, 0, 0, 0]","[0.19512194395065308, 0.24242423474788666, 0.12903225421905518, 0.13333332538604736, 0.09756097197532654, 0.1875]",SJx94o0qYX,"['precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization ', 'Investigates the problem of neural network quantization by employing an end-to-end precision highway to reduce the accumulated quantization error and enable ultra-low precision in deep neural networks. ', 'This paper studies methods to improve the performance of quantized neural networks', 'This paper proposes to keep a high activation/gradient flow in two kinds of networks structures, ResNet and LSTM.']","['quantization neural network inherent problem called accumulated quantization error  key obstacle towards ultralow precision  eg  2 3bit precision ', 'resolve problem  propose precision highway  form endtoend highprecision information flow performing ultralowprecision computation ', 'first  describe precision highway reduce accumulated quantization error convolutional recurrent neural network ', 'also provide quantitative analysis benefit precision highway evaluate overhead stateoftheart hardware accelerator ', 'experiment  proposed method outperforms best existing quantization method offering 3bit weightactivation quantization accuracy loss 2bit quantization 245  top1 accuracy loss resnet50 ', 'also report proposed method significantly outperforms existing method 2bit quantization lstm language modeling ']","Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision., To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation., First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks., We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator., In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50., We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.",13,6.177304964539007,10.846153846153847
555,"['The vast majority of natural sensory data is temporally redundant.', 'For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  ', 'Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  ', 'This can be an obscene waste of energy.  ', 'We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  ', 'We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  ', 'Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  ', 'We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.2857142686843872, 0.0, 0.0, 0.0, 0.277777761220932, 0.0, 0.0, 0.08510638028383255]",HkZy-bW0-,"['An algorithm for training neural networks efficiently on temporally redundant data.', 'The paper describes a neural coding scheme for spike based learning in deep neural networks', '\nThis paper presents a method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data.', 'This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network, combining the three components in a way not seen previously.']","['vast majority natural sensory data temporally redundant ', 'instance  video frame audio sample sampled nearby point time tend similar value ', 'typically  deep learning algorithm take advantage redundancy reduce computation ', 'obscene waste energy ', 'present variant backpropagation neural network computation scale rate change data  rate process data ', 'implementing form predictive coding wherein neuron communicate combination state  temporal change state  quantize signal using sigmadelta modulation ', 'intriguingly  simple communication rule give rise unit resemble biologicallyinspired leaky integrateandfire neuron  spiketimingdependent weightupdate similar spiketiming dependent plasticity  stdp   synaptic learning rule observed brain ', 'demonstrate mnist  temporal variant mnist  youtubebb  dataset video wild  algorithm performs well standard deep network trained backpropagation  despite communicating discrete value layer ']","The vast majority of natural sensory data is temporally redundant., For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  , Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  , This can be an obscene waste of energy.  , We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  , We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  , Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  , We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ",20,5.5132275132275135,9.45
556,"['Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).', 'The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - I(X;T).', 'In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of ; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.', 'We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.', 'To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.', 'We demonstrate the three caveats on the MNIST dataset.']","[0, 0, 0, 1, 0, 0]","[0.1599999964237213, 0.24390242993831635, 0.20000000298023224, 0.29999998211860657, 0.1764705777168274, 0.08695651590824127]",rke4HiAcY7,"['Information bottleneck behaves in surprising ways whenever the output is a deterministic function of the input.', 'Argues that most real classification problems show such a deterministic relation between the class labels and the inputs X and explores several issues that result from such pathologies.', 'Explores issues that arise when applying information bottlenext concepts to deterministic supervised learning models', 'The authors clarify several counter-intuitive behaviors of the information bottleneck method for supervised learning of a deterministic rule.']","['information bottleneck  ib  method extracting information one random variable x relevant predicting another random variable   ib identifies intermediate  bottleneck  variable low mutual information  x   high mutual information    ', ' ib curve  characterizes set bottleneck variable achieve maximal    given  x    typically explored maximizing  ib lagrangian        x   ', 'case  deterministic function x  including many classification problem supervised learning output class deterministic function input x  demonstrate three caveat using ib situation deterministic function x   1  ib curve recovered maximizing ib lagrangian different value    2   uninteresting  trivial solution point ib curve   3  multilayer classifier achieve low prediction error  different layer exhibit strict tradeoff compression prediction  contrary recent proposal ', 'also show small perturbation away deterministic function x  three caveat arise approximate way ', 'address problem  1   propose functional  unlike ib lagrangian  recover ib curve case ', 'demonstrate three caveat mnist dataset ']","Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T)., The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - I(X;T)., In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of ; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal., We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way., To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases., We demonstrate the three caveats on the MNIST dataset.",17,5.163090128755365,12.263157894736842
557,"['We prove, under two sufficient conditions, that idealised models can have no adversarial examples.', 'We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these.', 'We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice.', 'We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting.', 'This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well.', 'Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.']","[1, 0, 0, 0, 0, 0]","[0.42424240708351135, 0.4000000059604645, 0.11764705181121826, 0.178571417927742, 0.16393442451953888, 0.060606054961681366]",B1eZRiC9YX,"['We prove that idealised Bayesian neural networks can have no adversarial examples, and give empirical evidence with real-world BNNs.', 'The paper studies the adversarial robustness of Bayesian classifiers and state two conditions that they show are provably sufficient for ""idealised models"" on ""idealised datasets"" to not have adversarial examples', 'Paper posit a class of discriminative BAyesian classifiers that do not have any adversarial examples.']","['prove  two sufficient condition  idealised model adversarial example ', 'discus idealised model satisfy condition  show idealised bayesian neural network  bnns  satisfy ', 'continue studying nearidealised bnns using hmc inference  demonstrating theoretical idea practice ', 'experiment hmc synthetic data derived mnist know groundtruth image density  showing nearperfect epistemic uncertainty correlate density image manifold  adversarial image lie manifold setting ', 'suggests mc dropout  seen performing approximate inference  observed effective defence adversarial example practice  highlight failurecases nonidealised bnns relying dropout  suggesting new attack dropout model new defence well ', 'lastly  demonstrate defence catsvsdogs image classification task vgg13 variant ']","We prove, under two sufficient conditions, that idealised models can have no adversarial examples., We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these., We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice., We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting., This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well., Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.",16,5.74,9.375
558,"['Deep neural networks are susceptible to adversarial attacks.', 'In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.', 'Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.', 'We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.', 'This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversaryeven if the model was not trained to do this task.', 'These perturbations can thus be considered a program for the new task.', 'We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.']","[0, 0, 0, 1, 0, 0, 0]","[0.15789473056793213, 0.08163265138864517, 0.3333333432674408, 0.8771929740905762, 0.3492063581943512, 0.1904761791229248, 0.2711864411830902]",Syx_Ss05tm,"['We introduce the first instance of adversarial attacks that reprogram the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input.', 'The authors present a novel adversarial attack scheme where a neural net is repurposed to accomplish a different task than the one it was originally trained on', 'This paper proposed ""adversarial reprogramming"" of well-trained and fixed neural networks and show that adversarial reprogramming is less effective on untrained networks.', ""The paper extends the idea of 'adversarial attacks' in supervised learning of NNs to a full repurposing of the solution of a trained net.""]","['deep neural network susceptible adversarial attack ', 'computer vision  wellcrafted perturbation image cause neural network make mistake confusing cat computer ', 'previous adversarial attack designed degrade performance model cause machine learning model produce specific output chosen ahead time attacker ', 'introduce attack instead reprogram target model perform task chosen attacker without attacker needing specify compute desired output testtime input ', 'attack find single adversarial perturbation  added testtime input machine learning model order cause model perform task chosen adversaryeven model trained task ', 'perturbation thus considered program new task ', 'demonstrate adversarial reprogramming six imagenet classification model  repurposing model perform counting task  well classification task  classification mnist cifar10 example presented input imagenet model ']","Deep neural networks are susceptible to adversarial attacks., In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer., Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker., We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input., This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversaryeven if the model was not trained to do this task., These perturbations can thus be considered a program for the new task., We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.",11,5.366666666666666,16.363636363636363
559,"['As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data.', 'This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization.', 'This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters.', 'In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap.', 'Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary.', 'We find that it is necessary to use margin distributions at multiple layers of a deep network.', 'On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap.', 'In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\n', 'Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.']","[0, 0, 0, 0, 0, 0, 0, 0, 1]","[0.25641024112701416, 0.1249999925494194, 0.22857142984867096, 0.2631579041481018, 0.11764705181121826, 0.24242423474788666, 0.25806450843811035, 0.14035087823867798, 0.2857142686843872]",HJlQfnCqKX,"['We develop a new scheme to predict the generalization gap in deep networks with high accuracy.', 'Authors suggest using a geometric margin and layer-wise margin distribution for predicting generalization gap.', 'Empirically shows an interesting connection between the proposed margin statistics and the generalization gap, which can be used to provide some prescriptive insights towards understanding generalization in deep neural nets. ']","['shown recent research  deep neural network perfectly fit randomly labeled data  poor accuracy held data ', 'phenomenon indicates loss function crossentropy reliable indicator generalization ', 'lead crucial question generalization gap predicted training data network parameter ', 'paper  propose measure  conduct extensive empirical study well predict generalization gap ', 'measure based concept margin distribution  distance training point decision boundary ', 'find necessary use margin distribution multiple layer deep network ', 'cifar10 cifar100 datasets  proposed measure correlate strongly generalization gap ', 'addition  find following factor importance  normalizing margin value scale independence  using characterization margin distribution rather margin  closest distance decision boundary   working log space instead linear space  effectively using product margin rather sum  ', 'measure easily applied feedforward deep network architecture may point towards new training loss function could enable better generalization ']","As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data., This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization., This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters., In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap., Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary., We find that it is necessary to use margin distributions at multiple layers of a deep network., On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap., In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).
, Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.",18,5.397196261682243,11.88888888888889
560,"['We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned.', 'Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks.', 'Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems.', 'We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.', 'While our focus is theoretical, we also present experiments that illustrate our theoretical findings.']","[1, 0, 0, 0, 0]","[0.4000000059604645, 0.21621620655059814, 0.19999998807907104, 0.1538461446762085, 0.0]",rkMnHjC5YQ,"['We propose an algorithm for provably recovering parameters (convolutional and output weights) of a convolutional network with overlapping patches.', 'This paper studies the theoretical learning of one-hidden-layer convolutional neural nets, resulting in a learning algorithm and provable guarantees using the algorithm.', 'This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations.']","['propose new algorithm learn onehiddenlayer convolutional neural network convolutional weight output weight parameter learned ', 'algorithm work general class  potentially overlapping  patch  including commonly used structure computer vision task ', 'algorithm draw idea  1  isotonic regression learning neural network  2  landscape analysis nonconvex matrix factorization problem ', 'believe finding may inspire development designing provable algorithm learning neural network complex model ', 'focus theoretical  also present experiment illustrate theoretical finding ']","We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned., Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks., Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems., We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models., While our focus is theoretical, we also present experiments that illustrate our theoretical findings.",7,6.17,14.285714285714286
561,"['Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data.', 'Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem.', 'A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping.', ""Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training."", 'We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.', 'These arguments are supported by experimental results on several data sets.']","[0, 0, 0, 1, 0, 0]","[0.0624999962747097, 0.09302325546741486, 0.1818181723356247, 0.21052631735801697, 0.14814814925193787, 0.0]",B1hYRMbCW,"['A new regularization term can improve your training of wasserstein gans', 'The paper proposes a regularization scheme for Wasserstein GAN based on relaxation of the constraints on the Lipschitz constant of 1.', 'The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric.']","['since invention  generative adversarial network  gans  become popular approach learning model distribution real  unlabeled  data ', 'convergence problem training overcome wasserstein gans minimize distance model empirical distribution term different metric  thereby introduce lipschitz constraint optimization problem ', 'simple way enforce lipschitz constraint class function  modeled neural network  weight clipping ', 'augmenting loss regularization term penalizes deviation gradient norm critic  function network input  one  proposed alternative improves training ', 'present theoretical argument using weaker regularization term enforcing lipschitz constraint preferable ', 'argument supported experimental result several data set ']","Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data., Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem., A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping., Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training., We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable., These arguments are supported by experimental results on several data sets.",11,5.645833333333333,13.090909090909092
562,"['We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \n', 'The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.', 'From this theory, we obtain an easy-to-implement regularizer for the parameter updates.', ""Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves.""]","[1, 0, 0, 0]","[0.5925925970077515, 0.05714285373687744, 0.1818181723356247, 0.23529411852359772]",Bye5OiR5F7,"['We propose the Wasserstein proximal method for training GANs. ', 'Proposes a new GAN procedure that takes into account points generated in the previous iteration and updates the generator to be carried out l times.', 'Considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed.', 'The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN and the authors propose the Wasserstein proximal operator as a regularization.']","['introduce new method training gans applying wasserstein2 metric proximal generator ', 'approach based gradient operator induced optimal transport  connects geometry sample space parameter space implicit deep generative model ', 'theory  obtain easytoimplement regularizer parameter update ', 'experiment demonstrate method improves speed stability training gans term wallclock time frechet inception distance  fid  learning curve ']","We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. 
, The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models., From this theory, we obtain an easy-to-implement regularizer for the parameter updates., Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\'echet Inception Distance (FID) learning curves.",6,5.719512195121951,13.666666666666666
563,"['Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.\n', 'MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.\n', 'The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.\n', 'In this paper, we develop a weighted mini-bucket approach for bounding the MEU.', ' These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter', '.\nThey can also be used as heuristic  functions to guide search, especially for planning \n', 'such as MDPs and POMDPs.\n', 'We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.\n']","[1, 0, 0, 0, 0, 0, 0, 0]","[0.20408162474632263, 0.1621621549129486, 0.0, 0.060606054961681366, 0.0555555522441864, 0.11764705181121826, 0.0, 0.0]",r1ls4-DpvN,"['This paper introduces an elimination based heuristic function for sequential decision making, suitable for guiding AND/OR search algorithms for solving influence diagrams.', 'generalizes minibuckets inference heuristic to influence diagrams.']","['influence diagram provide modeling inference framework sequential decision problem  representing probabilistic knowledge bayesian network preference agent utility function random variable decision variable ', 'mdps pomdps  widely used planning uncertainty also represented influence diagram ', 'time space complexity computing maximum expected utility  meu  maximizing policy exponential induced width underlying graphical model  often prohibitively large due growth information set sequence decision ', 'paper  develop weighted minibucket approach bounding meu ', 'bound used standalone approximation improved function controlling ibound parameter', ' also used heuristic function guide search  especially planning', 'mdps pomdps ', 'evaluate scheme empirically stateoftheart  thus illustrating potential ']","Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.
, MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.
, The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.
, In this paper, we develop a weighted mini-bucket approach for bounding the MEU.,  These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter, .
They can also be used as heuristic  functions to guide search, especially for planning 
, such as MDPs and POMDPs.
, We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.
",14,5.54320987654321,10.8
564,"['Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\n', 'In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity.', 'In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\n', 'Methods propagating also the variance have been proposed by several authors in different context.', 'The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\n', 'The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\n', 'We evaluate the accuracy of the approximation and suggest a simple calibration.', 'Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.09090908616781235, 0.19607841968536377, 0.05882352590560913, 0.1111111044883728, 0.1304347813129425, 0.2545454502105713, 0.1818181723356247, 0.2380952388048172]",SkMuPjRcKQ,"['Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.', 'The authors focus on the problem of uncertainty propagation DNN', 'This paper revisits the feed-forward propagation of mean and variance in neurons, by addressing the problem of propagating uncertainty through max-pooling layers and softmax.']","['probabilistic neural network deal various source stochasticity  input noise  dropout  stochastic neuron  parameter uncertainty modeled random variable  etc ', 'paper revisit feedforward propagation approach allows one estimate neuron mean variance wrt  mentioned source stochasticity ', 'contrast  standard nns propagate point estimate  discarding uncertainty ', 'method propagating also variance proposed several author different context ', 'view presented attempt clarify assumption derivation behind method  relate classical nns broaden scope applicability ', 'main technical contribution new approximation distribution argmax maxrelated transforms  allow fully analytic uncertainty propagation network softmax maxpooling layer well leaky relu activation ', 'evaluate accuracy approximation suggest simple calibration ', 'applying method network dropout allows faster training give improved test likelihood without need sampling ']","Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.
, In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity., In contrast, standard NNs propagate only point estimates, discarding the uncertainty.
, Methods propagating also the variance have been proposed by several authors in different context., The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.
, The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.
, We evaluate the accuracy of the approximation and suggest a simple calibration., Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.",16,6.006024096385542,9.764705882352942
565,"['Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. \n', 'However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.', 'Some of the most promising adversarial models today minimize a Wasserstein objective.', 'It is smoother and more stable to optimize.', 'In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.', 'By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.', 'We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.', 'The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.051282044500112534, 0.09756097197532654, 0.1818181723356247, 0.3448275923728943, 0.1860465109348297, 0.380952388048172, 0.20512819290161133, 0.2702702581882477]",HJE6X305Fm,"['A discriminator that is not easily fooled by adversarial example makes GAN training more robust and leads to a smoother objective.', 'This paper proposes a new way to stabilize the training process of GAN by regularizing the Discriminator to be robust to adversarial examples.', '\nThe paper proposes a systematic way of training GANs with robustness regularization terms, allowing for smoother training of GANs. ', 'Presents idea that making a discriminator robust to adversarial perturbations the GAN objective can be made smooth which results in better results both visually and in terms of FID.']","['generative adversarial network one leading tool generative modeling  image editing content creation ', 'however  hard train require delicate balancing act two deep network fighting never ending duel ', 'promising adversarial model today minimize wasserstein objective ', 'smoother stable optimize ', 'paper  show wasserstein distance one large family objective function yield property ', 'making discriminator gan robust adversarial attack turn gan objective smooth stable loss ', 'experimentally show gan objective  including wasserstein gans  benefit adversarial robustness quantitatively qualitatively ', 'training additionally becomes robust suboptimal choice hyperparameters  model architecture  objective function ']","Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. 
, However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel., Some of the most promising adversarial models today minimize a Wasserstein objective., It is smoother and more stable to optimize., In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties., By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss., We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively., The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.",15,5.5928571428571425,9.333333333333334
566,"['We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\n', 'First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\n', 'Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\n', 'Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\n', 'The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\n', 'Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.']","[0, 0, 0, 0, 1, 0]","[0.2222222238779068, 0.1538461446762085, 0.17391303181648254, 0.1111111044883728, 0.25641024112701416, 0.2380952388048172]",By-IifZRW,"['We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.', 'Propose placing Gaussian process priors on the functional form of each activation function in the neural net to learn the form of activation functions.']","['propose method learn stochastic activation function use probabilistic neural network ', 'first  develop framework embed stochastic activation function based gaussian process probabilistic neural network ', 'second  analytically derive expression propagation mean covariance network  thus allowing efficient implementation training without need sampling ', 'third  show apply variational bayesian inference regularize efficiently train model ', 'resulting model deal uncertain input implicitly provides estimate confidence prediction ', 'like conventional neural network scale datasets arbitrary size extended convolutional recurrent connection  desired ']","We propose a method to learn stochastic activation functions for use in probabilistic neural networks.
, First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.
, Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.
, Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.
, The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.
, Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.",11,5.766666666666667,10.909090909090908
567,"['Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice.', 'In this paper, we bridge the gap between these good empirical results \n', 'and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks.', 'More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\n', 'bounded width and small depth can approximate a deep ReLU network in which the dense matrices are\n', 'of low rank.', 'Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks.', 'We support our experimental results with thorough experiments on a large, real world video classification problem.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.14814814925193787, 0.060606054961681366, 0.3870967626571655, 0.3636363446712494, 0.3684210479259491, 0.0833333283662796, 0.2631579041481018, 0.10810810327529907]",SkeUG30cFQ,"['We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.', 'The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks.', 'This paper proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.']","['recent result linear algebra stating matrix decomposed product diagonal circulant matrix lead design compact deep neural network architecture perform well practice ', 'paper  bridge gap good empirical result', 'theoretical approximation capability deep diagonalcirculant relu network ', 'precisely  first demonstrate deep diagonalcirculant relu network', 'bounded width small depth approximate deep relu network dense matrix', 'low rank ', 'based result  provide new bound expressive power universal approximativeness type network ', 'support experimental result thorough experiment large  real world video classification problem ']","Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice., In this paper, we bridge the gap between these good empirical results 
, and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks., More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of
, bounded width and small depth can approximate a deep ReLU network in which the dense matrices are
, of low rank., Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks., We support our experimental results with thorough experiments on a large, real world video classification problem.",12,5.536,10.416666666666666
568,"['Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.', 'However, manual remote piloting of a drone is prone to errors.', 'In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.', 'Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.', 'The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.', 'A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.']","[0, 0, 0, 1, 0, 0]","[0.10810810327529907, 0.23076923191547394, 0.1621621549129486, 0.4583333432674408, 0.04878048226237297, 0.1463414579629898]",BviYjfnIk,"['StarHopper is a novel touch screen interface for efficient and flexible object-centric camera drone navigation', 'The authors outline a new drone control interface StarHopper that they have developed, combining automated and manual piloting into a new hybrid navigation interface and gets rid of the assumption that the target object is already in the drones FOV by using an additional overhead camera.', 'This paper presents StarHopper, a system for semi-automatic drone navigation in the context of remote inspection.', 'Introduces StarHopper, an application that uses computer vision techniques with touch input to support drone piloting with an object-centric approach.']","['camera drone  rapidly emerging technology  offer people ability remotely inspect environment high degree mobility agility ', 'however  manual remote piloting drone prone error ', 'contrast  autopilot system require significant degree environmental knowledge necessarily designed support flexible visual inspection ', 'inspired camera manipulation technique interactive graphic  designed starhopper  novel touch screen interface efficient objectcentric camera drone navigation  user directly specifies navigation drone camera relative specified object interest ', 'system relies minimal environmental information combine manual automated control mechanism give user freedom remotely explore environment efficiency accuracy ', 'lab study show starhopper offer efficiency gain 354  manual piloting  complimented overall user preference towards objectcentric navigation system ']","Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility., However, manual remote piloting of a drone is prone to errors., In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections., Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest., The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy., A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.",14,5.855263157894737,10.857142857142858
569,"['Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations.', 'RNN can capture long-range dependency but is hard to parallelize and not time-efficient.', 'CNN focuses on local dependency but does not perform well on some tasks.', 'SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length.', 'In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding.', 'It requires as little memory as RNN but with all the merits of SAN.', 'Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency.', 'Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required.', 'Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information.', 'On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.']","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]","[0.10810810327529907, 0.05882352590560913, 0.12121211737394333, 0.24390242993831635, 0.31578946113586426, 0.11764705181121826, 0.1249999925494194, 0.15789473056793213, 0.0476190410554409, 0.1818181723356247]",H1cWzoxA-,"['A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks', 'Proposes applyting self-attention at two levels to limit the memory requirement in attention-based models with a negligible impact on speed.', 'This paper introduces bi-directional block self-attention model as a general-purpose encoder for various sequence modeling tasks in NLP']","['recurrent neural network  rnn   convolutional neural network  cnn  selfattention network  san  commonly used produce contextaware representation ', 'rnn capture longrange dependency hard parallelize timeefficient ', 'cnn focus local dependency perform well task ', 'san model dependency via highly parallelizable computation  memory requirement grows rapidly line sequence length ', 'paper  propose model  called  bidirectional block selfattention network  biblosan    rnncnnfree sequence encoding ', 'requires little memory rnn merit san ', 'biblosan split entire sequence block  applies intrablock san block modeling local context  applies interblock san output block capture longrange dependency ', 'thus  san need process short sequence  small amount memory required ', 'additionally  use featurelevel attention handle variation context around word  use forwardbackward mask encode temporal order information ', 'nine benchmark datasets different nlp task  biblosan achieves improves upon stateoftheart accuracy  show better efficiencymemory tradeoff existing rnncnnsan ']","Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations., RNN can capture long-range dependency but is hard to parallelize and not time-efficient., CNN focuses on local dependency but does not perform well on some tasks., SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length., In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding., It requires as little memory as RNN but with all the merits of SAN., Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency., Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required., Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information., On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.",23,5.836734693877551,8.521739130434783
570,"['End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.', 'In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.', 'The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.', 'We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.', 'On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.']","[0, 0, 0, 0, 1]","[0.04878048226237297, 0.21621620655059814, 0.08888888359069824, 0.06451612710952759, 0.260869562625885]",Syl7OsRqY7,"['A new state-of-the-art model for multi-evidence question answering using coarse-grain fine-grain hierarchical attention.', 'Proposes a method for multi-hop QA based on two separate modules (coarse-grained and fine-grained modules).', 'This paper proposes an interesting coarse-grain fine-grain coattention network architecture to address multi-evidence question answering', 'Focuses on multi-choice QA and proposes a coarse-to-fine scoring framework.']","['endtoend neural model made significant progress question answering  however recent study show model implicitly assume answer evidence appear close together single document ', 'work  propose coarsegrain finegrain coattention network  cfc   new question answering model combine information evidence across multiple document ', 'cfc consists coarsegrain module interprets document respect query find relevant answer  finegrain module score candidate answer comparing occurrence across document query ', 'design module using hierarchy coattention selfattention  learn emphasize different part input ', 'qangaroo wikihop multievidence question answering task  cfc obtains new stateoftheart result 706  blind test set  outperforming previous best 3  accuracy despite using pretrained contextual encoders ']","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document., In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents., The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query., We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input., On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",12,5.76,12.5
571,"['Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.', 'In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.', 'We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.', 'We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).', 'We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.']","[0, 1, 0, 0, 0]","[0.1875, 0.2666666507720947, 0.1666666567325592, 0.11428570747375488, 0.1818181723356247]",HyexAiA5Fm,"['We propose new methodology for unbalanced optimal transport using generative adversarial networks.', 'The authors consider the unbalanced optimal transport problem between two measures with different total mass using a stochastic min-max algorithm and local scaling', 'The authors propose an approach to estimate unbalanced optimal transport between sampled measures that scales well in the dimension and in the number of samples.', 'The paper introduces a static formulation for unbalanced optimal transport by learning simultaneously a transport map T and scaling factor xi.']","['generative adversarial network  gans  expressive class neural generative model tremendous success modeling highdimensional continuous measure ', 'paper  present scalable method unbalanced optimal transport  ot  based generativeadversarial framework ', 'formulate unbalanced ot problem simultaneously learning transport map scaling factor push source measure target measure costoptimal manner ', 'provide theoretical justification formulation  showing closely related existing static formulation liero et al   2018  ', 'propose algorithm solving problem based stochastic alternating gradient update  similar practice gans  perform numerical experiment demonstrating methodology applied population modeling ']","Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures., In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework., We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner., We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018)., We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.",9,5.88,12.5
572,"['Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.', 'Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.', 'We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.']","[0, 0, 1]","[0.10810810327529907, 0.25, 0.2926829159259796]",BJxbYoC9FQ,"['We propose a new saliency map extraction method which results in extracting higher quality maps.', 'Proposes a classifier-agnostic method for saliency map extraction.', 'This paper introduces a new saliency map extractor that seems to improve state-of-the-art results.', 'The authors argue that when an extracted saliency map is directly dependent on a model, then it might not be useful for a different classifier, and suggests a scheme to approximate the solution.']","['extracting saliency map  indicate part image important classification  requires many trick achieve satisfactory performance using classifierdependent method ', 'instead  propose classifieragnostic saliency map extraction  find part image classifier could use  one given advance ', 'observe proposed approach extract higher quality saliency map outperforms existing weaklysupervised localization technique  setting new state art result imagenet dataset ']","Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods., Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance., We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.",9,6.1688311688311686,8.555555555555555
573,"['Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs).', 'However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images.', 'To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration.', 'The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances.', 'To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances.', 'We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances.', 'Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases.', 'Code and results are available in https://github.com/sangwoomo/instagan']","[0, 0, 0, 1, 0, 0, 0, 0]","[0.23529411852359772, 0.1249999925494194, 0.2380952388048172, 0.34285715222358704, 0.2222222238779068, 0.307692289352417, 0.1818181723356247, 0.0]",ryxwJhC9YX,"['We propose a novel method to incorporate the set of instance attributes for image-to-image translation.', 'This paper proposes a method -- InstaGAN -- which builds on CycleGAN by taking into account instance information in the form of per-instance segmentation masks, with results that compare favorably to CycleGAN and other baselines.', ' Proposes to add instance-aware segmentation masks for the problem of unpaired image-to-image translation.']","['unsupervised imagetoimage translation gained considerable attention due recent impressive progress based generative adversarial network  gans  ', 'however  previous method often fail challenging case  particular  image multiple target instance translation task involves significant change shape  eg  translating pant skirt fashion image ', 'tackle issue  propose novel method  coined instanceaware gan  instagan   incorporates instance information  eg  object segmentation mask  improves multiinstance transfiguration ', 'proposed method translates image corresponding set instance attribute maintaining permutation invariance property instance ', 'end  introduce context preserving loss encourages network learn identity function outside target instance ', 'also propose sequential minibatch inferencetraining technique handle multiple instance limited gpu memory enhances network generalize better multiple instance ', 'comparative evaluation demonstrates effectiveness proposed method different image datasets  particular  aforementioned challenging case ', 'code result available http  githubcomsangwoomoinstagan']","Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs)., However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images., To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration., The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances., To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances., We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances., Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases., Code and results are available in https://github.com/sangwoomo/instagan",20,6.441340782122905,8.95
574,"['Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  ', 'While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  ', 'In this paper, we provide a new explanation.', 'By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions.', 'We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.\n', 'As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.\n', 'This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  ', 'If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  ', 'By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.']","[0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.11320754140615463, 0.1818181723356247, 0.060606058686971664, 0.1090909019112587, 0.145454540848732, 0.2641509473323822, 0.07999999821186066, 0.10526315122842789, 0.20895521342754364]",rye4g3AqFm,"['The parameter-function map of deep networks is hugely biased; this can explain why they generalize. We use PAC-Bayes and Gaussian processes to obtain nonvacuous bounds.', 'The paper studies the generalization capabilities of deep neural networks, with the help of the PAC-Bayesian learning theory and empirically backed intuitions.', 'This paper proposes an explaination of the generalization behaviors of large over-parameterized neural networks by claiming the parameter-function map in neural networks are biased towards ""simple"" functions and generalization behavior will be good if the target concept is also ""simple"".']","['deep neural network  dnns  generalize remarkably well without explicit regularization even strongly overparametrized regime classical learning theory would instead predict would severely overfit ', 'many proposal kind implicit regularization made rationalise success  consensus fundamental reason dnns strongly overfit ', 'paper  provide new explanation ', 'applying general probabilitycomplexity bound recently derived algorithmic information theory  ait   argue parameterfunction map many dnns exponentially biased towards simple function ', 'provide clear evidence strong simplicity bias model dnn boolean function  well much larger fully connected convolutional network trained cifar10 mnist ', 'target function many real problem expected highly structured  intrinsic simplicity bias help explain deep network generalize well real world problem ', 'picture also facilitates novel pacbayes approach prior taken dnn inputoutput function space  rather conventional prior parameter space ', 'assume training algorithm sample parameter close uniformly within zeroerror region pacbayes theorem used guarantee good expected generalization target function producing highlikelihood training set ', 'exploiting recently discovered connection dnns gaussian process estimate marginal likelihood  produce relatively tight generalization pacbayes error bound correlate well true error realistic datasets mnist cifar10 architecture including convolutional fully connected network ']","Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  , While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  , In this paper, we provide a new explanation., By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions., We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.
, As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.
, This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  , If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  , By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.",16,5.899253731343284,16.75
575,"['We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\n', 'While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\n', 'Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\n', 'We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters.', 'In the variational E-step,\n', 'the latent states are then', ' \noptimized according to a tractable free-energy objective', '. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\n', 'individuals as the (log) joint probabilities given by the used generative model.\n\n', 'As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches).', 'Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood.', 'In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.4888888895511627, 0.22580644488334656, 0.2790697515010834, 0.21052631735801697, 0.0714285671710968, 0.06896551698446274, 0.06451612710952759, 0.3396226465702057, 0.1111111044883728, 0.2641509473323822, 0.2083333283662796, 0.25925925374031067]",SyjjD1WRb,"['We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization', 'The paper presents a combination of evolutionary computation and variational EM for models with binary latent variables represented via a particle-based approximation', 'The paper makes an attempt to tightly integrate expectation-maximization training algorithms with evolutionary algorithms.']","['establish theoretical link evolutionary algorithm variational parameter optimization probabilistic generative model binary hidden variable ', 'novel approach independent actual generative model  use two model investigate applicability scalability  noisyor bayes net  standard example binary data  binary sparse coding  model continuous data  ', 'learning probabilistic generative model first formulated approximate maximum likelihood optimization using variational expectation maximization  em  ', 'choose truncated posterior variational distribution discrete latent state serve variational parameter ', 'variational estep ', 'latent state', 'optimized according tractable freeenergy objective', ' given data point  show evolutionary algorithm used variational optimization loop   considering bitvectors latent state genome individual   b  defining fitness', 'individual  log  joint probability given used generative model ', 'proof concept  apply novel evolutionary em approach optimization parameter noisyor bayes net binary sparse coding artificial real data  natural image patch  ', 'using point mutation singlepoint crossover evolutionary algorithm  find scalable variational em algorithm obtained efficiently improve data likelihood ', 'general believe  link established  standard well recent result field evolutionary optimization leveraged address difficult problem parameter optimization generative model ']","We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.
, While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).

, Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).
, We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters., In the variational E-step,
, the latent states are then,  
optimized according to a tractable free-energy objective, . Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the
, individuals as the (log) joint probabilities given by the used generative model.

, As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches)., Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood., In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.",19,5.845559845559846,12.95
576,"['While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.', 'Performing forward prediction on sequences of distributions has many important applications.', 'However, there are two main challenges in designing a network model for this task.', 'First, neural networks are unable to encode distributions compactly as each node encodes just a real value.', 'A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.', 'However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.', 'In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.', 'The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.', 'Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.']","[0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.09756097197532654, 0.3333333432674408, 0.2222222238779068, 0.06666666269302368, 0.08510638028383255, 0.10526315122842789, 0.19354838132858276, 0.0555555522441864, 0.13793103396892548]",SJlp8sA5Y7,"['We propose an efficient recurrent network model for forward prediction on time-varying distributions.', 'This paper proposes a method for creating neural nets that maps historical distributions onto distributions and applies the method to several distribution prediction tasks.', 'Proposes a Reccurent Distribution Regression Network which uses a recurrent architecture upon a previous model Distribution Regression Network.', 'This paper is on regressing over probability distributions by studying time varying distributions in a recurrent neural network setting']","['deep neural network achieved groundbreaking prediction result many task  class data existing architecture optimal  sequence probability distribution ', 'performing forward prediction sequence distribution many important application ', 'however  two main challenge designing network model task ', 'first  neural network unable encode distribution compactly node encodes real value ', 'recent work distribution regression network  drn  solved problem novel network encodes entire distribution single node  resulting improved accuracy using much fewer parameter neural network ', 'however  despite compact distribution representation  drn address second challenge  need model time dependency sequence distribution ', 'paper  propose recurrent distribution regression network  rdrn  adopts recurrent architecture drn ', 'combination compact distribution representation shared weight architecture across time step make rdrn suitable modeling time dependency distribution sequence ', 'compared neural network drn  rdrn achieves best prediction performance keeping network compact ']","While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions., Performing forward prediction on sequences of distributions has many important applications., However, there are two main challenges in designing a network model for this task., First, neural networks are unable to encode distributions compactly as each node encodes just a real value., A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks., However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions., In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN., The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence., Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.",18,5.875647668393782,10.722222222222221
577,"['We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.', ""By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront."", 'In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.', 'Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).']","[0, 0, 0, 1]","[0.26229506731033325, 0.1111111044883728, 0.178571417927742, 0.2985074520111084]",rJXMpikCZ,"[""A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task."", 'This paper proposes a new method for classifying nodes of a graph, which can be used in semi-supervised scenarios and on a completely new graph. ', 'The paper introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks.', 'Provides a fair and almost comprehensive discussion of the state of art approaches to learning vector representations for the nodes of a graph.']","['present graph attention network  gat   novel neural network architecture operate graphstructured data  leveraging masked selfattentional layer address shortcoming prior method based graph convolution approximation ', 'stacking layer node able attend neighborhood  feature  enable  implicitly  specifying different weight different node neighborhood  without requiring kind computationally intensive matrix operation  inversion  depending knowing graph structure upfront ', 'way  address several key challenge spectralbased graph neural network simultaneously  make model readily applicable inductive well transductive problem ', 'gat model achieved matched stateoftheart result across four established transductive inductive graph benchmark  cora  citeseer pubmed citation network datasets  well proteinprotein interaction dataset  wherein test graph remain unseen training  ']","We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations., By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront., In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems., Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",12,6.2827586206896555,12.083333333333334
578,"['While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints.', 'Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics.', 'Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task.', 'In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.', ""Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network."", ""In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model."", 'In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer.', 'The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network.', 'Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network.', ""Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network."", ""We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.""]","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.1428571343421936, 0.0624999962747097, 0.11428570747375488, 0.11428570747375488, 0.06666666269302368, 0.0, 0.0, 0.12121211737394333, 0.1818181723356247, 0.04878048226237297, 0.1463414579629898]",B1hcZZ-AW,"['A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation', 'This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process', 'Introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model.']","['bigger deeper neural network architecture continue advance stateoftheart many computer vision task  realworld adoption network impeded hardware speed constraint ', 'conventional model compression method attempt address problem modifying architecture manually using predefined heuristic ', 'since space reduced architecture large  modifying architecture deep neural network way difficult task ', 'paper  tackle issue introducing principled method learning reduced network architecture datadriven way using reinforcement learning ', 'approach take larger teacher  network input output compressed student  network derived teacher  network ', 'first stage method  recurrent policy network aggressively remove layer large teacher  model ', 'second stage  another recurrent policy network carefully reduces size remaining layer ', 'resulting network evaluated obtain reward  score based accuracy compression network ', 'approach us reward signal policy gradient train policy find locally optimal student network ', 'experiment show achieve compression rate 10x model resnet34 maintaining similar performance input teacher  network ', 'also present valuable transfer learning result show policy pretrained smaller teacher  network used rapidly speed training larger teacher  network ']","While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints., Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics., Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task., In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning., Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network., In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model., In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer., The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network., Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network., Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network., We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.",16,5.5458167330677295,15.6875
579,"['Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss.', 'However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples.', 'In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.', 'We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset.', 'Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.']","[1, 0, 0, 0, 0]","[0.37288135290145874, 0.11538460850715637, 0.35087719559669495, 0.2181818187236786, 0.19230768084526062]",HJxyAjRcFX,"['We prove that the mode collapse in conditional GANs is largely attributed to a mismatch between reconstruction loss and GAN loss and introduce a set of novel loss functions as alternatives for reconstruction loss.', 'The paper proposes a modification to the traditional conditional GAN objective in order to promote diverse, multimodal generation of images. ', 'This paper proposes an alternative to L1/L2 errors that are used to augment adversarial losses when training conditional GANs.']","['recent advance conditional image generation task  imagetoimage translation image inpainting  largely accounted success conditional gan model  often optimized joint use gan loss reconstruction loss ', 'however  reveal training recipe shared almost existing method cause one critical side effect  lack diversity output sample ', 'order accomplish training stability multimodal output generation  propose novel training scheme new set loss named moment reconstruction loss simply replace reconstruction loss ', 'show approach applicable conditional generation task performing thorough experiment imagetoimage translation  superresolution image inpainting using cityscape celeba dataset ', 'quantitative evaluation also confirm method achieve great diversity output retaining even improving visual fidelity generated sample ']","Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss., However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples., In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss., We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset., Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.",11,5.871621621621622,13.454545454545455
580,"['Generative models are important tools to capture and investigate the properties of complex empirical data.', 'Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data.', 'Does learning the parameters of both architectures obey the same rules?', 'We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.', 'Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference.', 'In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n']","[1, 0, 0, 0, 0, 0]","[0.307692289352417, 0.10256409645080566, 0.1904761791229248, 0.2857142686843872, 0.2083333283662796, 0.05714285373687744]",SySisz-CW,"['We use causal inference to characterise the architecture of generative models', 'This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN.', 'This work exploits the causality principle to quantify how the weights of successive layers adapt to each other.']","['generative model important tool capture investigate property complex empirical data ', 'recent development generative adversarial network  gans  variational autoencoders  vaes  use two similar  textit  reverse   deep convolutional architecture  one generate one extract information data ', 'learning parameter architecture obey rule ', 'exploit causality principle independence mechanism quantify weight successive layer adapt ', 'using recently introduced spectral independence criterion  quantify dependency kernel successive convolutional layer show independent generative process information extraction  line result field causal inference ', 'addition  experiment generation human face suggest independence successive layer generator result improved performance architecture ']","Generative models are important tools to capture and investigate the properties of complex empirical data., Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data., Does learning the parameters of both architectures obey the same rules?, We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other., Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference., In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.
",12,6.109589041095891,12.166666666666666
581,"['Many deep reinforcement learning approaches use graphical state representations,\n', 'this means visually distinct games that share the same underlying structure cannot\n', 'effectively share knowledge.', 'This paper outlines a new approach for learning\n', 'underlying game state embeddings irrespective of the visual rendering of the game\n', 'state.', 'We utilise approaches from multi-task learning and domain adaption in\n', 'order to place visually distinct game states on a shared embedding manifold.', 'We\n', 'present our results in the context of deep reinforcement learning agents.']","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.0952380895614624, 0.25, 0.0, 0.29999998211860657, 0.0, 0.09090908616781235, 0.5, 0.08695651590824127]",BJB7fkWR-,"['An approach to learning a shared embedding space between visually distinct games.', 'A new approach for learning underlying structure of visually distinct games combining convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning and adversarial approach to force the embedding representation to be independent of the visual representation of games', 'Introduces a method to learn a policy on visually distinct games by adapting deep reinforcement learning.', 'This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics']","['many deep reinforcement learning approach use graphical state representation ', 'mean visually distinct game share underlying structure', 'effectively share knowledge ', 'paper outline new approach learning', 'underlying game state embeddings irrespective visual rendering game', 'state ', 'utilise approach multitask learning domain adaption', 'order place visually distinct game state shared embedding manifold ', '', 'present result context deep reinforcement learning agent ']","Many deep reinforcement learning approaches use graphical state representations,
, this means visually distinct games that share the same underlying structure cannot
, effectively share knowledge., This paper outlines a new approach for learning
, underlying game state embeddings irrespective of the visual rendering of the game
, state., We utilise approaches from multi-task learning and domain adaption in
, order to place visually distinct game states on a shared embedding manifold., We
, present our results in the context of deep reinforcement learning agents.",10,6.012658227848101,7.9
582,"['We study discrete time dynamical systems governed by the state equation $h_{t+1}=(Ah_t+Bu_t)$.', 'Here A,B are weight matrices,  is an activation function, and $u_t$ is the input data.', 'This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.', 'We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.', 'We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.', 'Our results apply to increasing activations whose derivatives are bounded away from zero.', 'The analysis is based on', 'i) an SGD convergence result with nonlinear activations and', 'ii) careful statistical characterization of the state vector.', 'Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.2702702581882477, 0.04999999701976776, 0.17777776718139648, 0.24390242993831635, 0.1904761791229248, 0.052631575614213943, 0.0, 0.05882352590560913, 0.1818181723356247, 0.1428571343421936]",rkeMHjR9Ym,"['We study the state equation of a recurrent neural network. We show that SGD can efficiently learn the unknown dynamics from few input/output observations under proper assumptions.', 'The paper studies discrete-time dynamical systems with a non-linear state equation, proving that running SGD on a fixed-length trajectory gives logarithmic convergence.', 'This work considers the problem of learning a non-linear dynamical system in which the output equals the state. ', '\nThis paper studies the ability of SGD to learn dynamics of a linear system and non-linear activation.']","['study discrete time dynamical system governed state equation  h  t1    ahtbut   ', ' b weight matrix   activation function   ut  input data ', 'relation backbone recurrent neural network  eg  lstms  broad application sequential learning task ', 'utilize stochastic gradient descent learn weight matrix finite inputstate trajectory   ut  ht    t0  n  ', 'prove sgd estimate linearly converges ground truth weight using nearoptimal sample size ', 'result apply increasing activation whose derivative bounded away zero ', 'analysis based', ' sgd convergence result nonlinear activation', 'ii  careful statistical characterization state vector ', 'numerical experiment verify fast convergence sgd relu leaky relu consistence theory ']","We study discrete time dynamical systems governed by the state equation $h_{t+1}=(Ah_t+Bu_t)$., Here A,B are weight matrices,  is an activation function, and $u_t$ is the input data., This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks., We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$., We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size., Our results apply to increasing activations whose derivatives are bounded away from zero., The analysis is based on, i) an SGD convergence result with nonlinear activations and, ii) careful statistical characterization of the state vector., Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.",12,5.774436090225564,10.23076923076923
583,"['Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation.', 'The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform.', 'In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.', 'To make this, we propose several novel methods for learning neuron manifold from DNN model.', 'Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data.', 'Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.']","[0, 0, 0, 1, 0, 0]","[0.04999999701976776, 0.0476190447807312, 0.0, 0.17391303181648254, 0.0, 0.07692307233810425]",SJlYcoCcKX,"['A new knowledge distill method for transfer learning', 'The work introduces a knowledge distillation method using the proposed neuron manifold concept. ', 'Proposes a knowledge distilling method in which neural manifold is taken as the transferred knowledge.']","['although deep neural network show extraordinary power various task  feasible deploying large model embedded system due high computational cost storage space limitation ', 'recent work knowledge distillation  kd  aim transferring model knowledge welltrained teacher model small fast student model significantly help extending usage large deep neural network portable platform ', 'paper  show  properly defining neuron manifold deep neuron network  dnn   significantly improve performance student dnn network approximating neuron manifold powerful teacher network ', 'make  propose several novel method learning neuron manifold dnn model ', 'empowered neuron manifold knowledge  experiment show great improvement across variety dnn architecture training data ', 'compared kd method  neuron manifold transfer  nmt  best transfer ability learned feature ']","Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation., The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform., In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network., To make this, we propose several novel methods for learning neuron manifold from DNN model., Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data., Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.",13,5.613924050632911,12.153846153846153
584,"['We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.', 'Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization.', 'At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models.', 'Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively.', 'We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters.', 'Lastly we visualize and discuss the learned features of slimmable networks.', 'Code and models are available at: https://github.com/JiahuiYu/slimmable_networks']","[1, 0, 0, 0, 0, 0, 0]","[1.0, 0.22727271914482117, 0.1599999964237213, 0.24561403691768646, 0.13793103396892548, 0.10526315122842789, 0.05714285373687744]",H1gMCsAqY7,"['We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.', 'The paper proposes an idea of combining different size models together into one shared net, greatly improving performance for detection', 'This paper trains a single network executable at different widths.']","['present simple general method train single neural network executable different width  number channel layer   permitting instant adaptive accuracyefficiency tradeoff runtime ', 'instead training individual network different width configuration  train shared network switchable batch normalization ', 'runtime  network adjust width fly according ondevice benchmark resource constraint  rather downloading offloading different model ', 'trained network  named slimmable neural network  achieve similar  many case better  imagenet classification accuracy individually trained model mobilenet v1  mobilenet v2  shufflenet resnet50 different width respectively ', 'also demonstrate better performance slimmable model compared individual one across wide range application including coco boundingbox object detection  instance segmentation person keypoint detection without tuning hyperparameters ', 'lastly visualize discus learned feature slimmable network ', 'code model available  http  githubcomjiahuiyuslimmablenetworks']","We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime., Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization., At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models., Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively., We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters., Lastly we visualize and discuss the learned features of slimmable networks., Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",16,6.528662420382165,9.8125
585,"['Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval.', 'Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model.', 'In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.']","[0, 0, 1]","[0.1764705777168274, 0.20000000298023224, 0.21739129722118378]",Skvd-myR-,"['Similarity network to learn a non-metric visual similarity estimation between a pair of images', 'The authors propose learning similarity measure for visual similarity and obtain by this an improvement in very well-known datasets of Oxford and Paris for image retrival.', 'The paper argues that it is more suitable to use non-metric distances instead of metric distances.']","['measuring visual  dis  similarity two instance within data distribution fundamental task many application  specially image retrieval ', 'theoretically  nonmetric distance able generate complex accurate similarity model metric distance  provided nonlinear data distribution precisely captured similarity model ', 'work  analyze simple approach deep learning network used approximation nonmetric similarity function study model generalize across different image retrieval datasets ']","Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval., Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model., In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.",7,5.885057471264368,12.428571428571429
586,"['Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.\n', 'However, it is often the case that data are abundant in some domains but scarce in others.', 'Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain.', 'In general, this requires learning plausible mappings between domains.', 'CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint.', 'However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data.', 'In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction.', 'We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised.', 'In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  ', 'Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models.', 'Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation.', 'In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model.', 'Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.']","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.1860465109348297, 0.052631575614213943, 0.23255813121795654, 0.06451612710952759, 0.1395348757505417, 0.04347825422883034, 0.25925925374031067, 0.20512819290161133, 0.27586206793785095, 0.05405404791235924, 0.25641024112701416, 0.15789473056793213, 0.12765957415103912]",B1G9doA9F7,"['A new cyclic adversarial learning augmented with auxiliary task model which improves domain adaptation performance in low resource supervised and unsupervised situations ', 'Proposes an extension of cycle-consistent adversatial adaptation methods in order to tackle domain adaptation where limited supervised target data is available.', 'This paper introduces a domain adaptation approach based on the idea of Cyclic GAN and proposes two different algorithms.']","['training model perform task typically requires large amount data domain task applied ', 'however  often case data abundant domain scarce others ', 'domain adaptation deal challenge adapting model trained datarich source domain perform well datapoor target domain ', 'general  requires learning plausible mapping domain ', 'cyclegan powerful framework efficiently learns map input one domain another using adversarial training cycleconsistency constraint ', 'however  conventional approach enforcing cycleconsistency via reconstruction may overly restrictive case one domain limited training data ', 'paper  propose augmented cyclic adversarial learning model enforces cycleconsistency constraint via external task specific model  encourages preservation taskrelevant content opposed exact reconstruction ', 'explore digit classification lowresource setting supervised  semi unsupervised situation  well high resource unsupervised ', 'lowresource supervised setting  result show approach improves absolute performance 14  4  adapting svhn mnist vice versa  respectively  outperforms unsupervised domain adaptation method require highresource unlabeled target domain ', 'moreover  using unsupervised target data  approach still outperforms many highresource unsupervised model ', 'model also outperforms usps mnist synthetic digit svhn high resource unsupervised adaptation ', 'speech domain  similarly adopt speech recognition model domain task specific model ', 'approach improves absolute performance speech recognition 2  female speaker timit dataset  majority training sample male voice ']","Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.
, However, it is often the case that data are abundant in some domains but scarce in others., Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain., In general, this requires learning plausible mappings between domains., CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint., However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data., In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction., We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised., In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  , Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models., Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation., In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model., Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.",27,5.726027397260274,10.814814814814815
587,"['Nodes residing in different parts of a graph can have similar structural roles within their local network topology.', 'The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs.', 'However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node.', 'Here we develop GraphWave, a method that represents each nodes local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns.', 'We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network.', 'Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features.', 'We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.']","[0, 0, 0, 1, 0, 0, 0]","[0.277777761220932, 0.24390242993831635, 0.24390242993831635, 0.2926829159259796, 0.1904761791229248, 0.17142856121063232, 0.1666666567325592]",rJR2ylbRb,"['We develop a method for learning structural signatures in networks based on the diffusion of spectral graph wavelets.', ""Using spectral graph wavelet diffusion patterns of a node's local meighbothood to embed the node in a low-dimensional space"", 'The paper derived a way to compare nodes in graph based on wavelet analysis of graph laplacian. ']","['node residing different part graph similar structural role within local network topology ', 'identification role provides key insight organization network also used inform machine learning graph ', 'however  learning structural representation node challenging unsupervisedlearning task  typically involves manually specifying tailoring topological feature node ', 'develop graphwave  method represents node  local network neighborhood via lowdimensional embedding leveraging spectral graph wavelet diffusion pattern ', 'prove node similar local network neighborhood similar graphwave embeddings even though node may reside different part network ', 'method scale linearly number edge require handtailoring topological feature ', 'evaluate performance synthetic realworld datasets  obtaining improvement 71  stateoftheart baseline ']","Nodes residing in different parts of a graph can have similar structural roles within their local network topology., The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs., However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node., Here we develop GraphWave, a method that represents each nodes local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns., We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network., Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features., We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.",11,5.980263157894737,13.818181818181818
588,"['Driving simulators play an important role in vehicle research.', 'However, existing virtual reality simulators do not give users a true sense of presence.', 'UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.', 'It is powered by SUMO and Unity.', ""UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management."", 'We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.', ""We present a user study that subjectively measures user's sense of presence in UniNet."", 'Our findings suggest that our novel mixed reality system does increase this sensation.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0714285671710968, 0.12121211737394333, 0.3243243098258972, 0.07692307233810425, 0.051282044500112534, 0.29999998211860657, 0.24242423474788666, 0.1249999925494194]",4ZO8BVlix-,"['A mixed reality driving simulator using stereo cameras and passthrough VR evaluated in a user study with 24 participants.', 'Proposes a complicated system for driving simulation.', 'This paper presents a mixed reality driving simulator setup to enhance the sensation of presence', 'Proposes a mixed reality driving simulator that incorporates traffic generation and claims an enhanced ""presence"" due to an MR system.']","['driving simulator play important role vehicle research ', 'however  existing virtual reality simulator give user true sense presence ', 'uninet driving simulator  designed allow user interact visualize simulated traffic mixed reality ', 'powered sumo unity ', 'uninet modular architecture allows u investigate interdisciplinary research topic vehicular adhoc network  humancomputer interaction  traffic management ', 'accomplish giving user ability observe interact simulated traffic high fidelity driving simulator ', 'present user study subjectively measure user sense presence uninet ', 'finding suggest novel mixed reality system increase sensation ']","Driving simulators play an important role in vehicle research., However, existing virtual reality simulators do not give users a true sense of presence., UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality., It is powered by SUMO and Unity., UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management., We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator., We present a user study that subjectively measures user's sense of presence in UniNet., Our findings suggest that our novel mixed reality system does increase this sensation.",12,5.726495726495727,9.75
589,"['We consider the problem of improving kernel approximation via feature maps.', 'These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.', 'We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.', 'Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.', 'We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.']","[1, 0, 0, 0, 0]","[0.25, 0.23999999463558197, 0.0, 0.09999999403953552, 0.0]",H1U_af-0-,"['Quadrature rules for kernel approximation.', 'The paper proposes improving the kernel approximation of random features by using quadrature rules like stochastic spherical-radial rules.', 'The authors propose a novel version of the random feature map approach to approximately solve large-scale kernel problems.', 'This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature, a new way to apply quadrature rules to improve kernel approximation.']","['consider problem improving kernel approximation via feature map ', 'map arise monte carlo approximation integral representation kernel function scale kernel method larger datasets ', 'propose use efficient numerical integration technique obtain better estimate integral compared stateoftheart method ', 'approach allows use information integrand enhance approximation facilitates fast computation ', 'derive convergence behavior conduct extensive empirical study support hypothesis ']","We consider the problem of improving kernel approximation via feature maps., These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets., We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods., Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations., We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.",5,6.083333333333333,16.8
590,"['Human world knowledge is both structured and flexible.', 'When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts.', 'Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context.', 'Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects.', 'To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts.', 'Using this dataset, we developed neural listener and speaker models with strong capacity for generalization.', 'By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities.', ""We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.""]","[0, 0, 0, 1, 0, 0, 0, 0]","[0.0, 0.060606054961681366, 0.1111111044883728, 0.3333333432674408, 0.1818181723356247, 0.0, 0.11538460850715637, 0.072727270424366]",rkgZ3oR9FX,"['How to build neural-speakers/listeners that learn fine-grained characteristics of 3D objects, from referential language.', 'The authors provide a study on learning to refer to 3D objects, collecting a dataset of referential expressions and training several models by experimenting with a number of architectural choices']","['human world knowledge structured flexible ', 'people see object  represent pixel array meaningful arrangement semantic part ', 'moreover  people refer object  provide description merely true also relevant current context ', ' combine two observation order learn finegrained correspondence language contextually relevant geometric property 3d object ', ' employed interactive communication task human participant construct large dataset containing natural utterance referring 3d object shapenet wide variety context ', 'using dataset  developed neural listener speaker model strong capacity generalization ', 'performing targeted lesion visual linguistic input  discovered neural listener depends heavily partrelated word associate word correctly corresponding geometric property object  suggesting learned taskrelevant structure linking two input modality ', 'show neural speaker  listeneraware    plan utterance according imagined listener would interpret word context   produce discriminative referring expression  listenerunaware  speaker  measured human performance identifying correct object ']","Human world knowledge is both structured and flexible., When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts., Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context., Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects., To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts., Using this dataset, we developed neural listener and speaker models with strong capacity for generalization., By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities., We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.",17,5.786729857819905,12.411764705882353
591,"['Object-based factorizations provide a useful level of abstraction for interacting with the world.', 'Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.', 'We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.', 'Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.', 'For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.', 'After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.']","[0, 0, 1, 0, 0, 0]","[0.19354838132858276, 0.11764705181121826, 0.514285683631897, 0.11764705181121826, 0.3181818127632141, 0.09302324801683426]",HJx9EhC9tQ,"['We present a framework for learning object-centric representations suitable for planning in tasks that require an understanding of physics.', 'The paper presents a platform for predicting images of objects interacting with each other under the effect of gravitational forces.', ""The paper presents a method that learns to reproduce 'block towers' from a given image."", 'Proposes a method which learns to reason on physical interaction of different objects with no supervison of object properties.']","['objectbased factorization provide useful level abstraction interacting world ', 'building explicit object representation  however  often requires supervisory signal difficult obtain practice ', 'present paradigm learning objectcentric representation physical scene understanding without direct supervision object property ', 'model  objectoriented prediction planning  o2p2   jointly learns perception function map image observation object representation  pairwise physic interaction function predict time evolution collection object  rendering function map object back pixel ', 'evaluation  consider accuracy physical prediction model  also utility downstream task require actionable representation intuitive physic ', 'training model image prediction task  use learned representation build block tower complicated observed training ']","Object-based factorizations provide a useful level of abstraction for interacting with the world., Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice., We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties., Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels., For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics., After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.",15,6.027027027027027,9.866666666666667
592,"['We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.', 'Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.', 'For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.', 'Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.', 'We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.']","[0, 0, 0, 0, 1]","[0.3684210479259491, 0.17777776718139648, 0.4444444477558136, 0.3255814015865326, 0.44897958636283875]",BJk7Gf-CZ,"['We provide efficiently checkable necessary and sufficient conditions for global optimality in deep linear neural networks, with some initial extensions to nonlinear settings.', 'The paper gives conditions for the global optimality of the loss function of deep linear neural networks', 'The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks.', 'Studies some theoretical properties of deep linear networks.']","['study error landscape deep linear nonlinear neural network squared error loss ', 'minimizing loss deep linear neural network nonconvex problem  despite recent progress  understanding loss surface still incomplete ', 'deep linear network  present necessary sufficient condition critical point risk function global minimum ', 'surprisingly  condition provide efficiently checkable test global optimality  test typically intractable nonconvex optimization ', 'extend result deep nonlinear neural network prove similar sufficient condition global optimality  albeit limited function space setting ']","We study the error landscape of deep linear and nonlinear neural networks with the squared error loss., Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete., For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum., Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization., We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.",11,5.598214285714286,10.181818181818182
593,"['Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.', 'The summarised information can be used to represent time series features.', 'In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.', 'The fixed-length vector can therefore represent features only in the selected dimensions.', 'In addition, we propose using rolling fixed window approach to generate samples.', 'The change of time series features over time can be summarised as a smooth trajectory path.', 'The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. \n\n', 'This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.']","[0, 1, 0, 0, 0, 0, 0, 0]","[0.12121211737394333, 0.380952388048172, 0.0, 0.09090908616781235, 0.09090908616781235, 0.23999999463558197, 0.0, 0.05128204822540283]",r1cLblgCZ,"['Using recurrent auto-encoder model to extract multidimensional time series features', 'This writeup describes an application of recurrent autoencoder to analyze of multidimensional time series', 'The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations, showing that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. ', 'Proposes a strategy inspired by the recurrent auto-encoder model such that clustering multidimensional time series data can be performed based on context vectors.']","['recurrent autoencoder model summarise sequential data encoder structure fixedlength vector reconstruct original sequential form decoder structure ', 'summarised information used represent time series feature ', 'paper  propose relaxing dimensionality decoder output performs partial reconstruction ', 'fixedlength vector therefore represent feature selected dimension ', 'addition  propose using rolling fixed window approach generate sample ', 'change time series feature time summarised smooth trajectory path ', 'fixedlength vector analysed additional visualisation unsupervised clustering technique ', 'proposed method applied largescale industrial process sensor signal analysis purpose cluster vector representation used reflect operating state selected aspect industrial system ']","Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure., The summarised information can be used to represent time series features., In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction., The fixed-length vector can therefore represent features only in the selected dimensions., In addition, we propose using rolling fixed window approach to generate samples., The change of time series features over time can be summarised as a smooth trajectory path., The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. 

, This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.",10,6.020689655172414,14.5
594,"['We view molecule optimization as a graph-to-graph translation problem.', 'The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.', 'Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.', 'A key challenge is therefore to model diverse translation outputs.', 'Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.', 'Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.', 'We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n']","[0, 0, 0, 0, 1, 0, 0]","[0.29999998211860657, 0.060606054961681366, 0.2142857164144516, 0.0952380895614624, 0.4117647111415863, 0.0, 0.1249999925494194]",B1xJAsA5F7,"['We introduce a graph-to-graph encoder-decoder framework for learning diverse graph translations.', 'Proposes a graph-to-graph translation model for molecule optimization inspired by matched molecular pair analysis.', 'Extension of JT-VAE into the graph to graph translation scenario by adding the latent variable to capture multi-modality and an adversarial regularization in the latent space', 'Proposes a quite complex system, involving many different choices and components, for obtaining chemical compouds with improved properties starting from a given corpora.']","['view molecule optimization graphtograph translation problem ', 'goal learn map one molecular graph another better property based available corpus paired molecule ', 'since molecule optimized different way  multiple viable translation input graph ', 'key challenge therefore model diverse translation output ', 'primary contribution include junction tree encoderdecoder learning diverse graph translation along novel adversarial training method aligning distribution molecule ', 'diverse output distribution model explicitly realized lowdimensional latent vector modulate translation process ', 'evaluate model multiple molecule optimization task show model outperforms previous stateoftheart baseline significant margin ']","We view molecule optimization as a graph-to-graph translation problem., The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules., Since molecules can be optimized in different ways, there are multiple viable translations for each input graph., A key challenge is therefore to model diverse translation outputs., Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules., Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process., We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. 
",8,6.032,15.625
595,"['Partial differential equations (PDEs) are widely used across the physical and computational sciences.', 'Decades of research and engineering went into designing fast iterative solution methods.', 'Existing solvers are general purpose, but may be sub-optimal for specific classes of problems.', 'In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain.', 'We achieve this goal by learning to modify the updates of an existing solver using a deep neural network.', 'Crucially, our approach is proven to preserve strong correctness and convergence guarantees.', 'After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.']","[0, 0, 0, 1, 0, 0, 0]","[0.0, 0.0833333283662796, 0.07692307233810425, 0.2666666507720947, 0.25806450843811035, 0.1666666567325592, 0.0555555522441864]",rklaWn0qK7,"['We learn a fast neural solver for PDEs that has convergence guarantees.', 'Develops a method to accelerate the finite difference method in solving PDEs and proposes a revised framework for fixed point iteration after discretization.', 'The authors propose a linear method for speeding up PDE solvers.']","['partial differential equation  pdes  widely used across physical computational science ', 'decade research engineering went designing fast iterative solution method ', 'existing solver general purpose  may suboptimal specific class problem ', 'contrast existing handcrafted solution  propose approach learn fast iterative solver tailored specific domain ', 'achieve goal learning modify update existing solver using deep neural network ', 'crucially  approach proven preserve strong correctness convergence guarantee ', 'training single geometry  model generalizes wide variety geometry boundary condition  achieves 23 time speedup compared stateoftheart solver ']","Partial differential equations (PDEs) are widely used across the physical and computational sciences., Decades of research and engineering went into designing fast iterative solution methods., Existing solvers are general purpose, but may be sub-optimal for specific classes of problems., In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain., We achieve this goal by learning to modify the updates of an existing solver using a deep neural network., Crucially, our approach is proven to preserve strong correctness and convergence guarantees., After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.",12,5.779661016949152,9.833333333333334
596,"['Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.', 'We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.', 'We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.', 'Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.', 'With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.', 'Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.']","[0, 1, 0, 0, 0, 0]","[0.29999998211860657, 0.4615384638309479, 0.22857142984867096, 0.1621621549129486, 0.13333332538604736, 0.0]",rkxacs0qY7,"['We perform functional variational inference on the stochastic processes defined by Bayesian neural networks.', 'Fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples.', 'Presents a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors features in the literature.', 'Presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally rather than via a prior over weights. ']","['variational bayesian neural network  bnn  perform variational inference weight  difficult specify meaningful prior approximating posterior highdimensional weight space ', 'introduce functional variational bayesian neural network  fbnns   maximize evidence lower bound  elbo  defined directly stochastic process  ie  distribution function ', 'prove kl divergence stochastic process equal supremum marginal kl divergence finite set input ', 'based  introduce practical training objective approximates functional elbo using finite measurement set spectral stein gradient estimator ', 'fbnns  specify prior entail rich structure  including gaussian process implicit stochastic process ', 'empirically  find fbnns extrapolate well using various structured prior  provide reliable uncertainty estimate  scale large datasets ']","Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space., We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions., We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs., Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator., With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes., Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.",15,6.227941176470588,8.5
597,"['Words are not created equal.', 'In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal.', 'In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.', 'This connection allows us to introduce a novel principled hypernymy score for word embeddings.', 'Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds.', 'We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry.', 'Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.', 'In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.']","[0, 0, 1, 0, 0, 0, 0, 0]","[0.0, 0.21621620655059814, 0.3829787075519562, 0.2142857164144516, 0.25, 0.1111111044883728, 0.0952380895614624, 0.06896550953388214]",Ske5r3AqK7,"['We embed words in the hyperbolic space and make the connection  with the Gaussian word embeddings.', 'This paper adapts the Glove word embedding to a hyperbolic space given by the Poincare half-plane model', 'This paper proposes an approach to implement a GLOVE-based hyperbolic word embedding model, which is optimized via the Riemannian Optimization methods.']","['word created equal ', 'fact  form aristocratic graph latent hierarchical structure next generation unsupervised learned word embeddings reveal ', 'paper  justified notion deltahyperbolicity treelikeliness space  propose embed word cartesian product hyperbolic space theoretically connect gaussian word embeddings fisher geometry ', 'connection allows u introduce novel principled hypernymy score word embeddings ', 'moreover  adapt wellknown glove algorithm learn unsupervised word embeddings type riemannian manifold ', 'explain solve analogy task using riemannian parallel transport generalizes vector arithmetic new type geometry ', 'empirically  based extensive experiment  prove embeddings  trained unsupervised  first simultaneously outperform strong popular baseline task similarity  analogy hypernymy detection ', 'particular  word hypernymy  obtain new stateoftheart fully unsupervised wbless classification accuracy ']","Words are not created equal., In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal., In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry., This connection allows us to introduce a novel principled hypernymy score for word embeddings., Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds., We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry., Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection., In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.",19,5.834319526627219,8.894736842105264
598,"['Answering questions about a text frequently requires aggregating information from multiple places in that text.', 'End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.', 'We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.', 'In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.', 'However, with this supervision, it can perform well.', 'On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.', 'A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.', 'We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.']","[0, 0, 0, 0, 0, 0, 0, 1]","[0.0, 0.10526315122842789, 0.1111111044883728, 0.11764705181121826, 0.0, 0.0624999962747097, 0.060606054961681366, 0.25806450843811035]",B1lf43A5Y7,"['Memory Networks do not learn multi-hop reasoning unless we supervise them.', ""Claims multi-hop reasoning is not easy to learn directly and requires direct supervision and doing well on WikiHop doesn't necessarily mean the model is actually learning to hop."", 'The paper proposes to investigate the well-known problem of memory network learning and more precisely the difficulty of the attention learning supervision with such models.', 'This paper argues that memory network fails to learn reasonable multi-hop reasoning.']","['answering question text frequently requires aggregating information multiple place text ', 'endtoend neural network model  dominant approach current literature  theoretically learn distill manipulate representation text without explicit supervision ', 'investigate canonical architecture task  memory network  analyze effective really context three multihop reasoning setting ', 'simple synthetic setting  pathfinding task babi dataset  model fails learn correct reasoning without additional supervision attention mechanism ', 'however  supervision  perform well ', 'real text dataset  wikihop  memory network give nearly stateoftheart performance  without using multihop capability ', 'tougher anonymized version wikihop dataset qualitatively similar babi  model fails perform well unless additional supervision ', 'hypothesize many  multihop  architecture truly learn reasoning advertised  though could learn reasoning appropriately supervised ']","Answering questions about a text frequently requires aggregating information from multiple places in that text., End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so., We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings., In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism., However, with this supervision, it can perform well., On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities., A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision., We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.",20,5.788571428571428,8.75
599,"['Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood.', 'We compute deep convolutional network generators by inverting a fixed embedding operator.', 'Therefore, they do not require to be optimized with a discriminator or an encoder.', 'The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images.', 'This embedding is computed with a wavelet Scattering transform.', 'Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.']","[0, 0, 1, 0, 0, 0]","[0.07547169178724289, 0.2926829159259796, 0.5116279125213623, 0.1599999964237213, 0.31578946113586426, 0.19607841968536377]",r1NYjfbR-,"['We introduce generative networks that do not require to be learned with a discriminator or an encoder; they are obtained by inverting a special embedding operator defined by a wavelet Scattering transform.', 'Introduces scattering transforms as image generative models in the context of Generative Adversarial Networks and suggest why they could be seen as Gaussianization transforms with controlled information loss and invertibility. ', 'The paper proposes a generative model for images that does no require to learn a discriminator (as in GANs) or learned embedding.']","['generative adversarial net  gans  variational autoencoders  vaes  provide impressive image generation gaussian white noise  underlying mathematics well understood ', 'compute deep convolutional network generator inverting fixed embedding operator ', 'therefore  require optimized discriminator encoder ', 'embedding lipschitz continuous deformation generator transform linear interpolation input white noise vector deformation output image ', 'embedding computed wavelet scattering transform ', 'numerical experiment demonstrate resulting scattering generator similar property gans vaes  without learning discriminative network encoder ']","Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood., We compute deep convolutional network generators by inverting a fixed embedding operator., Therefore, they do not require to be optimized with a discriminator or an encoder., The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images., This embedding is computed with a wavelet Scattering transform., Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.",9,6.419047619047619,11.666666666666666
600,"[""Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token."", 'Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.', ""Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input."", 'We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.', 'The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.\n', 'A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that \n', 'Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.']","[0, 0, 0, 1, 0, 0, 0]","[0.2222222238779068, 0.1702127605676651, 0.1860465109348297, 0.4390243887901306, 0.32258063554763794, 0.25, 0.18518517911434174]",B1xf9jAqFQ,"['We propose a new model for neural speed reading that utilizes the inherent punctuation structure of a text to define effective jumping and skipping behavior.', 'The paper proposes a Structural-Jump-LSTM model to speed up machine reading with two agents instead of one', 'Proposes a novel model for neural speed reading in which the new reader has the ability to skip a word or sequence of words.', 'The paper proposes a fast-reading method using skip and jump actions, showing that the proposed method is as accurate as LSTM but uses much less computation.']","['recurrent neural network  rnns  model natural language sequentially  reading  input token outputting distributed representation token ', 'due sequential nature rnns  inference time linearly dependent input length  input read regardless importance ', 'effort speed inference  known  neural speed reading   either ignore skim part input ', 'present structuraljumplstm  first neural speed reading model skip jump text inference ', 'model consists standard lstm two agent  one capable skipping single word reading  one capable exploiting punctuation structure  subsentence separator      sentence end symbol       end text marker  jump ahead reading word ', 'comprehensive experimental evaluation model five stateoftheart neural reading model show', 'structuraljumplstm achieves best overall floating point operation  flop  reduction  hence faster   keeping accuracy even improving compared vanilla lstm read whole text ']","Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token., Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance., Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input., We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference., The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.
, A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that 
, Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.",15,5.56,11.666666666666666
601,"['One of the key challenges of session-based recommender systems is to enhance users purchase intentions.', 'In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP).', 'In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning.', 'Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).', 'Specically, IRN enables the agent to explore its environment and learn predictive representations via three key components.', 'The imagination core generates predicted trajectories, i.e., imagined items that users may purchase.', 'The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards.', 'To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms.', 'Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines.', 'Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can ""guess what you like"" via internal planning.']","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]","[0.27586206793785095, 0.11428570747375488, 0.3243243098258972, 0.31578946113586426, 0.25, 0.06666666269302368, 0.12121211737394333, 0.10256409645080566, 0.2702702581882477, 0.20512820780277252]",SkfTIj0cKX,"['We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.', 'The paper proposes improving the performance of recommendation systems through reinforcement learning by using an Imagination Reconstruction Network.', 'The paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. ']","['one key challenge sessionbased recommender system enhance user  purchase intention ', 'paper  formulate sequential interaction user session recommender agent markov decision process  mdp  ', 'practice  purchase reward delayed sparse  may buried click  making impoverished signal policy learning ', 'inspired prediction error minimization  pem  embodied cognition  propose simple architecture augment reward  namely imagination reconstruction network  irn  ', 'specically  irn enables agent explore environment learn predictive representation via three key component ', 'imagination core generates predicted trajectory  ie  imagined item user may purchase ', 'trajectory manager control granularity imagined trajectory using planning strategy  balance longterm reward shortterm reward ', 'optimize action policy  imaginationaugmented executor minimizes intrinsic imagination error simulated trajectory selfsupervised reconstruction  maximizing extrinsic reward using modelfree algorithm ', 'empirically  irn promotes quicker adaptation user interest  show improved robustness coldstart scenario ultimately higher purchase performance compared several baseline ', 'somewhat surprisingly  irn using purchase reward achieves excellent nextclick prediction performance  demonstrating agent  guess like  via internal planning ']","One of the key challenges of session-based recommender systems is to enhance users purchase intentions., In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP)., In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning., Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN)., Specically, IRN enables the agent to explore its environment and learn predictive representations via three key components., The imagination core generates predicted trajectories, i.e., imagined items that users may purchase., The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards., To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms., Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines., Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can ""guess what you like"" via internal planning.",26,6.4,8.076923076923077
602,"['The question why deep learning algorithms generalize so well has attracted increasing\n', 'research interest.', 'However, most of the well-established approaches,\n', 'such as hypothesis capacity, stability or sparseness, have not provided complete\n', 'explanations (Zhang et al., 2016; Kawaguchi et al., 2017).', 'In this work, we focus\n', 'on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\n', 'will not change much due to perturbations of its training examples, then it\n', 'will also generalize well.', 'As most deep learning algorithms are stochastic (e.g.,\n', 'Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\n', 'arguments of Xu & Mannor, and introduce a new approach  ensemble\n', 'robustness  that concerns the robustness of a population of hypotheses.', 'Through\n', 'the lens of ensemble robustness, we reveal that a stochastic learning algorithm can\n', 'generalize well as long as its sensitiveness to adversarial perturbations is bounded\n', 'in average over training examples.', 'Moreover, an algorithm may be sensitive to\n', 'some adversarial examples (Goodfellow et al., 2015) but still generalize well.', 'To\n', 'support our claims, we provide extensive simulations for different deep learning\n', 'algorithms and different network architectures exhibiting a strong correlation between\n', 'ensemble robustness and the ability to generalize.']","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]","[0.1538461446762085, 0.20000000298023224, 0.0, 0.0, 0.0, 0.19999998807907104, 0.07407406717538834, 0.0, 0.25, 0.25, 0.23076923191547394, 0.260869562625885, 0.37037035822868347, 0.0, 0.0, 0.0, 0.0, 0.1599999964237213, 0.0833333283662796, 0.380952388048172]",r1YUtYx0-,"['Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness', ""This paper presents an adaptation of the algorithmic robustness of Xu&Mannor'12 and presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error. "", 'Proposes a study of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness and gives bounds on generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice.', 'The paper studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context']","['question deep learning algorithm generalize well attracted increasing', 'research interest ', 'however  wellestablished approach ', 'hypothesis capacity  stability sparseness  provided complete', 'explanation  zhang et al  2016  kawaguchi et al  2017  ', 'work  focus', 'robustness approach  xu  mannor  2012   ie  error hypothesis', 'change much due perturbation training example ', 'also generalize well ', 'deep learning algorithm stochastic  eg ', 'stochastic gradient descent  dropout  bayesbybackprop   revisit robustness', 'argument xu  mannor  introduce new approach  ensemble', 'robustness  concern robustness population hypothesis ', '', 'lens ensemble robustness  reveal stochastic learning algorithm', 'generalize well long sensitiveness adversarial perturbation bounded', 'average training example ', 'moreover  algorithm may sensitive', 'adversarial example  goodfellow et al  2015  still generalize well ', '', 'support claim  provide extensive simulation different deep learning', 'algorithm different network architecture exhibiting strong correlation', 'ensemble robustness ability generalize ']","The question why deep learning algorithms generalize so well has attracted increasing
, research interest., However, most of the well-established approaches,
, such as hypothesis capacity, stability or sparseness, have not provided complete
, explanations (Zhang et al., 2016; Kawaguchi et al., 2017)., In this work, we focus
, on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis
, will not change much due to perturbations of its training examples, then it
, will also generalize well., As most deep learning algorithms are stochastic (e.g.,
, Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness
, arguments of Xu & Mannor, and introduce a new approach  ensemble
, robustness  that concerns the robustness of a population of hypotheses., Through
, the lens of ensemble robustness, we reveal that a stochastic learning algorithm can
, generalize well as long as its sensitiveness to adversarial perturbations is bounded
, in average over training examples., Moreover, an algorithm may be sensitive to
, some adversarial examples (Goodfellow et al., 2015) but still generalize well., To
, support our claims, we provide extensive simulations for different deep learning
, algorithms and different network architectures exhibiting a strong correlation between
, ensemble robustness and the ability to generalize.",41,5.755102040816326,4.780487804878049
603,"['Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  ', 'However, such models require many thousands of gradient-based weight updates and unique image examples for training.', 'Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  ', 'In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.', 'Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  ', 'Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.', 'Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.']","[0, 0, 0, 0, 1, 0, 0]","[0.0, 0.0, 0.0, 0.06451612710952759, 0.09999999403953552, 0.0, 0.0]",r1wEFyWCW,"['Few-shot learning PixelCNN', 'The paper proposes on using density estimation when the availability of training data is low by using a meta-learning model.', 'This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning', 'The paper focuses on few shot learning with autoregressive density estimation and improves PixelCNN with neural attention and meta learning techniques.']","['deep autoregressive model shown stateoftheart performance density estimation natural image largescale datasets imagenet ', 'however  model require many thousand gradientbased weight update unique image example training ', 'ideally  model would rapidly learn visual concept handful example  similar manner human learns across many vision task ', 'paper  show 1  neural attention 2  meta learning technique used combination autoregressive model enable effective fewshot density estimation ', 'proposed modification pixelcnn result stateofthe art fewshot density estimation omniglot dataset ', 'furthermore  visualize learned attention policy find learns intuitive algorithm simple task image mirroring imagenet handwriting omniglot without supervision ', 'finally  extend model natural image demonstrate fewshot image generation stanford online product dataset ']","Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  , However, such models require many thousands of gradient-based weight updates and unique image examples for training., Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  , In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation., Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  , Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision., Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.",13,5.888888888888889,11.76923076923077
604,"['Neural networks exhibit good generalization behavior in the\n', 'over-parameterized regime, where the number of network parameters\n', 'exceeds the number of observations.', 'Nonetheless,\n', 'current generalization bounds for neural networks fail to explain this\n', 'phenomenon.', 'In an attempt to bridge this gap, we study the problem of\n', 'learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function.', 'In the case where the network has Leaky\n', 'ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.\n', 'Specifically, we prove convergence rates of SGD to a global\n', 'minimum and provide generalization guarantees for this global minimum\n', 'that are independent of the network size. \n', 'Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model.', 'This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.']","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.07407406717538834, 0.07407406717538834, 0.0, 0.13793103396892548, 0.0, 0.3529411852359772, 0.07692307233810425, 0.1875, 0.06896550953388214, 0.0, 0.07407406717538834, 0.09302324801683426, 0.1666666567325592]",rJ33wwxRb,"['We show that SGD learns two-layer over-parameterized neural networks with Leaky ReLU activations that provably generalize on linearly separable data.', 'The paper studies overparameterised models being able to learn well-generalising solutions by using a 1-hidden layer network with fixed output layer.', 'This paper shows that on linearly seperabel data, SGD on an overparameterized network can still lean a classifier that provably generalizes.']","['neural network exhibit good generalization behavior', 'overparameterized regime  number network parameter', 'exceeds number observation ', 'nonetheless ', 'current generalization bound neural network fail explain', 'phenomenon ', 'attempt bridge gap  study problem', 'learning twolayer overparameterized neural network  data generated linearly separable function ', 'case network leaky', 'relu activation  provide optimization generalization guarantee overparameterized network ', 'specifically  prove convergence rate sgd global', 'minimum provide generalization guarantee global minimum', 'independent network size ', 'therefore  result clearly show use sgd optimization find global minimum  avoids overfitting despite high capacity model ', 'first theoretical demonstration sgd avoid overfitting  learning overspecified neural network classifier ']","Neural networks exhibit good generalization behavior in the
, over-parameterized regime, where the number of network parameters
, exceeds the number of observations., Nonetheless,
, current generalization bounds for neural networks fail to explain this
, phenomenon., In an attempt to bridge this gap, we study the problem of
, learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function., In the case where the network has Leaky
, ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.
, Specifically, we prove convergence rates of SGD to a global
, minimum and provide generalization guarantees for this global minimum
, that are independent of the network size. 
, Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model., This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.",23,5.920529801324503,6.565217391304348
605,"['A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.', 'We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}.', 'These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.', 'We propose to learn about decision states from prior experience.', 'By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.', 'We find that this simple mechanism effectively identifies decision states, even in partially observed settings.', 'In effect, the model learns the sensory cues that correlate with potential subgoals.', 'In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.']","[0, 0, 0, 0, 1, 0, 0, 0]","[0.0, 0.05882352590560913, 0.0, 0.0, 0.1621621549129486, 0.0, 0.07692307233810425, 0.09756097197532654]",rJg8yhAqKm,"['Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus', 'Proposes regularizing standard RL losses with the negative conditional mutual information for policy search in a multi-goal RL setting.', 'This paper proposes the concept of decision state and proposes a KL divergence regularization to learn the structure of the tasks to use this information to encourage the policy to visit the decision states.', 'The paper proposes a method of regularising goal-conditioned policies with a mutual information term. ']","['central challenge reinforcement learning discovering effective policy task reward sparsely distributed ', 'postulate absence useful reward signal  effective exploration strategy seek  decision state  ', 'state lie critical junction state space agent transition new  potentially unexplored region ', 'propose learn decision state prior experience ', 'training goalconditioned model information bottleneck  identify decision state examining model access goal state bottleneck ', 'find simple mechanism effectively identifies decision state  even partially observed setting ', 'effect  model learns sensory cue correlate potential subgoals ', 'new environment  model identify novel subgoals exploration  guiding agent sequence potential decision state new region state space ']","A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed., We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}., These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions., We propose to learn about decision states from prior experience., By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck., We find that this simple mechanism effectively identifies decision states, even in partially observed settings., In effect, the model learns the sensory cues that correlate with potential subgoals., In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.",15,5.7254901960784315,10.2
606,"['Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.', 'This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).', 'We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.', 'We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.', 'This allows us to estimate a descent direction which can then be passed to a first-order optimizer.', 'We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.', 'Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).', 'We provide a demo of Guided ES at: redacted URL']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.0952380895614624, 0.21212120354175568, 0.25, 0.2448979616165161, 0.08695651590824127, 0.2857142686843872, 0.2461538463830948, 0.1463414579629898]",B1xFxh0cKX,"['We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.', 'The authors propose an approach that combines random search with the surrogate gradient information and give a discussion on variance-bias trade-off as well as a discussion on hyperparameter optimization.', ' The paper proposes a method to improve random search by building a subspace of the previous k surrogate gradients.', 'This paper attempts accelerating the OpenAI type evolution by introducing a non-isotrophic distribution with a covariance matrix in the form I + UU^t and external information such as a surrogate gradient to determine U']","['many application machine learning require optimizing function whose true gradient unknown  surrogate gradient information  direction may correlated  necessarily identical  true gradient  available instead ', 'arises approximate gradient easier compute full gradient  eg  metalearning unrolled optimization   true gradient intractable replaced surrogate  eg  certain reinforcement learning application training network discrete variable  ', 'propose guided evolutionary strategy  method optimally using surrogate gradient direction along random search ', 'define search distribution evolutionary strategy elongated along subspace spanned surrogate gradient ', 'allows u estimate descent direction passed firstorder optimizer ', 'analytically numerically characterize tradeoff result tuning strongly search distribution stretched along guiding subspace  use derive setting hyperparameters work well across problem ', 'finally  apply method example problem including truncated unrolled optimization training neural network discrete variable  demonstrating improvement standard evolutionary strategy firstorder method  directly follow surrogate gradient  ', 'provide demo guided e  redacted url']","Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead., This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables)., We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search., We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients., This allows us to estimate a descent direction which can then be passed to a first-order optimizer., We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems., Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient)., We provide a demo of Guided ES at: redacted URL",16,5.935185185185185,12.0
607,"['Point clouds are an important type of geometric data and have widespread use in computer graphics and vision.', 'However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space.', 'Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation.', 'This paper studies the unsupervised problem of a generative model exploiting graph convolution.', 'We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator.', 'The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry.', 'We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.']","[0, 0, 0, 0, 1, 0, 0]","[0.0, 0.0, 0.11999999731779099, 0.1538461446762085, 0.1621621549129486, 0.13793103396892548, 0.0]",SJeXSo09FQ,"['A GAN using graph convolution operations with dynamically computed graphs from hidden features', 'The paper proposes a version of GANs specifically designed for generating point clouds with the core contribution of the work the upsampling operation.', 'This paper proposes graph-convolutional GANs for irregular 3D point clouds that learn domain and features at the same time.']","['point cloud important type geometric data widespread use computer graphic vision ', 'however  learning representation point cloud particularly challenging due nature unordered collection point irregularly distributed 3d space ', 'graph convolution  generalization convolution operation data defined graph  recently shown successful extracting localized feature point cloud supervised semisupervised task classification segmentation ', 'paper study unsupervised problem generative model exploiting graph convolution ', 'focus generator gan define method graph convolution graph known advance output generator ', 'proposed architecture learns generate localized feature approximate graph embeddings output geometry ', 'also study problem defining upsampling layer graphconvolutional generator  learns exploit selfsimilarity prior data distribution sample effectively ']","Point clouds are an important type of geometric data and have widespread use in computer graphics and vision., However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space., Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation., This paper studies the unsupervised problem of a generative model exploiting graph convolution., We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator., The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry., We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.",11,5.604651162790698,15.636363636363637
608,"['Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples.', 'However, mislabeled examples are to hard avoid in extremely large datasets.', 'We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics.', 'We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses.', 'This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead.', 'Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.']","[0, 0, 0, 0, 1, 0]","[0.0, 0.0833333283662796, 0.13333332538604736, 0.27586206793785095, 0.3333333432674408, 0.06896550953388214]",HyGDdsCcFQ,"['We introduce a fast and easy-to-implement algorithm that is robust to dataset noise.', 'The paper aims to remove potential examples with label noise by discarding the ones with large losses in the training procedure.']","['memorization overparameterized neural network severely hurt generalization presence mislabeled example ', 'however  mislabeled example hard avoid extremely large datasets ', 'address problem using implicit regularization effect stochastic gradient descent large learning rate  find able separate clean mislabeled example remarkable success using loss statistic ', 'leverage identify onthefly discard mislabeled example using threshold loss ', 'lead onthefly data denoising  odd   simple yet effective algorithm robust mislabeled example  introducing almost zero computational overhead ', 'empirical result demonstrate effectiveness odd several datasets containing artificial realworld mislabeled example ']","Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples., However, mislabeled examples are to hard avoid in extremely large datasets., We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics., We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses., This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead., Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.",10,6.230769230769231,11.7
609,"['Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.', 'Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.', 'Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.\n', 'The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.', 'In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.', 'We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.', 'While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.', 'To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.', 'Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.']","[0, 0, 0, 0, 1, 0, 0, 0, 0]","[0.10256409645080566, 0.1538461446762085, 0.09302324801683426, 0.13636362552642822, 0.2857142686843872, 0.1621621549129486, 0.05128204822540283, 0.1860465109348297, 0.145454540848732]",S1lTEh09FQ,"['Gradient-based attacks on binarized neural networks are not effective due to the non-differentiability of such networks; Our IPROP algorithm solves this problem using integer optimization', 'Proposes a new target propagation style algorithm to generate strong adversarial attacks on binarized neural networks.', 'This paper proposed a new attack algorithm based on MILP on binary neural networks.', 'This paper presents an algorithm to find adversarial attacks to binary neural networks which iteratively finds desired representations layer by layer from the top to the input and is more efficient than solving the full mixed integer linear programming (MILP) solver.']","['binarized neural network  bnns  recently attracted significant interest due computational efficiency ', 'concurrently  shown neural network may overly sensitive  attack   tiny adversarial change input  may detrimental use safetycritical domain ', 'designing attack algorithm effectively fool trained model key step towards learning robust neural network ', 'discrete  nondifferentiable nature bnns  distinguishes fullprecision counterpart  pose challenge gradientbased attack ', 'work  study problem attacking bnn lens combinatorial integer optimization ', 'propose mixed integer linear programming  milp  formulation problem ', 'exact flexible  milp quickly becomes intractable network perturbation space grow ', 'address issue  propose iprop  decompositionbased algorithm solves sequence much smaller milp problem ', 'experimentally  evaluate proposed method standard gradientbased attack  pgd  mnist fashionmnist  show iprop performs favorably compared pgd  scaling beyond limit milp ']","Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency., Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains., Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.
, The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks., In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization., We propose a Mixed Integer Linear Programming (MILP) formulation of the problem., While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow., To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems., Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.",20,5.867403314917127,9.05
610,"['Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.', 'We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.', 'This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.', 'With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.', 'We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.', 'In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.', 'These results constitute a new state-of-the-art in their respective settings.']","[0, 1, 0, 0, 0, 0, 0]","[0.19354838132858276, 0.5405405163764954, 0.2222222238779068, 0.19230768084526062, 0.1860465109348297, 0.1621621549129486, 0.1428571343421936]",SklckhR5Ym,"['Decoding the last token in the context using the predicted next token distribution acts as a regularizer and improves language modeling.', 'The authors introduce the idea of past decoding for the purpose of regularization for improved perplexity on Penn Treebank', 'Proposes an additional loss term to use when training an LSTM LM and shows that by adding this loss term they can achieve SOTA perplexity on a number of LM benchmarks.', 'Suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.']","['highly regularized lstms achieve impressive result several benchmark datasets language modeling ', 'propose new regularization method based decoding last token context using predicted distribution next token ', 'bias model towards retaining contextual information  turn improving ability predict next token ', 'negligible overhead number parameter training time  past decode regularization  pdr  method achieves word level perplexity 556 penn treebank 635 wikitext2 datasets using single softmax ', 'also show gain using pdr combination mixtureofsoftmaxes  achieving word level perplexity 538 605 datasets ', 'addition  method achieves 1169 bitspercharacter penn treebank character dataset character level language modeling ', 'result constitute new stateoftheart respective setting ']","Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling., We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token., This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token., With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax., We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets., In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling., These results constitute a new state-of-the-art in their respective settings.",11,5.641379310344828,13.181818181818182
611,"['The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.', 'Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.', 'Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.', 'Specifically, we assume that the instances are sampled from a Markov chain.', 'Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.', 'One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.', 'This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.', 'To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  ', 'Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.']","[0, 0, 0, 0, 0, 0, 1, 0, 0]","[0.0, 0.1875, 0.12121211737394333, 0.08695651590824127, 0.060606054961681366, 0.11428570747375488, 0.19354838132858276, 0.06451612710952759, 0.0952380895614624]",rJ695PxRW,"['Propose to observe implicit orders in datasets in a generative model viewpoint.', 'The authors deal with the problem of implicit ordering in a dataset and the challenge of recovering it and propose to learn a distance-metric-free model that assumes a Markov chain as the generative mechanism of the data ', 'The paper proposes Generative Markov Networks - a deep-learning-based approach to modeling sequences and discovering order in datasets.', 'Proposes learning the order of an unordered data sample by learning a Markov chain.']","['assumption data sample independently identically distributed backbone many learning algorithm ', 'nevertheless  datasets often exhibit rich structure practice  argue exist unknown order within data instance ', 'aiming find order  introduce novel generative markov network  gmn  use extract order data instance automatically ', 'specifically  assume instance sampled markov chain ', 'goal learn transitional operator chain well generation order maximizing generation probability possible data permutation ', 'one key idea use neural network soft lookup table approximating possibly huge  discrete transition matrix ', 'strategy allows u amortize space complexity single model make transitional operator generalizable unseen instance ', 'ensure learned markov chain ergodic  propose greedy batchwise permutation scheme allows fast training ', 'empirically  evaluate learned markov chain showing gmns able discover order among data instance also perform comparably well stateoftheart method oneshot recognition benchmark task ']","The assumption that data samples are independently identically distributed is the backbone of many learning algorithms., Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances., Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically., Specifically, we assume that the instances are sampled from a Markov chain., Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations., One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix., This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances., To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  , Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.",16,5.494949494949495,12.375
612,"['We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.', 'The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.', 'To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.', 'We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.']","[1, 0, 0, 0]","[0.5, 0.2790697515010834, 0.1111111044883728, 0.06896550953388214]",B1KJJf-R-,"['Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model', 'Presents a seq2Tree model to translate a problem statement in natural language to the corresponding functional program in DSL, which has shown an improvement over the seq2seq baseline approach.', 'This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples.', 'The paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network.']","['present neural program search  algorithm generate program natural language description small number input  output example ', 'algorithm combine method deep learning program synthesis field designing rich domainspecific language  dsl  defining efficient search algorithm guided seq2tree model ', 'evaluate quality approach also present semisynthetic dataset description test example corresponding program ', 'show algorithm significantly outperforms sequencetosequence model attention baseline ']","We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples., The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it., To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs., We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.",5,5.988372093023256,17.2
613,"['Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks.', 'The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable).', 'In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions.', 'This is a very mild condition satisfied even by neural networks with a single neuron.', 'Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics.', 'When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set.', 'When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.', 'Our analysis sheds lights on understanding the practical performance of GANs.']","[0, 0, 0, 0, 0, 1, 0, 0]","[0.27272728085517883, 0.1818181723356247, 0.260869562625885, 0.2857142686843872, 0.1666666567325592, 0.30434781312942505, 0.09756097197532654, 0.1875]",Hk9Xc_lR-,"['This paper studies the discrimination and generalization properties of GANs when the discriminator set is a restricted function class like neural networks.', 'Balances capacities of generator and discriminator classes in GANs by guaranteeing that induced IPMs are metrics and not pseudo metrics', 'This paper provides a mathematical analysis of the role of the size of the adversary/discriminator set in GANs']","['generative adversarial training generally understood minimizing certain moment matching loss defined set discriminator function  typically neural network ', 'discriminator set large enough able uniquely identify true distribution  discriminative   also small enough go beyond memorizing sample  generalizable  ', 'paper  show discriminator set guaranteed discriminative whenever linear span dense set bounded continuous function ', 'mild condition satisfied even neural network single neuron ', ' develop generalization bound learned distribution true distribution different evaluation metric ', 'evaluated neural distance  bound show generalization guaranteed long discriminator set small enough  regardless size generator hypothesis set ', 'evaluated kl divergence  bound provides explanation counterintuitive behavior testing likelihood gan training ', 'analysis shed light understanding practical performance gans ']","Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks., The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable)., In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions., This is a very mild condition satisfied even by neural networks with a single neuron., Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics., When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set., When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training., Our analysis sheds lights on understanding the practical performance of GANs.",15,5.760233918128655,11.4
614,"['Normalization layers are a staple in state-of-the-art deep neural network architectures.', 'They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.', 'In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.', 'Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.', 'We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.', 'Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.']","[0, 0, 0, 0, 1, 0]","[0.2857142686843872, 0.13333332538604736, 0.1666666567325592, 0.0476190410554409, 0.29411762952804565, 0.21621620655059814]",H1gsz30cKX,"['All you need to train deep residual networks is a good initialization; normalization layers are not necessary.', 'A method is presented for initialization and normalization of deep residual networks. This is based on observations of forward and backward explosion in such networks. The method performance is on par with the best results obtained by other networks with more explicit normalization.', 'The authors propose a novel way to initialize residual networks, which is motivated by the need to avoid exploding/vanishing gradients.', 'Proposes a new initialization method used to train very deep RedNets without using batch-norm.']","['normalization layer staple stateoftheart deep neural network architecture ', 'widely believed stabilize training  enable higher learning rate  accelerate convergence improve generalization  though reason effectiveness still active research topic ', 'work  challenge commonlyheld belief showing none perceived benefit unique normalization ', 'specifically  propose fixedupdate initialization  fixup   initialization motivated solving exploding vanishing gradient problem beginning training via properly rescaling standard initialization ', 'find training residual network fixup stable training normalization  even network 10000 layer ', 'furthermore  proper regularization  fixup enables residual network without normalization achieve stateoftheart performance image classification machine translation ']","Normalization layers are a staple in state-of-the-art deep neural network architectures., They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic., In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization., Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization., We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers., Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.",14,6.4186046511627906,9.214285714285714
615,"['Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult.', 'In a such situation, learning a metric of a sequence from data is one possible solution.', 'The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning.', ""In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training."", 'The partial reward function is a reward function for a partial sequence of a certain length.', 'SeqGAN employs a reward function for completed sequence only.', 'By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole.', 'In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence.', 'Expert-based reward function training is not a kind of GAN frameworks.', 'This makes the optimization of the generator easier.', 'We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE.', 'Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.']","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","[0.4324324131011963, 0.17142856121063232, 0.25, 0.25, 0.1249999925494194, 0.20000000298023224, 0.2222222238779068, 0.1395348757505417, 0.0624999962747097, 0.0714285671710968, 0.17391303181648254, 0.17391303181648254]",r1kP7vlRb,"['This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.', 'Describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value. ', 'This paper considers the problem of improving sequence generation by learning better metrics, specifically the exposure bias problem']","['designing metric manually unsupervised sequence generation task  text generation  essentially difficult ', 'situation  learning metric sequence data one possible solution ', 'previous study  seqgan  proposed framework unsupervised sequence generation  metric learned data  generator optimized regard learned metric policy gradient  inspired generative adversarial net  gans  reinforcement learning ', 'paper  make two proposal learn better metric seqgan  partial reward function expertbased reward function training ', 'partial reward function reward function partial sequence certain length ', 'seqgan employ reward function completed sequence ', 'combining longscale shortscale partial reward function  expect learned metric able evaluate partial correctness well coherence sequence  whole ', 'expertbased reward function training  reward function trained discriminate expert  true  sequence fake sequence produced editing expert sequence ', 'expertbased reward function training kind gan framework ', 'make optimization generator easier ', 'examine effect partial reward function expertbased reward function training synthetic data real text data  show improvement seqgan model trained mle ', 'specifically  whereas seqgan gain 042 improvement nll mle synthetic data  best model gain 302 improvement  whereas seqgan gain 0029 improvement bleu mle  best model gain 0250 improvement ']","Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult., In a such situation, learning a metric of a sequence from data is one possible solution., The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning., In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training., The partial reward function is a reward function for a partial sequence of a certain length., SeqGAN employs a reward function for completed sequence only., By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole., In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence., Expert-based reward function training is not a kind of GAN frameworks., This makes the optimization of the generator easier., We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE., Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.",29,5.384328358208955,9.241379310344827
616,"['One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  ', 'For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  ', 'Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  ', 'We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ']","[1, 0, 0, 0]","[0.3333333134651184, 0.2790697515010834, 0.307692289352417, 0.2083333283662796]",rJTGkKxAZ,"['Decompose the task of learning a generative model into learning disentangled latent factors for subsets of the data and then learning the joint over those latent factors.  ', 'Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference', 'The paper investigates the potential of hierarchical latent variable models for generating images and image sequences and proposes to train several ALI models stacked on top of each other to create a hierarchical representation of the data.', 'The paper aims to learn the hierarchies for training GAN in a hierarchical optimization schedule directly instead of being designed by a human']","['one successful technique generative model decomposing complicated generation task series simpler generation task ', 'example  generating image low resolution learning refine high resolution image often improves result substantially ', 'explore novel strategy decomposing generation complicated object first generate latent variable describe subset observed variable  map latent variable observed space ', 'show allows u achieve decoupled training complicated generative model present theoretical experimental result supporting benefit approach ']","One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  , For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  , Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  , We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ",6,5.513274336283186,18.833333333333332
617,"['Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.', 'In this paper, we propose a new way to leverage visual knowledge for sentence representations.', 'Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.', 'We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.', 'We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.']","[0, 1, 0, 0, 0]","[0.13793103396892548, 0.5185185074806213, 0.16326530277729034, 0.27586206793785095, 0.14814814925193787]",BJe8niAqKX,"['We propose a joint model to incorporate visual knowledge in sentence representations', 'The paper proposes a method to use videos paired with captions to improve sentence embeddings', 'This submission proposes a model for sentence learning sentence representations that are grounded, based on associated video data.', 'Proposes a method for improving text-based sentence embeddings through a joint multimodal framework.']","['visual grounding language active research field aiming enriching textbased representation visual information ', 'paper  propose new way leverage visual knowledge sentence representation ', 'approach transfer structure visual representation space textual space using two complementary source information   1  cluster information  implicit knowledge two sentence associated visual content describe underlying reality  2  perceptual information contained within structure visual space ', 'use joint approach encourage beneficial interaction training textual  perceptual  cluster information ', 'demonstrate quality learned representation semantic relatedness  classification  crossmodal retrieval task ']","Visual grounding of language is an active research field aiming at enriching text-based representations with visual information., In this paper, we propose a new way to leverage visual knowledge for sentence representations., Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space., We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information., We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.",10,6.158333333333333,12.0
